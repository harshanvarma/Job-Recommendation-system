Python (Programming Language)-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Python (Programming Language),Apache Spark,PySpark  Good to have skills :NA Minimum 3 year(s) of experience is required  Educational Qualification :NA Key Reponsibilities :1 Should be able to work on Databricks, Pyspark 2 Should be able to work on AWS S3/ Cloud Front / GCP DAG, Bigquery Cloud services 3 Should be able to work on sql with good basic data warehouse Technical Experience : 1 Should be proficent atleast P3 in Python with 4 or more year of expereince 2 Should have AWS S3/ Cloud Front / GCP DAG, Bigquery Cloud exposure 3 Should have good sql understanding and basic data warehouse 4 Good to have Databricks and React web component Professional Attributes :A:Good communication Skill B:Team player Qualification NA",1.10E+11,11-04-2024,10-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, p3, pyspark, data bricks, spark, cloud services, project management, p6, ms project, primavera, cloudfront, autocad, sql, application development, primavera project planner, apache, gcp, project control, mysql, bigquery, aws, primavera p6, project planning",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Microsoft Azure Analytics Services Good to have skills : NA Minimum  2  year(s) of experience is required Educational Qualification : 15 years Education Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements.  Must have Skills :Microsoft Azure Analytics Services, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :1Function as the Data Architect for a small, simple project or proposal or as a team lead for medium or large sized project or proposal 2 Discuss specific Big data architecture and related issues with client architect or team in area of expertise 3 Analyze and assess the impact of the requirements on the data and its lifecycle 4 Lead Big data architecture and design medium-big Cloud based, Big Data and Analytical Solutions using Lambda architecture Technical Experience :1Strong experience in Azure is preferred with hands-on experience in two or more of these skills :Azure Synapse Analytics, Azure HDInsight, Azure Databricks with PySpark or Scala or SparkSQL, Azure Analysis Services 2Experience in one or more Real-time or Streaming technologies including:Azure Stream Analytics, Azure Data Explorer, Azure Time Series Insights, etc3Experience in handling medium to large Big Data implementations Professional Attributes :Good Communication skills, leadership skills, team handling skills, presentation skills, ability to work under pressure Educational Qualification:15 years EducationAdditional Info :NA Qualification 15 years Education",1.21E+11,12-05-2024,10-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"analytics services, azure analytics, ssi, microsoft azure, azure hdinsight, azure databricks, c#, python, oracle, scala, azure synapse, pyspark, data architecture, sql server, application development, sql, spark, azure analysis, azure data explorer, mysql, big data",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Python Developer," We are looking for an experienced AWS Python Developer to join our team. The ideal candidate should be passionate about coding and developing scalable and high-performance applications. You will work closely with our front-end developers, designers, and other members of the team to deliver quality solutions that meet the needs of our clients.     Qualification: Bachelors degree in Computer Science or related field or higher with minimum 5 years of relevant experience.           Be on the cutting edge of the advancing industry practice of site reliability engineering with the opportunity to work with a team of passionate IT engineers. Our mission is to enhance the performance and availability of Data and Analytical technologies as we move forward and adopt the methodology of site reliability engineering (SRE) for our IT assets. SRE is a set of principles and practices that incorporates resiliency software engineering via automation. Take pride in creating and modifying applications to provide early warning awareness of potential problems, eliminate toil, facilitate the flow of critical data pipelines and measure real service level objectives(SLO) using service level indicators(SLI). Be a part of a team that drives a culture of continuous improvement, development, and integration of new ideas and initiatives. Enjoy the benefits of supporting systems that are efficient and improve customer satisfaction. We are looking for candidates with application delivery experience and a desire to apply their development skills to improve the operational effectiveness of our IT systems. Be a part of Cignas future by joining a versatile team focused on growth and ingenuity.     The role requires a wide variety of strengths and capabilities, including:   Knowledge of application, data and infrastructure architecture disciplines     o Eliminating workarounds and manual tasks because they take valuable time away from development.   o Architecting robust data quality, latency, and resource capacity enterprise alerting   o Create proactive measurable and effective SLOs to ensure SLA commitments   This is a development role to be involved in AWS and on-prem relational database technologies, as well as utilizing Python and other program languages. Support awareness tools like Dynatrace and Splunk are key cornerstones of our SRE footprint.   Ability to work in large, collaborative teams to achieve organization goals   Willing to work on a distributed team spanning multiple time zones.   Analytical, problem-solving, and critical thinking skills   Good communication skills   Strong skills in troubleshooting and documentation of operational procedures   The desire to continually learn and test your own boundaries   Exposure to standard software development practices including agile, deployment pipelines, continuous integrations and continuous delivery   Understanding of software skills such as business analysis, development, maintenance and software improvements     Qualifications:   Bachelor s degree or equivalent experience   6+ years of experience in Developer, Operations roles   5+ years Python experiences required   4+ years Hands on experiences in some AWS technologies like - Cloud Watch, Step Functions, Lambda, Glue, EC2, Redshift, DynamoDB, SQS, ECS/EKS, Elastic Search, SnowFlake, Elastic Cache, databricks, TeraForm to name a few   Experience with monitoring and observability systems like Dynatrace, Splunk etc   Experience with build and deployment pipeline technologies like Jenkins, Ansible, GitHub   Experience in Pyspark   Experience working with relational databases and data warehousing technologies   System administration skills of Unix, Linux, and Windows operation systems a plus   Experience with text mining a plus     The primary responsibilities include   Operational analysis   o Manage and monitor production systems   o Application Availability & performance issues   o Data and Integration issues   o Stakeholder communication   Incident Management and root cause analysis   o Facilitate the incident management process   o Guide the team on root cause analysis   o Create and Track problem analyses   Manage performance metrics   o Work with stakeholders to identify and define the service level indicators and objectives   o Measure and report the performance metrics   Automation and tools   o Work with the service delivery teams in developing and testing automation of repetitive and manual efforts.   o Work closely with engineering teams on monitoring and performance management tools initiatives to improve system reliability and performance       Required qualifications to be successful in this role:    Computer Science (BE / BTech / MTech / MS) from Tier I Premier institutes         Must-Have Skills:      :        Strong proficiency in Python & AWS   Experience with Splunk & Dynatrace   Experience with CI/CD Knowledge   Familiarity with version control systems such as Git   Experience with agile development methodologies and DevOps practices   Excellent problem-solving and analytical skills   Strong communication and collaboration skills     Good-to-Have Skills:   Experience with cloud platforms such as AWS & Python   Experience with containerization technologies such as Docker and Kubernetes   Familiarity with message brokers such as Lambda & EC2,S3           Skills:      Python     Spark SQL     SQL     Teradata     Terraform         ",1.41E+11,14-09-2023,13-12-2023,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Unix, Linux, Business analysis, Coding, Windows, Teradata, Troubleshooting, SQL, Python, System administration",-,9am-6pm,"Full Time, Permanent",CGI,Organization,CGI,https://img.naukimg.com/logo_images/groups/v1/1402790.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google BigQuery-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Google BigQuery  Good to have skills :Teradata BI Minimum 2 year(s) of experience is required  Educational Qualification :minimum 15 years Full time education Key Reponsibilities :1 Shall have sound understanding of Data Warehousing Concepts with strong command over SQL skills, having hands on exposure on developing data pipelines using google cloud native services, deep understanding on data modelling skills with on ground exposure of logical and physical data modelling, data classification and documentation of business processes relationship with data model Technical Experience : 1 Must have Google BigQuery Developer or similar role, with a strong command of SQL, data modeling, and data warehousing concepts 2 Good to have understanding and hands-on experience with Google Cloud Platform GCP services, specifically Big Query 3 Good to have Proficiency in scripting languages such as Python or Bash for ETL processes and automation Professional Attributes :1 Showcase quick learning skills to adapt on client specific tools and processes to adhere governance, security deliverable quality needs, should self-organize and be agile team player, having mature understanding of development test deliverables, adhere to agile best practices",2.70E+11,27-04-2024,26-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sql, data modeling, data warehousing concepts, bigquery, scripting languages, python, data warehousing, sql server, application development, plsql, bteq, unix shell scripting, gcp, teradata bi, bash, mysql, agile, etl, informatica, etl development, unix, teradata sql",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Databricks Unified Data Analytics Platform-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Databricks Unified Data Analytics Platform  Good to have skills :PySpark Minimum 3 year(s) of experience is required  Educational Qualification :Any technical graduation Key Reponsibilities : Work on client projects to deliver AWS, PySpark, Databricks based Data engineering & Analytics solutions  Build and operate very large data warehouses or data lakes.  ETL optimization, designing, coding, & tuning big data processes using Apache Spark.  Build data pipelines & applications to stream and process datasets at low latencies.  Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.Technical Experience : Minimum of 1 years of experience in Databricks engineering solutions on AWS Cloud platforms using PySpark  Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture & delivery.  Minimum 2 year of Experience in one or more programming languages Python, Java, Scala  Experience using airflow for the data pipelines in min 1 project  1 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, TerraformProfessional Attributes : Ready to work in B Shift (12 PM 10 PM)  A Client facing skills:solid experience working in client facing environments, to be able to build trusted relationships with client stakeholders  Good critical thinking and problem-solving abilities  Health care knowledge  Good Communication Skil",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data analytics, pyspark, aws cloud microservices, etl, big data, kubernetes, python, scala, docker, application development, data bricks, coding, java, git, apache, aws cloud, spark, jenkins, data warehousing concepts, shell scripting, terraform, hadoop, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : PySpark Good to have skills : Databricks Unified Data Analytics Platform, Python (Programming Language) Minimum  3  year(s) of experience is required Educational Qualification : Ant technical graduation Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using PySpark. Your typical day will involve working with Databricks Unified Data Analytics Platform and Python to develop and maintain applications that meet the needs of our clients. Key Responsibilities: Work on client projects to deliver AWS, PySpark, Databricks based Data engineering & Analytics solutions  Build and operate very large data warehouses or data lakes.  ETL optimization, designing, coding, & tuning big data processes using Apache Spark.  Build data pipelines & applications to stream and process datasets at low latencies.  Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. Technical Experience: Minimum of 1 years of experience in Databricks engineering solutions on AWS Cloud platforms using PySpark  Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture & delivery.  Minimum 2 year of Experience in one or more programming languages Python, Java, Scala  Experience using airflow for the data pipelines in min 1 project  1 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform Additional Information: The candidate should have a minimum of 5 years of experience in PySpark. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful software solutions. This position is based at our Chennai office.-Willing to work in B shift Qualification Ant technical graduation",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pyspark, aws cloud microservices, hadoop, etl, big data, kubernetes, python, scala, docker, application development, data bricks, java, git, apache, spark, aws cloud, jenkins, data warehousing concepts, terraform, shell scripting, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : PySpark Good to have skills : NA Minimum  10+  year(s) of experience is required Educational Qualification : Graduate Project Role :Data Platform Engineer Project Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have Skills :PySpark, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :Work on client projects to deliver AWS, PySpark, Databricks based Data engineering Analytics solutions Build and operate very large data warehouses or data lakes ETL optimization, designing, coding, tuning big data processes using Apache Spark Build data pipelines applications to stream and process datasets at low latencies Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data Technical Experience :Minimum of 1 years of experience in Databricks engineering solutions on AWS Cloud platforms using PySpark Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture delivery Minimum 2 year of Experience in one or more programming languages Python, Java, Scala Experience using airflow for the data pipelines in min 1 project 1 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform Professional Attributes :-Ready to work in B Shift 12 PM 10 PM 2-A Client facing skills:solid experience working in client facing environments, to be able to build trusted relationships with client stakeholders 3-Good critical thinking and problem-solving abilities 4-Health care knowledge Good Communication Skills Educational Qualification:GraduateAdditional Info : Qualification Graduate",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, pyspark, aws cloud microservices, etl, big data, kubernetes, scala, data warehousing, docker, java, git, apache, data modeling, aws cloud, spark, jenkins, shell scripting, hadoop, python, warehouse, application development, data bricks, terraform, aws, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : PySpark Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Graduate Project Role :Data Platform Engineer Project Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have Skills :PySpark, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :Work on client projects to deliver AWS, PySpark, Databricks based Data engineering Analytics solutions Build and operate very large data warehouses or data lakes ETL optimization, designing, coding, tuning big data processes using Apache Spark Build data pipelines applications to stream and process datasets at low latencies Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data Technical Experience :Minimum of 1 years of experience in Databricks engineering solutions on AWS Cloud platforms using PySpark Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture delivery Minimum 2 year of Experience in one or more programming languages Python, Java, Scala Experience using airflow for the data pipelines in min 1 project 1 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform Professional Attributes :-Ready to work in B Shift 12 PM 10 PM 2-A Client facing skills:solid experience working in client facing environments, to be able to build trusted relationships with client stakeholders 3-Good critical thinking and problem-solving abilities 4-Health care knowledge Good Communication Skills Educational Qualification:GraduateAdditional Info:resources who are based in Bengaluru and are willing to travel to the office in client place. Qualification Graduate",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, pyspark, aws cloud microservices, etl, big data, kubernetes, scala, data warehousing, docker, java, git, apache, data modeling, aws cloud, spark, jenkins, shell scripting, hadoop, python, warehouse, application development, data bricks, terraform, aws, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
python developer,"Rekruiters Resource Managment LLP is looking for python developer to join our dynamic team and embark on a rewarding career journey      Coordinating with development teams to determine application requirements.      Writing scalable code using Python programming language.      Testing and debugging applications.      Developing back-end components.      Integrating user-facing elements using server-side logic.      Assessing and prioritizing client feature requests.      Integrating data storage solutions.      Reprogramming existing databases to improve functionality.      Developing digital tools to monitor online traffic.      Write effective, scalable code      Develop back-end components to improve responsiveness and overall performance      Integrate user-facing elements into applications      Test and debug programs      Improve functionality of existing systems      Implement security and data protection solutions      Assess and prioritize feature requests      Coordinate with internal teams to understand user requirements and provide technical solutions.    ",2.30E+11,23-02-2024,23-05-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,Management Consulting,"python, css, c, software testing, natural language processing, python development, machine learning, artificial intelligence, javascript, sql, pandas, django, git, data science, postgresql, linux, oops, debugging, html, mysql, data structures, flask, aws, programming",-,9am-6pm,"Full Time, Permanent",Rekruiters Resource Managment Llp,Organization,Rekruiters Resource Managment Llp,https://img.naukri.com/logo_images/v3/5451356.gif,"Kochi, Kolkata, Mumbai, Unnao, New Delhi, Hyderabad, Ahmedabad, Bengaluru","Kochi, Kolkata, Mumbai, Unnao, New Delhi, Hyderabad, Ahmedabad, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Cloud Developer,"Your Role and Responsibilities As a python cloud development engineer you will Design, development and test Power Private Cloud software.  Able to understand the architecture and turn the requirements into high level and low level designs  Active open source contributions and participation and open source tools knowledge is preferred  Software development in Cloud Domain, OpenStack, Virtualization, Linux OS Internals, Networking Storage/ Security Infrastructure as a Service is preferred Understanding of any cloud based virtualization software [GCP, Azure] and Openstack concepts. Work with OpenSource community and contribute towards open source development Testing. Good understanding and hands on with full stack cloud development is preferred.  Responsibilities: IBM India Systems Development Lab is looking for Software Engineer Developer to be integral part of a team responsible for Product Development/testing of PowerVC Platform on Power. The product is built on the Openstack cloud computing platform using Python. In addition, there is an opportunity to work on several OpenSource communities and front end/back-end development as well. Required Technical and Professional Expertise 4+ years of experience in the IT industry 4+ years of experience working as a developer well versed with feature enablement on private/public cloud platforms or in an equivalent role supporting partners and enterprises 4+ years of experience with solutions development or implementation in Unix or Linux environments using python programming language. 2+ years of experience working with Ansible [ Nice to have ]. Proficient in Python programming and scripting experience and Openstack concepts. Experience working in Open Source communities is a big plus. Proven experience architecting, designing, and developing complex customer solutions in a rapidly evolving technology domain Ability to perform customer-facing activities in a fast-paced environment with short timeframes Experience with concepts of Openstack cloud computing platform. Ability to uncover business challenges and develop custom solutions to solve these challenges Working knowledge of object-oriented design and design patterns applicable to modern software development Applied knowledge of working with agile, scrum, and DevOps teams Clear understanding of cloud service and deployment models Comfortable working with highly distributed teams, including interaction with open source communities Ability to study on your own, learn quickly, and put new knowledge into practice Any commercial experience with technologies like Openstack, virtualization and public cloud services, e.g., Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) Ability to quickly learn new technologies, frameworks, and techniques; ability to facilitate technical conversations within your team and with external stakeholders Keen interest contributing to and building communities in open source Ability to work both on your own and as part of an agile team. Preferred Technical and Professional Expertise Hands-on experience on Openstack based clouds, basic concepts of virtualization, strong with Python programming, automation using Ansible.",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, design patterns, linux, openstack, unix, openstack cloud, public cloud, architecting, cloud services, web services, cloud development, microsoft azure, linux internals, networking, ansible, docker, open source, gcp, devops, full stack, jenkins, scrum, agile, aws",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer,"SUMMARY    We are hiring for  Python developer  for one of our esteemed client for  Bangalore Location:    Python developers         Exp - 4-8 years            Location - Bangalore               4+ year?? experience in developing complex python application.            Experience in Python (and its variants such as Python Spark, Python Flask, Python Django, Python Tensorflow)              Expertise in Large Language Models (LLM) based prompt and response pair generation and validation              Solid experience in programming, data structures and problem solving              Good to have NLP, Generative AI and AI/ML expertise              Excellent communication skills              Hands on designing and architectural experience          ",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, natural language processing, large, python development, problem solving, machine learning, artificial intelligence, python flask, sql, pandas, tensorflow, django, application, data science, spark, data structures, mysql, html, flask, programming, communication skills, ml, architecture",-,9am-6pm,"Full Time, Permanent",2coms,Organization,2coms,https://img.naukimg.com/logo_images/groups/v1/467982.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Databricks Unified Data Analytics Platform-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Databricks Unified Data Analytics Platform  Good to have skills :Apache Spark,PySpark Minimum 3 year(s) of experience is required  Educational Qualification : Key Reponsibilities :1Azure Devops CI/CD Integration Specialist to help us set up the end-to-end technical Continuous Integration/Continuous Deployment framework in Azure ADF, Databricks code, SQL and AAS and embedding the processes around it as well with the team 2Build processes supporting data transformation, data structures, metadata, dependency and workload management 3 Azure Data Factory, Azure Data Lake Storage, Azure SQL, Pyspark Technical Experience : 1Extensive experience with Azure Data Bricks and good to have Synapse, SQL, Pyspark 2Experience with Azure:Azure Data Factory, Azure Data Lake Storage, Databricks, Stream Analytics, Azure Functions, Serverless Architecture, ARM Templates 3Experience with object-oriented/object function scripting languages:Python, SQL, Scala, Spark-SQL 4Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of data Professional Attributes :1Strong project management and organizational skills 2Experience supporting and working with cross-functional teams in a dynamic environment 3Analytical bent of mind 4Ability to manage interaction with business stakeholders and other within the organization 5Good communication and documentation skil",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, project management, data analytics, pyspark, spark, continuous integration, python, metadata, scala, azure data factory, relational databases, sql, application development, azure functions, apache, sql azure, devops, data structures, arm",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Python (Programming Language) Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : minimum 15 years of full-time education Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have Skills :Python (Programming Language)Good to Have Skills : No Function SpecializationJob Requirements :Key Responsibilities :A :Would be responsible for developing , troubleshooting manage project programs written in Python B :Code, test and debug programs according to Python best practices C:Communicate with a team in order to coordinate and document application development and testing Technical Experience :A:4-6 years of Python Development experience with strong knowledge of core python libraries numpy Pandas, boto3 B:Experience working with JSON, CSVs, Excel and database handling with Python programming and data models like dictionaries C:High-level understanding of SQL/NOSQL databases, caching, security, distributed systems, messaging technologies D:Strong expertise in AWS services like S3, EC2,SQS, SNS, ECS, Elasticsearch, cloudformation E:Experience in CI/CD with GIT and Jenkins Professional Attributes :A :should have good communication and Analytical skills B :ability to work independently and learn quickly Qualification minimum 15 years of full-time education",2.80E+11,28-04-2024,27-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, aws iam, numpy, pandas, boto3, continuous integration, software testing, python development, ci/cd, distribution system, aws cloudformation, nosql, sql, application development, amazon sqs, elastic search, amazon ec2, coding, git, ecs, jenkins, sns, json, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Databricks Unified Data Analytics Platform Good to have skills : Apache Spark, PySpark Minimum  7.5  year(s) of experience is required Educational Qualification : br/>Key Responsibilities :1Azure Devops CI/CD Integration Specialist to help us set up the end-to-end technical Continuous Integration/Continuous Deployment framework in Azure ADF, Databricks code, SQL and AAS and embedding the processes around it as well with the team 2Build processes supporting data transformation, data structures, metadata, dependency and workload management 3 Azure Data Factory, Azure Data Lake Storage, Azure SQL, Pyspark br/> Technical Experience :1Extensive experience with Azure Data Bricks and good to have Synapse, SQL, Pyspark 2Experience with Azure:Azure Data Factory, Azure Data Lake Storage, Databricks, Stream Analytics, Azure Functions, Serverless Architecture, ARM Templates 3Experience with object-oriented/object function scripting languages:Python, SQL, Scala, Spark-SQL 4Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of data br/> Professional Attributes :1Strong project management and organizational skills 2Experience supporting and working with cross-functional teams in a dynamic environment 3Analytical bent of mind 4Ability to manage interaction with business stakeholders and other within the organization 5Good communication and documentation skil",20524909868,02-05-2024,31-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, project management, python, data analytics, scala, continuous integration, metadata, pyspark, azure data factory, relational databases, sql, application development, azure functions, apache, sql azure, spark, devops, data structures, arm",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Amazon Web Services (AWS) Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : BE Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Amazon Web Services (AWS). Your typical day will involve working with AWS services, developing and testing applications, and collaborating with cross-functional teams to deliver high-quality solutions.  Roles & Responsibilities: Design, develop, and test applications using AWS services, adhering to best practices and coding standards. Collaborate with cross-functional teams to identify and prioritize application requirements, ensuring alignment with business processes. Configure and deploy applications to AWS, ensuring scalability, availability, and security. Troubleshoot and debug application issues, providing timely resolutions and ensuring high application performance. Stay updated with the latest AWS services and features, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Strong experience in Amazon Web Services (AWS). Good To Have Skills:Experience in other cloud platforms such as Microsoft Azure or Google Cloud Platform. Experience in designing and developing scalable, highly available, and fault-tolerant applications using AWS services such as EC2, S3, RDS, Lambda, and API Gateway. Experience in developing applications using programming languages such as Java, Python, or Node.js. Experience in using DevOps tools such as Jenkins, Git, and Docker for continuous integration and deployment. Solid understanding of application security, including authentication, authorization, and encryption. Experience in using monitoring and logging tools such as CloudWatch and ELK stack for application performance monitoring and troubleshooting. Additional Information: The candidate should have a minimum of 5 years of experience in Amazon Web Services (AWS). The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality solutions using AWS. This position is based at our Bengaluru office. Qualification BE",20524910680,02-05-2024,31-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"docker, lambda expressions, application security, jenkins, aws, python, aws iam, api gateway, web services, microsoft azure, amazon rds, application development, amazon ec2, node.js, git, java, gcp, devops, troubleshooting, api, amazon cloudwatch",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Amazon Web Services (AWS) Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : BE Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Amazon Web Services (AWS). Your typical day will involve working with AWS services, developing and testing applications, and collaborating with cross-functional teams to deliver high-quality solutions.  Roles & Responsibilities: Design, develop, and test applications using AWS services, adhering to best practices and coding standards. Collaborate with cross-functional teams to identify and prioritize application requirements, ensuring alignment with business processes. Configure and deploy applications to AWS, ensuring scalability, availability, and security. Troubleshoot and debug application issues, providing timely resolutions and ensuring high application performance. Stay updated with the latest AWS services and features, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Strong experience in Amazon Web Services (AWS). Good To Have Skills:Experience in other cloud platforms such as Microsoft Azure or Google Cloud Platform. Experience in designing and developing scalable, highly available, and fault-tolerant applications using AWS services such as EC2, S3, RDS, Lambda, and API Gateway. Experience in developing applications using programming languages such as Java, Python, or Node.js. Experience in using DevOps tools such as Jenkins, Git, and Docker for continuous integration and deployment. Solid understanding of application security, including authentication, authorization, and encryption. Experience in using monitoring and logging tools such as CloudWatch and ELK stack for application performance monitoring and troubleshooting. Additional Information: The candidate should have a minimum of 5 years of experience in Amazon Web Services (AWS). The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality solutions using AWS. This position is based at our Bengaluru office. Qualification BE",20524909417,02-05-2024,31-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"docker, lambda expressions, application security, jenkins, aws, python, aws iam, api gateway, web services, microsoft azure, amazon rds, application development, amazon ec2, node.js, git, java, gcp, devops, troubleshooting, api, amazon cloudwatch",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Databricks Unified Data Analytics Platform Good to have skills : PySpark, Python (Programming Language) Minimum  5  year(s) of experience is required Educational Qualification : Any technical graduation Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Databricks Unified Data Analytics Platform. Your typical day will involve working with PySpark and Python to develop and maintain applications that meet the needs of our clients. Key Responsibilities: Work on client projects to deliver AWS, PySpark, Databricks based Data engineering & Analytics solutions  Build and operate very large data warehouses or data lakes.  ETL optimization, designing, coding, & tuning big data processes using Apache Spark.  Build data pipelines & applications to stream and process datasets at low latencies.  Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. Technical Experience: Minimum of 1 years of experience in Databricks engineering solutions on AWS Cloud platforms using PySpark  Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture & delivery.  Minimum 2 year of Experience in one or more programming languages Python, Java, Scala  Experience using airflow for the data pipelines in min 1 project  1 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform Additional Information: The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality applications. Willing to work in B shift Qualification Any technical graduation",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data analytics, aws cloud microservices, hadoop, etl, big data, kubernetes, python, scala, pyspark, docker, application development, data bricks, java, git, spark, aws cloud, jenkins, data warehousing concepts, terraform, shell scripting, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 years of education Summary :As an Application Developer, specializing in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with various DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Lead the design, development, and implementation of DevOps processes and tools, including continuous integration and delivery pipelines, automated testing, and deployment automation. Collaborate with cross-functional teams to ensure seamless integration of DevOps processes and tools into the application development lifecycle. Ensure the availability, scalability, and reliability of applications by implementing robust monitoring, logging, and alerting mechanisms. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage, including experience with containerization technologies like Docker and Kubernetes. Professional & Technical Skills: Must To Have Skills:Strong experience in DevOps processes and tools, including continuous integration and delivery pipelines, automated testing, and deployment automation. Good To Have Skills:Experience with containerization technologies like Docker and Kubernetes. Solid understanding of software development methodologies, including Agile and Waterfall. Experience with cloud platforms like AWS, Azure, or GCP. Strong understanding of infrastructure as code (IaC) principles and tools like Terraform or CloudFormation. Additional Information: The candidate should have a minimum of 7.5 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Mumbai office. Qualification 15 years of education",80524912863,08-05-2024,06-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"docker, devops, waterfall, software development methodologies, terraform, kubernetes, continuous integration, automation testing, microsoft azure, aws cloudformation, application development, gcp, agile, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : Must have minimum 15 years of full time education Summary :As an Application Developer, specializing in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with various DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Lead the design, development, and implementation of DevOps processes and tools, including continuous integration and delivery pipelines, automated testing, and deployment automation. Collaborate with cross-functional teams to ensure seamless integration of DevOps processes and tools into the application development lifecycle. Ensure the availability, scalability, and reliability of applications by implementing robust monitoring, logging, and alerting mechanisms. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage, including experience with containerization technologies like Docker and Kubernetes.  Professional & Technical Skills: Must To Have Skills:Strong experience in DevOps processes and tools, including continuous integration and delivery pipelines, automated testing, and deployment automation. Good To Have Skills:Experience with containerization technologies like Docker and Kubernetes. Solid understanding of software development methodologies, including Agile and Waterfall. Experience with cloud platforms like AWS, Azure, or GCP. Strong understanding of infrastructure as code (IaC) principles and tools like Terraform or CloudFormation.  Additional Information: The candidate should have a minimum of 7.5 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Mumbai office. Qualification Must have minimum 15 years of full time education",80524911241,08-05-2024,06-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, waterfall, software development methodologies, kubernetes, continuous integration, automation testing, aws cloudformation, application development, gcp, terraform, agile, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Databricks Unified Data Analytics Platform-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Databricks Unified Data Analytics Platform  Good to have skills :Apache Spark,PySpark Minimum 5 year(s) of experience is required  Educational Qualification : Key Reponsibilities :1Azure Devops CI/CD Integration Specialist to help us set up the end-to-end technical Continuous Integration/Continuous Deployment framework in Azure ADF, Databricks code, SQL and AAS and embedding the processes around it as well with the team 2Build processes supporting data transformation, data structures, metadata, dependency and workload management 3 Azure Data Factory, Azure Data Lake Storage, Azure SQL, Pyspark Technical Experience : 1Extensive experience with Azure Data Bricks and good to have Synapse, SQL, Pyspark 2Experience with Azure:Azure Data Factory, Azure Data Lake Storage, Databricks, Stream Analytics, Azure Functions, Serverless Architecture, ARM Templates 3Experience with object-oriented/object function scripting languages:Python, SQL, Scala, Spark-SQL 4Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of data Professional Attributes :1Strong project management and organizational skills 2Experience supporting and working with cross-functional teams in a dynamic environment 3Analytical bent of mind 4Ability to manage interaction with business stakeholders and other within the organization 5Good communication and documentation skil",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, project management, data analytics, pyspark, spark, continuous integration, python, metadata, scala, azure data factory, relational databases, sql, application development, azure functions, apache, sql azure, devops, data structures, arm",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Google BigQuery Good to have skills : NA Minimum  10  year(s) of experience is required Educational Qualification : Fulltime 15 years qualification Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have Skills :Google BigQueryGood to Have Skills : No Technology SpecializationJob Requirements :Key Responsibilities :1:Data Proc, Pub,Sub, Data flow, Kalka Streaming, Looker, SQL - No FLEX2:Proven track record of delivering data integration, data warehousing soln3:Strong SQL And Hands-on, Pro in BigQuery SQL language,Exp in Shell Scripting, Python - No FLEX4:Exp with data integration and migration projects ,Oracle, SQL5:understanding on cloud native services :bucket storage, GBQ, cloud function, pub sub, composer, and Kubernetes6:Exp in cloud solutions, mainly data platform services , GCP Certifications Technical Experience :1:Expert in Python - NO FLEX. Strong hands-on- knowledge in SQL - NO FLEX, Python programming using Pandas, NumPy, deep understanding of various data structure dictionary, array, list, tree etc, experiences in pytest, code coverage skills2:Exp with building solutions using cloud native services:bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes NO FLEX3:Pro with tools to automate AZDO CI CD pipelines like Control-M , GitHub, JIRA, confluence , CI CD Pipeline Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:Fulltime 15 years qualificationAdditional Info : Qualification Fulltime 15 years qualification",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sql, shell scripting, bigquery, python, google, kubernetes, css, confluence, ci/cd, data warehousing, numpy, spring, java, gcp, flex, mysql, html, jira, github, oracle, pytest, warehouse, sql server, javascript, dataproc, application development, pandas, gbq, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Databricks Unified Data Analytics Platform Good to have skills : PySpark Minimum  5  year(s) of experience is required Educational Qualification : br/>Key Responsibilities :1Azure Devops CI/CD Integration Specialist to help us set up the end-to-end technical Continuous Integration/Continuous Deployment framework in Azure ADF, Databricks code, SQL and AAS and embedding the processes around it as well with the team 2Build processes supporting data transformation, data structures, metadata, dependency and workload management 3 Azure Data Factory, Azure Data Lake Storage, Azure SQL, Pyspark br/> Technical Experience :1Extensive experience with Azure Data Bricks and good to have Synapse, SQL, Pyspark 2Experience with Azure:Azure Data Factory, Azure Data Lake Storage, Databricks, Stream Analytics, Azure Functions, Serverless Architecture, ARM Templates 3Experience with object-oriented/object function scripting languages:Python, SQL, Scala, Spark-SQL 4Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of data br/> Professional Attributes :1Strong project management and organizational skills 2Experience supporting and working with cross-functional teams in a dynamic environment 3Analytical bent of mind 4Ability to manage interaction with business stakeholders and other within the organization 5Good communication and documentation skil",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, project management, python, data analytics, scala, continuous integration, metadata, pyspark, azure data factory, relational databases, sql, application development, azure functions, sql azure, spark, devops, data structures, arm",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
PySpark-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :PySpark  Good to have skills :Python (Programming Language),Apache Spark Minimum 5 year(s) of experience is required  Educational Qualification : Educational Qualification:15 years of full term education Key Reponsibilities :A:Responsibility to model, architect and build data assets and data products in line with solution design specifications This will include data modelling, data controls development, pipeline development and customer data curationB:Supporting the building of automated data engineering pipelinesC:Supporting data discovery and sourcing identify and source golden source data and develop data lineageD:Work closely with core technology and architecture teams in the bank to build data knowledg Technical Experience : Technical Experience :A:Min 7 years of overall ExperienceB:5 yrs of Hands on Exp of Pyspark ,AWS Glue , Python , SQLC:Experience in Leading a team managing DeliveryD:Understanding of Development LifeCycle, CICD PipelinesE:Experience around Working with Agile Practices ,JIRAF:experience in Designing Data Driven Solutions Professional Attributes :Have excellent communication Qualification NA",1.40E+11,14-04-2024,13-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pyspark, ci/cd, spark, python, aws glue, hive, amazon redshift, data warehousing, dbms, sql, plsql, apache, java, solution design, mysql, hadoop, big data, etl, oracle, data engineering, aws lambda, sql server, application development, amazon ec2, athena, agile, sqoop, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Google BigQuery Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements.  Must have Skills :Google BigQuery, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :1:Proven track record of delivering data integration, data warehousing solutions 2:Exp with data integration and migration projects 3:Proficient in BigQuery SQL language4:Good understanding on cloud native services :bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes5:Exp working with cloud solutions, mainly data platform services Willingness to get GCP Certifications 6:Required Preferred work experience in Shell Scripting, Python, Oracle, SQL Server Technical Experience :1:Expert in PytStrong hands-on and strong knowledge in SQL, Python programming using Pandas, NumPy, deep understanding of various data structure dictionary, array, list, tree etc, experiences in pytest, code coverage skills are preferred2:Strong hands-on experience with building solutions using cloud native services:bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes etc. Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:Additional Info :Level and Across Accenture Location Facilities",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, sql, bigquery, python, cloud native, kubernetes, functional, data warehousing, numpy, microservices, spring, java, gcp, shell scripting, mysql, html, pubsub, rest, oracle, code coverage, pytest, sql server, javascript, application development, pandas, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Kubernetes Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 Years full time education Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Kubernetes. Your typical day will involve working with the development team, analyzing requirements, and developing scalable and reliable applications.  Roles & Responsibilities: Design, develop, and maintain Kubernetes-based applications to meet business requirements. Collaborate with cross-functional teams to analyze requirements and develop scalable and reliable applications. Develop and maintain automated deployment pipelines using Kubernetes and related tools. Ensure the security, scalability, and reliability of applications by implementing best practices and industry standards. Troubleshoot and debug issues in production and non-production environments. Professional & Technical Skills: Must To Have Skills:Expertise in Kubernetes. Good To Have Skills:Experience with Docker, Helm, and other Kubernetes-related tools. Strong understanding of containerization and microservices architecture. Experience with cloud platforms such as AWS, Azure, or GCP. Proficiency in programming languages such as Java, Python, or Go. Solid grasp of DevOps principles and practices. Additional Information: The candidate should have a minimum of 5 years of experience in Kubernetes. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering scalable and reliable applications. This position is based at our Bengaluru office. Qualification 15 Years full time education",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, microservices, containerization, java, devops, rest, python, golang, microsoft azure, hibernate, helm, javascript, docker, application development, sql, spring, spring boot, gcp, j2ee, troubleshooting, mysql, html, aws, requirement analysis",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Spring Boot-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Spring Boot  Good to have skills :NA Minimum 3 year(s) of experience is required  Educational Qualification :15years of Full time education Key Reponsibilities :A:Build and deploy backend applications from scratch B:Interact with stakeholders to understand requirements, analyse impact of changes, come up with reasonable timelines and execute C:Plan & execute extensive load testing for services D:Create an execution plan for the team to iterate based on the load test findings to ensure a resilient, self healing set of subsystems which have loose coupling and high cohesion D:Understand & analyse existing systems to identify improvements & performance / maintenance issues Fix or rebuild existing systems to ensure resilience E:Write extensive test cases in a way that systems can be easily modified in future with confidence & without ambiguity Technical Experience : A:1-5 years of hands-on experience in Backend technologies B:Experience in at least one of Java, Elixir or Golang C:Experience with debugging solving issues in a cloud environment D:Experience with PostgreSQL or similar database E:Experience with NoSQL databases F:Deep understanding of technology and architecture in a highly scalable and available environment G:Experience with open-source technologies such as Elasticsearch, Redis, Kafka etc and their adoption into products Professional Attributes :Willingness to work in a startup like environmentGood to have :prior Fin tech ecp, exp in using AWS services such as S3, SNS, SQS etc, Contributed to open-source projects, exp with Big data pipelines and BI tools",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"elixir, java, postgresql, debugging, spring boot, css, golang, hibernate, redis, microservices, sql, spring, elastic search, react.js, git, spring mvc, linux, j2ee, mysql, html, big data, rest, python, maven, javascript, nosql, amazon sqs, angular, node.js, kafka, sns, php, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : SAS Visual Text Analytics Good to have skills : SAS Visual Data Mining and Machine Learning, SAS Platform Minimum  3  year(s) of experience is required Educational Qualification : Role  Summary /PurposeWe are looking for an experienced system administrator to deploy and manage SAS Viya on AWS and to participate in platform responsibilities related to on-prem SAS and migration to the public/private cloud, including working/monitoring/administrating SAS VIYA 4 platform, automating deployment with Terraform, IaC scripts and help us building CICD pipeline for SAS Viya 4 Deployment/Upgrade while working in collaboration with SYF DevOps Team. The profile must also understand security roles/policies (IAM) and other AWS Services, including S3, EKS, Redshift, RDS, FSx (Filesystem) and Terraform, IaC, etc.Essential Responsibilities Work with cloud architects, SAS vendors, and other stakeholders to deploy SAS Viya on public/private cloud on Containers (Kubernetes) as available for AWS (EKS) or on-prem (TKGI) and help design the IaC and Terraform scripts for automated deployment while working with SYF DevOps and Cloud Platform Engineering Team. Work with the SAS platform and run teams to ensure the environment remains highly available and current. Complete any required documentation related to the build and run. Perform other duties as needed to ensure the effective running of the environment. Implement best practices for automating the SAS Viya Deployment process using Terraform and IaC scripts and work with the DevOps team to deploy the automated scripts in the CICD pipeline.  Good understanding of IAM Roles and Policies and working with the IAM Team to deploy/design roles/policies as required by the Application.  Hands-on experience with various AWS services like S3, Redshift, RDS, EKS, FSx (Filesystem),including DevOps, Terraform, etc. Qualifications / Requirements Bachelor's Degree, or equivalent, in a quantitative field, such as Engineering, Computer Science, etc.) At least seven (5-7) years of experience in Information Technology. At least two (2-3) years of experience with public cloud deployments (MUST) for DevOps, IaC, and IAM roles and policies. Proficient in DevOps and Terraform/IaC in the public cloud. Strong communication skills Good to have experience as an SAS Viya administratorDesired Characteristics Able to work effectively with multiple teams and stakeholders. Willingness to stay abreast of the latest developments in technology. Able to efficiently balance the demands of supporting the environment, Platform Engineering Team, and Run team. Able to effectively troubleshoot issues with installation and configuration. Demonstrated ability to work effectively in a team environment. Self-motivated person who can complete tasks with minimal supervision from Platform Leads.",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"public cloud, iac, text analytics, devops, terraform, kubernetes, python, sas, information technology, amazon redshift, data mining, cloud deployment, file system, machine learning, eks, amazon rds, sas viya, application development, sql, iam, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Microsoft Azure DevOps Good to have skills : Microsoft SQL Server Minimum  5  year(s) of experience is required Educational Qualification : Btech Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Microsoft Azure DevOps. Your typical day will involve collaborating with cross-functional teams, developing and deploying applications, and ensuring their functionality and performance.  Roles & Responsibilities: Design, build, and configure applications to meet business process and application requirements using Microsoft Azure DevOps. Collaborate with cross-functional teams to identify and prioritize application requirements. Develop and deploy applications, ensuring their functionality and performance. Troubleshoot and debug applications to resolve issues and improve performance. Ensure the security, scalability, and maintainability of applications. Professional & Technical Skills: Must have-Good knowledge of Azure Cloud and Azure PaaS offeringsWork experience in Azure Devops CI/CD pipelines, preferably YamlWork experience in application integration using REST API and other integration patternsProgramming experience preferably Python.ETL knowledge, preferably ADF and Data BricksSQL knowledge Good To Have Skills:Knowledge of SAP PowerDesigner or any other data modeling toolData Governance and Data catalog knowledgeGood understanding on the Relational Databases and Datalakes.Good to have Bash scriptingGood to have Azure Cloud networking concepts.Certification on DP203Window's administration Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful applications. This position is based at our Bengaluru office. Qualification Btech",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure paas, azure cloud, sql server, etl, microsoft azure devops, continuous integration, rest, python, sap, ci/cd, networking, relational databases, azure devops, application development, sql, application integration, oracle adf, devops, bash, api",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Amazon Web Services (AWS) Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : BE Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Amazon Web Services (AWS). Your typical day will involve working with AWS services, developing and testing applications, and collaborating with cross-functional teams to deliver high-quality solutions.  Roles & Responsibilities: Design, develop, and test applications using AWS services, adhering to best practices and coding standards. Collaborate with cross-functional teams to identify and prioritize application requirements, ensuring alignment with business processes. Configure and deploy applications to AWS, ensuring scalability, availability, and security. Troubleshoot and debug application issues, providing timely resolutions and ensuring high application performance. Stay updated with the latest AWS services and features, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Strong experience in Amazon Web Services (AWS). Good To Have Skills:Experience in other cloud platforms such as Microsoft Azure or Google Cloud Platform. Experience in designing and developing scalable, highly available, and fault-tolerant applications using AWS services such as EC2, S3, RDS, Lambda, and API Gateway. Experience in developing applications using programming languages such as Java, Python, or Node.js. Experience in using DevOps tools such as Jenkins, Git, and Docker for continuous integration and deployment. Solid understanding of application security, including authentication, authorization, and encryption. Experience in using monitoring and logging tools such as CloudWatch and ELK stack for application performance monitoring and troubleshooting. Additional Information: The candidate should have a minimum of 5 years of experience in Amazon Web Services (AWS). The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality solutions using AWS. This position is based at our Bengaluru office. Qualification BE",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, java, application security, jenkins, aws, python, aws iam, api gateway, amazon rds, docker, application development, amazon ec2, coding, node.js, git, lambda expressions, gcp, devops, troubleshooting, api, amazon cloudwatch",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of regular education Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps and ensuring smooth application deployment and maintenance.  Roles & Responsibilities: Design, develop, and maintain applications using DevOps tools and methodologies. Collaborate with cross-functional teams to ensure smooth application deployment and maintenance. Troubleshoot and resolve application issues, ensuring high availability and performance. Implement and maintain DevOps best practices, including continuous integration and delivery, infrastructure as code, and automated testing. Professional & Technical Skills: Must To Have Skills:Strong experience in DevOps. Experience with containerization technologies such as Docker and Kubernetes. Experience with cloud platforms such as AWS, Azure, or GCP. Experience with configuration management tools such as Ansible, Chef, or Puppet. Experience with scripting languages such as Python, Bash, or PowerShell. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 years of regular education",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, docker, ansible, gcp, devops, continuous integration, python, microsoft azure, sql server, javascript, application development, sql, puppet, high availability, java, powershell, troubleshooting, bash, mysql, aws, application deployment",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Engineering qualification is required Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python, Ruby, or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 5 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification Engineering qualification is required",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"docker, devops, jenkins, cloud computing platform, software engineering, kubernetes, python, microsoft azure, aws cloudformation, ansible, ruby, application development, computer science, gcp, troubleshooting, terraform, bash, agile, aws, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Microsoft Azure Analytics Services Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Minimum 15 years of fulltime education Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have Skills :Microsoft Azure Analytics ServicesGood to Have Skills : No Technology SpecializationJob Requirements :Key Responsibilities :A Should have 5-6 years experience in developing azure cloud application and proficiency in Azure Data Factory, Azure Data Bricks , Logic Apps In depth Knowledge in Key vault, ADLS and SQL managed Instances B good to have knowledge on Self hosted integration run time , private endpoints and migration related activities C should have conceptual understanding and experience on the data warehouse tools such as Snowflake , synapse and redshift D Knowledge on Devops CI/CD process is a plus Technical Experience :A Strong experience in Azure is preferred with hands-on experience in two or more of these skills :Azure Synapse Analytics, Azure HDInsight, Azure Databricks with PySpark or Scala or SparkSQL, Azure Analysis Services B Experience in one or more Real-time or Streaming technologies including:Azure Stream Analytics, Azure Data Explorer, Azure Time Series Insights, etc C Candidate must have 3-7 years of IT experience and around 1-3 years of extensive Big data experience Professional Attributes :A Team player B Should have excellent client communication skills C Should have good analytical and problem-solving skills Educational Qualification:Minimum 15 years of fulltime educationAdditional Info : Qualification Minimum 15 years of fulltime education",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, azure data lake, azure data factory, azure logic apps, key vault, analytics services, scala, amazon redshift, azure analytics, microsoft azure, azure hdinsight, azure analysis services, azure cloud, sql, azure analysis, big data",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Databricks Unified Data Analytics Platform-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Databricks Unified Data Analytics Platform  Good to have skills :Apache Spark,PySpark Minimum 3 year(s) of experience is required  Educational Qualification :Graduation Key Reponsibilities :1Azure Devops CI/CD Integration Specialist to help us set up the end-to-end technical Continuous Integration/Continuous Deployment framework in Azure ADF, Databricks code, SQL and AAS and embedding the processes around it as well with the team 2Build processes supporting data transformation, data structures, metadata, dependency and workload management 3 Azure Data Factory, Azure Data Lake Storage, Azure SQL, Pyspark Technical Experience : 1Extensive experience with Azure Data Bricks and good to have Synapse, SQL, Pyspark 2Experience with Azure:Azure Data Factory, Azure Data Lake Storage, Databricks, Stream Analytics, Azure Functions, Serverless Architecture, ARM Templates 3Experience with object-oriented/object function scripting languages:Python, SQL, Scala, Spark-SQL 4Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of data Professional Attributes :1Strong project management and organizational skills 2Experience supporting and working with cross-functional teams in a dynamic environment 3Analytical bent of mind 4Ability to manage interaction with business stakeholders and other within the organization 5Good communication and documentation skil Qualification NA",1.40E+11,14-04-2024,13-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, project management, data analytics, pyspark, spark, continuous integration, metadata, azure data factory, relational databases, Initiative, sql, application development, azure functions, apache, sql azure, devops, data structures, arm",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Databricks Unified Data Analytics Platform-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Databricks Unified Data Analytics Platform  Good to have skills :Apache Spark Minimum 3 year(s) of experience is required  Educational Qualification :Graduation Key Reponsibilities :1Azure Devops CI/CD Integration Specialist to help us set up the end-to-end technical Continuous Integration/Continuous Deployment framework in Azure ADF, Databricks code, SQL and AAS and embedding the processes around it as well with the team 2Build processes supporting data transformation, data structures, metadata, dependency and workload management 3 Azure Data Factory, Azure Data Lake Storage, Azure SQL, Pyspark Technical Experience : 1Extensive experience with Azure Data Bricks and good to have Synapse, SQL, Pyspark 2Experience with Azure:Azure Data Factory, Azure Data Lake Storage, Databricks, Stream Analytics, Azure Functions, Serverless Architecture, ARM Templates 3Experience with object-oriented/object function scripting languages:Python, SQL, Scala, Spark-SQL 4Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of data Professional Attributes :1Strong project management and organizational skills 2Experience supporting and working with cross-functional teams in a dynamic environment 3Analytical bent of mind 4Ability to manage interaction with business stakeholders and other within the organization 5Good communication and documentation skil Qualification NA",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, project management, data analytics, scala, spark, continuous integration, metadata, pyspark, azure data factory, relational databases, Flexibility, application development, azure functions, apache, sql azure, devops, data structures, arm",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Databricks Unified Data Analytics Platform-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Databricks Unified Data Analytics Platform  Good to have skills :Apache Spark,PySpark Minimum 5 year(s) of experience is required  Educational Qualification : Key Reponsibilities :1Azure Devops CI/CD Integration Specialist to help us set up the end-to-end technical Continuous Integration/Continuous Deployment framework in Azure ADF, Databricks code, SQL and AAS and embedding the processes around it as well with the team 2Build processes supporting data transformation, data structures, metadata, dependency and workload management 3 Azure Data Factory, Azure Data Lake Storage, Azure SQL, Pyspark Technical Experience : 1Extensive experience with Azure Data Bricks and good to have Synapse, SQL, Pyspark 2Experience with Azure:Azure Data Factory, Azure Data Lake Storage, Databricks, Stream Analytics, Azure Functions, Serverless Architecture, ARM Templates 3Experience with object-oriented/object function scripting languages:Python, SQL, Scala, Spark-SQL 4Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of data Professional Attributes :1Strong project management and organizational skills 2Experience supporting and working with cross-functional teams in a dynamic environment 3Analytical bent of mind 4Ability to manage interaction with business stakeholders and other within the organization 5Good communication and documentation skil Qualification NA",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, project management, data analytics, pyspark, spark, continuous integration, metadata, Emotional intelligence, azure data factory, relational databases, sql, application development, azure functions, apache, sql azure, devops, data structures, arm",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google Cloud Platform Architecture-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Google Cloud Platform Architecture,.NET Architecture  Good to have skills :NA Minimum 5 year(s) of experience is required  Educational Qualification :Fulltime 15 years qualification Key Reponsibilities :1 Being the first point of contact for Scrum master dev team for any architectural review of new functionality being introduced in the application both from best practices point of viewconsidering teams current skill set also for any potential performance impacts as well 2 Understanding existing cloud architecture suggesting ways means to help improve on it in terms of best practices performance etc Technical Experience : 1 5-7yrs of good developer full stack experience ASPNet Java Angular Cloud Tech with experience of atleast 1-2 yrs on cloud tech in particular google cloud is needed any prior experience of working as a tech arch for a team would be helpful 2 Any certifications to formalize their knowledge on goole cloud cloud tech is also recommended 3 Hands-on work on pub subs, GKE, terraform modules, cloud functions, writing optimizing BQ queries, firestore, microservice arch Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required",20524907361,02-05-2024,31-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"google, cloud platform, gcp, platform architecture, net architecture, kubernetes, sap, enterprise architecture, microsoft azure, docker, ansible, angular, java, cloud architecture, asp.net, sap basis, linux, scrum, .net, terraform, sap hana, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Sr Python Developer, Rotational shift","We are hiring Python Developer for Our Client is No 1 Search Engine for Hyderabad location. Work from Office. Rotational shift 5 days working. Sr Python Developer Location Hyderabad Mini Exp - 5+ yrs of expeirnece in Python Skill Python Programming ( Should be good In Python Programming) Budget upto 17LPA Notice Period 15 days or Immediate Joiner Shift Timing Work from office with Rotational Shift, 5 days working. Pls call Hemanth 9715166618 for more info Thanks, Hemanth 9715166618",80524006239,08-05-2024,06-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, Django, Python Developer, Python Development, Flask",-,9am-6pm,"Full Time, Permanent",Creative Hands HR,Organization,Creative Hands HR,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer with AWS,We are looking for Python Developer with AWS with minimum 5+ years of experience. Contact : Shruthi (9500181847) Required Candidate profile Proficient in designing and developing AWS Lex chat bots using Python. Natural language processing (NLP) & large language models (LLMs) to enhance Chabot capabilities.,2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"AWS, Python, AWS Lambda, Natural language processing NLP, Amazon Comprehend, PostgreSQL, Amazon S3, Debugging, Amazon Polly, AWS Lex chat bots, AI concepts, Dialog Management, Large Language Models LLM, Slot filling",-,9am-6pm,"Full Time, Permanent",Vishanz Business Services,Organization,Vishanz Business Services,-,"Hyderabad, Pune, Bengaluru","Hyderabad, Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : PySpark Good to have skills : Databricks Unified Data Analytics Platform, Python (Programming Language) Minimum  5  year(s) of experience is required Educational Qualification : Ant technical graduation Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using PySpark. Your typical day will involve working with Databricks Unified Data Analytics Platform and Python to develop and maintain applications that meet the needs of our clients. Key Responsibilities: Work on client projects to deliver AWS, PySpark, Databricks based Data engineering & Analytics solutions  Build and operate very large data warehouses or data lakes.  ETL optimization, designing, coding, & tuning big data processes using Apache Spark.  Build data pipelines & applications to stream and process datasets at low latencies.  Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. Technical Experience: Minimum of 1 years of experience in Databricks engineering solutions on AWS Cloud platforms using PySpark  Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture & delivery.  Minimum 2 year of Experience in one or more programming languages Python, Java, Scala  Experience using airflow for the data pipelines in min 1 project  1 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform Additional Information: The candidate should have a minimum of 5 years of experience in PySpark. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful software solutions. This position is based at our Chennai office.-Willing to work in B shift Qualification Ant technical graduation",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pyspark, aws cloud microservices, hadoop, etl, big data, kubernetes, python, scala, docker, application development, data bricks, java, git, apache, spark, aws cloud, jenkins, data warehousing concepts, terraform, shell scripting, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Databricks Unified Data Analytics Platform Good to have skills : PySpark, Python (Programming Language) Minimum  5  year(s) of experience is required Educational Qualification : Any technical graduation Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Databricks Unified Data Analytics Platform. Your typical day will involve working with PySpark and Python to develop and maintain applications that meet the needs of our clients. Key Responsibilities: Work on client projects to deliver AWS, PySpark, Databricks based Data engineering & Analytics solutions  Build and operate very large data warehouses or data lakes.  ETL optimization, designing, coding, & tuning big data processes using Apache Spark.  Build data pipelines & applications to stream and process datasets at low latencies.  Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. Technical Experience: Minimum of 1 years of experience in Databricks engineering solutions on AWS Cloud platforms using PySpark  Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture & delivery.  Minimum 2 year of Experience in one or more programming languages Python, Java, Scala  Experience using airflow for the data pipelines in min 1 project  1 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform Additional Information: The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality applications. Willing to work in B shift Qualification Any technical graduation",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data analytics, aws cloud microservices, hadoop, etl, big data, kubernetes, python, scala, pyspark, docker, application development, data bricks, java, git, spark, aws cloud, jenkins, data warehousing concepts, terraform, shell scripting, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : AWS Architecture Good to have skills : Python (Programming Language) Minimum  5  year(s) of experience is required Educational Qualification : Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using AWS Architecture. Your typical day will involve working with AWS services, developing and testing code, and collaborating with cross-functional teams to deliver high-quality solutions.  Roles & Responsibilities:- Drive automation and integrate with CI/CD tools for continuous validation. Drive mentality of building well architected applications for AWS Cloud Drive the mentality of quality being owned by the entire team. Identify code defects and work with other developers to address quality issues in product code. Finding bottlenecks and thresholds in existing code through the use of automation tools. Articulate clear business objectives aligned to technical specifications and work in an iterative agile pattern daily. Ownership over your work task, and are comfortable interacting with all levels of the team and raise challenges when necessary. Professional & Technical Skills: Core code production for back, middle and front end applications Deploying and developing AWS cloud applications and services end to end Operational triage of bugs, failed test cases and system failures Creating and optimizing infrastructure performance metrics Mapping user stories to detailed technical specifications Complete detailed peer code reviews Architecting pilots and proofs-of-concept effort to spur innovation Working in all stages of the development lifecycle Automation of manual data object creation and test cases Ask smart questions, collaborate, team up, take risks, and champion new ideas  Extensive experience with AWS or other cloud technologies including Glue, Lambda, S3, IAM, VPC, EC2, Athena, Cloudwatch, Dynamo and RDS Understanding of the serverless mindset on architectural solutioning Strong Terraform IaaS experience. Experience with DevOps & CI/CD tools Jenkins, Cloudbees, Please Build, etc. Proficiency with OOP languages such as Python, Java, Scala, but Python preferred Proficiency working with large data stores and data sets Deep understanding of database concepts and design for SQL (primarily) and NoSQL (secondarily) -- schema design, optimization, scalability, etc. Solid experience with git software version control and good understanding of code branching strategies and organization for code reuse Qualification NA",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloud technologies, sql, oop languages, database creation, aws, continuous integration, scala, glue, ci/cd, git, java, iam, devops, oops, jenkins, cloudify, amazon cloudwatch, python, version control, dynamo, amazon rds, nosql, amazon ec2, terraform, athena, iaas",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Nagpur,Nagpur,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Google Cloud Platform Administration Good to have skills : Accenture Delivery Methods (ADM) Minimum  5  year(s) of experience is required Educational Qualification : Minimum 15 years of Full time Education Summary :As a Google Cloud Platform Administrator, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with the Google Cloud Platform, ensuring the smooth functioning of applications, and providing technical support to stakeholders.  Roles & Responsibilities: Design, build, and configure applications to meet business process and application requirements using Google Cloud Platform. Ensure the smooth functioning of applications by monitoring and maintaining the Google Cloud Platform infrastructure. Provide technical support to stakeholders, including troubleshooting and resolving issues related to the Google Cloud Platform. Collaborate with cross-functional teams to ensure the successful delivery of projects and applications. Professional & Technical Skills: Must To Have Skills:Experience in Google Cloud Platform Administration. Good To Have Skills:Experience with Accenture Delivery Methods (ADM). Strong understanding of cloud computing concepts and technologies. Experience with infrastructure automation tools such as Terraform or Ansible. Experience with containerization technologies such as Docker and Kubernetes. Additional Information: The candidate should have a minimum of 5 years of experience in Google Cloud Platform Administration. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful solutions. This position is based at our Pune office. Qualification Minimum 15 years of Full time Education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"google cloud platform administration, docker, ansible, terraform, cloud computing, kubernetes, python, vmware, microsoft azure, application development, sql, automation tools, java, gcp, linux, microsoft windows, troubleshooting, mysql, aws, jira",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Google Cloud Platform Administration Good to have skills : Accenture Delivery Methods (ADM) Minimum  5  year(s) of experience is required Educational Qualification : Minimum 15 years of Full time Education Summary :As a Google Cloud Platform Administrator, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with the Google Cloud Platform, ensuring the smooth functioning of applications, and providing technical support to stakeholders.  Roles & Responsibilities: Design, build, and configure applications to meet business process and application requirements using Google Cloud Platform. Ensure the smooth functioning of applications by monitoring and maintaining the Google Cloud Platform infrastructure. Provide technical support to stakeholders, including troubleshooting and resolving issues related to the Google Cloud Platform. Collaborate with cross-functional teams to ensure the successful delivery of projects and applications. Professional & Technical Skills: Must To Have Skills:Experience in Google Cloud Platform Administration. Good To Have Skills:Experience with Accenture Delivery Methods (ADM). Strong understanding of cloud computing concepts and technologies. Experience with infrastructure automation tools such as Terraform or Ansible. Experience with containerization technologies such as Docker and Kubernetes. Additional Information: The candidate should have a minimum of 5 years of experience in Google Cloud Platform Administration. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful solutions. This position is based at our Pune office. Qualification Minimum 15 years of Full time Education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, google cloud platform administration, docker, ansible, terraform, python, vmware, microsoft azure, application development, sql, automation tools, java, gcp, linux, microsoft windows, troubleshooting, mysql, cloud computing, aws, jira",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Google BigQuery Good to have skills : NA Minimum  10  year(s) of experience is required Educational Qualification : Fulltime 15 years qualification Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have Skills :Google BigQueryGood to Have Skills : No Technology SpecializationJob Requirements :Key Responsibilities :1:Data Proc, Pub,Sub, Data flow, Kalka Streaming, Looker, SQL - No FLEX2:Proven track record of delivering data integration, data warehousing soln3:Strong SQL And Hands-on, Pro in BigQuery SQL language,Exp in Shell Scripting, Python - No FLEX4:Exp with data integration and migration projects ,Oracle, SQL5:understanding on cloud native services :bucket storage, GBQ, cloud function, pub sub, composer, and Kubernetes6:Exp in cloud solutions, mainly data platform services , GCP Certifications Technical Experience :1:Expert in Python - NO FLEX. Strong hands-on- knowledge in SQL - NO FLEX, Python programming using Pandas, NumPy, deep understanding of various data structure dictionary, array, list, tree etc, experiences in pytest, code coverage skills2:Exp with building solutions using cloud native services:bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes NO FLEX3:Pro with tools to automate AZDO CI CD pipelines like Control-M , GitHub, JIRA, confluence , CI CD Pipeline Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:Fulltime 15 years qualificationAdditional Info : Qualification Fulltime 15 years qualification",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sql, shell scripting, bigquery, python, google, kubernetes, css, confluence, ci/cd, data warehousing, numpy, spring, java, gcp, flex, mysql, html, jira, github, oracle, pytest, warehouse, sql server, javascript, dataproc, application development, pandas, gbq, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Google BigQuery Good to have skills : NA Minimum  10  year(s) of experience is required Educational Qualification : Fulltime 15 years qualification Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have Skills :Google BigQueryGood to Have Skills : No Technology SpecializationJob Requirements :Key Responsibilities :1:Data Proc, Pub,Sub, Data flow, Kalka Streaming, Looker, SQL - No FLEX2:Proven track record of delivering data integration, data warehousing soln3:Strong SQL And Hands-on, Pro in BigQuery SQL language,Exp in Shell Scripting, Python - No FLEX4:Exp with data integration and migration projects ,Oracle, SQL5:understanding on cloud native services :bucket storage, GBQ, cloud function, pub sub, composer, and Kubernetes6:Exp in cloud solutions, mainly data platform services , GCP Certifications Technical Experience :1:Expert in Python - NO FLEX. Strong hands-on- knowledge in SQL - NO FLEX, Python programming using Pandas, NumPy, deep understanding of various data structure dictionary, array, list, tree etc, experiences in pytest, code coverage skills2:Exp with building solutions using cloud native services:bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes NO FLEX3:Pro with tools to automate AZDO CI CD pipelines like Control-M , GitHub, JIRA, confluence , CI CD Pipeline Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:Fulltime 15 years qualificationAdditional Info : Qualification Fulltime 15 years qualification",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sql, shell scripting, bigquery, python, google, kubernetes, css, confluence, ci/cd, data warehousing, numpy, spring, java, gcp, flex, mysql, html, jira, github, oracle, pytest, warehouse, sql server, javascript, dataproc, application development, pandas, gbq, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Google BigQuery Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : minimum 15 years of full-time education Project Role :Analytics and Modelor Project Role Description :Analyze and model client, market and key performance data. Use analytical tools and techniques to develop business insights and improve decision-making.  Must have Skills :Google BigQuery, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :1:Data Proc, Pub,Sub, Data flow, Kalka Streaming, Looker, SQL - No FLEX2:Proven track record of delivering data integration, data warehousing soln3:Strong SQL And Hands-on, Pro in BigQuery SQL language,Exp in Shell Scripting, Python - No FLEX4:Exp with data integration and migration projects ,Oracle, SQL5:understanding on cloud native services :bucket storage, GBQ, cloud function, pub sub, composer, and Kubernetes6:Exp in cloud solutions, mainly data platform services , GCP Certifications Technical Experience :1:Expert in Python - NO FLEX. Strong hands-on- knowledge in SQL - NO FLEX, Python programming using Pandas, NumPy, deep understanding of various data structure dictionary, array, list, tree etc, experiences in pytest, code coverage skills2:Exp with building solutions using cloud native services:bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes NO FLEX3:Pro with tools to automate AZDO CI CD pipelines like Control-M , GitHub, JIRA, confluence , CI CD Pipeline Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:minimum 15 years of full-time educationAdditional Info : Qualification minimum 15 years of full-time education",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, sql, shell scripting, bigquery, python, kubernetes, css, confluence, ci/cd, data warehousing, numpy, plsql, spring, java, gcp, flex, mysql, html, jira, rest, github, oracle, pytest, javascript, sql server, application development, pandas, spring boot, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Databricks Unified Data Analytics Platform Good to have skills : Apache Spark, PySpark Minimum  5  year(s) of experience is required Educational Qualification : Graduation Role:Application Developer  Project Role Description:Design, build and configure applications to meet business process and application requirements.  Must have Skills :Databricks Unified Data Analytics Platform, SSI: NON SSI:Good to Have Skills :SSI:Apache Spark, PySpark NON SSI :Job Requirements:'',//?field Key Responsibilities:1Azure Devops CI/CD Integration Specialist to help us set up the end-to-end technical Continuous Integration/Continuous Deployment framework in Azure ADF, Databricks code, SQL and AAS and embedding the processes around it as well with the team 2Build processes supporting data transformation, data structures, metadata, dependency and workload management 3 Azure Data Factory, Azure Data Lake Storage, Azure SQL, Pyspark  Technical Experience:1Extensive experience with Azure Data Bricks and good to have Synapse, SQL, Pyspark 2Experience with Azure:Azure Data Factory, Azure Data Lake Storage, Databricks, Stream Analytics, Azure Functions, Serverless Architecture, ARM Templates 3Experience with object-oriented/object function scripting languages:Python, SQL, Scala, Spark-SQL 4Advanced working SQL knowledge and experience working with relational databases, query authoring SQL as well as working familiarity with a variety of data  Professional Attributes:1Strong project management and organizational skills 2Experience supporting and working with cross-functional teams in a dynamic environment 3Analytical bent of mind 4Ability to manage interaction with business stakeholders and other within the organization 5Good communication and documentation skil  Educational Qualification:Graduation Additional Info:Level and Across Accenture Location Facilities Qualification Graduation",20524907387,02-05-2024,31-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, project management, data analytics, scala, ssi, continuous integration, python, metadata, pyspark, azure data factory, relational databases, sql, application development, azure functions, apache, sql azure, spark, devops, data structures, arm",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Graduation Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using DevOps. Your typical day will involve working with DevOps tools and technologies to ensure seamless application development and deployment.  Roles & Responsibilities: Design, build, and configure applications to meet business process and application requirements using DevOps. Collaborate with cross-functional teams to ensure seamless application development and deployment. Implement DevOps tools and technologies to automate the application development and deployment process. Ensure the security, scalability, and reliability of applications through continuous monitoring and testing. Professional & Technical Skills: Must To Have Skills:Proficiency in DevOps. Experience in implementing DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Strong understanding of software development life cycle (SDLC) and agile methodologies. Experience in cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Experience in scripting languages such as Python, Bash, or PowerShell. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Kolkata office. Qualification Graduation",3.00E+11,30-04-2024,29-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, software development life cycle, devops, jenkins, agile methodology, python, software testing, microsoft azure, javascript, sql server, docker, ansible, application development, sql, java, gcp, powershell, bash, mysql, html, agile, aws, sdlc",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Kolkata,Kolkata,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Apache Spark Good to have skills : Microsoft SQL Server, Unix to Linux Migration, Data Warehouse ETL Testing, Amazon Web Service Minimum  3  year(s) of experience is required Educational Qualification : 15 years of Full Time education Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements.  Must have Skills :Apache Spark, SSI: NON SSI:Good to Have Skills :SSI:Data Warehouse ETL Testing, Unix to Linux Migration, Microsoft SQL Server NON SSI:Amazon Web Service Job Requirements :Key Responsibilities :A:Will be responsible for various transformations and actions, spark configuration and tuning techniques B:Must have the Ability to work on Hadoop architecture; execution engines, frameworks, applications tools C:Should be able to work on Pyspark using Spark MLlib library D:Must be able to deliver Data warehousing concepts methods Technical Experience :A:Should have 4-5 years of experience using PySpark with Spark RDDs Spark SQL DataFrames B:Should have 2-3 years of experience in AWS Sagemaker and AWS Glue C:Should have 3-4 years of experience in data wrangling and data analysis with Pandas and Numpy D:Should have 3-4 years of Working experience with ML algorithms like Random Forest, Linear Regression, Logistic Regression, Decision Trees, K Means etc Professional Attributes :A:Should have good communication and analytical skills Educational Qualification:15 years of Full Time educationAdditional Info :General Shift",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, pyspark, spark, data warehousing concepts, hadoop, algorithms, data analysis, aws sagemaker, data warehousing, numpy, sql server, sql, application development, pandas, aws glue, etl testing, ml algorithms, apache, linux, spark mllib, aws, data wrangling, unix, ml",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Developer,"The role will be vital to incubate new ideas, continuous enhancement of the software products, and incorporation of the state-of-the art technology in existing and newly developed software applications in Cloud computing domain.",70124001741,28-04-2024,27-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Cloud Development, Cloud Developer, Cloud Infrastructure, Cloud Deployment, Cloud Architecture, Cloud Migration, Cloud Platform, Cloud Native, Hybrid Cloud",-,9am-6pm,"Full Time, Permanent",VHR Professional Services,Organization,VHR Professional Services,https://img.naukri.com/logo_images/v3/4426332.gif,"Singapore, Canada, United States (USA)","Singapore, Canada, United States (USA)",-,-,-,35-50 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Senior Associate, MLOps Engineering","   In this role, you will be responsible for solving complex problems creatively while adhering to the most advanced industry standards.             ?       Role Responsibilities         Developing, automating, and managing ML Platforms in the cloud.     Making sure that solutions and the current state of platform comply with company policies and standards.     Constantly evaluating processes to create automation, self-service, and improvements.     Expert understanding includes all major services, dependencies, methodologies, frameworks, and conventions that are needed for building and running the platform.     Work closely with the architecture team on determining cost-effective efficient solutions.             Role Requirements         4+ years of professional experience.     Proficient in coding either in Java or Python. Knowledge of any other language along with these is an added advantage.     Problem solver with ability to adapt to changing technology landscape.     Open to explore new things.     Takes complete ownership of code and processes to production and beyond.      Ensures quick resolution for issues, knows use of related tools (Splunk, OpenSearch, CloudWatch, Grafana, Prometheus, Elastic, or any other similar tool)     Has keen eyes for performance of the system. Knowledge of any performance monitoring tools is a plus.     Knows REST and multithreading concepts.     Knowledge of using or administering or enhancing any of the closed or open-source platforms like SageMaker /Dataiku /DataRobot /Databricks/ SeldonIO / H20 / Open Scoring or similar.             Nice To Have:         AWS Developer certification     Python certification     Java certification     Any Open-Source contribution     Hands on knowledge of working on any distributed computing frameworks.               Benefits             Employees Provident Fund [EPF]         Gratuity Payment         Public holidays         Annual Leave, Sick leave, Compensatory leave, and Maternity / Paternity leave         Annual Health Check up         Hospitalization Insurance Coverage (Mediclaim)         Group Life Insurance, Group Personal Accident Insurance Coverage, Business Travel Insurance         Cab Facility          Relocation Benefit       ",90524500663,09-05-2024,07-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Banking,"Health insurance, Automation, Multithreading, Provident fund, Coding, Open source, Financial services, Travel insurance, Python, Recruitment",-,9am-6pm,"Full Time, Permanent",Western Union,Organization,Western Union,https://img.naukimg.com/logo_images/groups/v1/55426.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"About the role Senior Cloud Infrastructure Engineer:  We are looking for a talented Cloud Infrastructure Engineer who will develop effective information technology solutions for OEC by creating new or enhancing existing internal or customer applications. Applies solid and fundamental concepts of cloud engineering and platform management methodology in a team environment. Determines cloud architecture solutions by analyzing business and functional requirements and technical specifications.  Key Responsibilities & Duties:  Ideal Experience and Skills:  Experience with Kubernetes, Docker and orchestration technologies  Experience with at least one of the following cloud platforms: Azure, AWS, GCP  Experience with Bash and PowerShell scripting  Experience with CI/CD platforms such as Azure Pipelines  Experience with Kubernetes deployment tools such as Helm  Experience with infrastructure as code technologies (i.e., terraform, chef, ansible)  Experience with Networking technologies  Experience developing a highly secure infrastructure using security best practices (i.e., container scans, static code analysis).  Experience with service mesh technology such as Istio  Excellent written and oral communication skills.  Preferred familiarity with ELK stack, Azure Dev Ops, Azure SQL, Messaging/Queueing technologies (Azure Service Bus, Kafka), Microservice Architecture, BDD/Gherkin/Selenium, Git ops, OAuth and OIDC, NuGet ",70324908207,08-05-2024,06-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Azure, Docker, GCP, CI/CD platform, chef, Cloud Platform, terraform, AWS, Kubernetes, ansible",-,9am-6pm,"Full Time, Permanent",OEC India,Organization,OEC India,https://img.naukimg.com/logo_images/groups/v1/4615599.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
ADF Support Analyst,"                     The role will    leverage    & enhance existing technologies    in      the area    Data and    analytics    Solutions like Power BI, Azure    data engineering    technologies, Teradata/Snowflake, and other Azure services    .                  The role will be responsible to develop    and support    IT products and solutions using these technologies and deploy them for business users                        Technical    Requirements                5      to    8    Years of IT &    Azure    Data    engineering    technologies experience              Prior experience in ETL, data pipelines, data flow techniques using Azure Data Services              Working experience in Python,    Py    Spark, Azure Data Factory, Azure Data Lake Gen2, Databricks, Azure Synapse and file formats like JSON & Parquet    .              Experience in creating ADF Pipelines to source and process data sets.              Experience in creating Databricks notebooks to cleanse, transform and enrich data sets.              Development experience in orchestration of pipelines              Good understanding about SQL,    Databases,    Datawarehouse systems preferably Teradata              Experience in deployment and monitoring techniques    .              Working experience with Azure DevOps CI/CD pipelines to deploy Azure resources.              Experience in handling operations/Integration with source repository              Must have good knowledge on Datawarehouse concepts and Datawarehouse modelling    .              Working knowledge of SNOW    including resolving incidents,    handling Change requests /Service    requests, reporting    on metrics    to    provide    insights    .              Collaborate with the project team to understand tasks to model tables using data warehouse best practices and develop data pipelines to ensure the efficient delivery of data.              Non-technical requirement:              Work with project leaders to model tables using data warehouse best practices and develop data pipelines to ensure the efficient delivery of data.              Think and work agile, from estimation to development, including testing, continuous integration, and deployment.              Manage    numerous    project tasks concurrently and strategically, prioritizing when necessary.              Proven ability to work as part of a virtual team of technical consultants working from    different locations    (including    onsite    ) around project delivery goals.              Technologies              A    zure data factory              Azure Data bricks              A    zure Synapse              Py    Spark    /SQL              ADLS,BLOB              Azure DevOps with CI/CD implementation.              Nice to Have skill    sets:                    Business Intelligence tools (preferred \u2013Power BI)                DP-203 Certified.                ",60524500919,06-05-2024,04-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Support Analyst, power bi, Deployment, JSON, Teradata, Business intelligence, Data warehousing, Monitoring, SQL, Python",-,9am-6pm,"Full Time, Permanent",Recode Solutions,Organization,Recode Solutions,-,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Sr. Google Cloud Engineer,"Proficiency in python, Pyspark, Airflow, and Cloud Computing Strong knowledge of Databases & Google BigQuery. Experience in working with Google Cloud Platform Excellent on GCP & GCP Services. Ability to work independently and as part of a team. Strong communication and collaboration skills Relevant certifications in cloud computing or related fields Prior experience in a similar role",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"BIGQUERY, AIRFLOW, GCP, Data modelling, hive, python, Google BigQuery",-,9am-6pm,"Full Time, Permanent",Zorba Consulting India Pvt. Ltd.,Organization,Zorba Consulting India Pvt. Ltd.,-,"Mumbai, Pune","Mumbai, Pune",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Solution Engineer- ELK,"   Having good experience in Elasticsearch, Logstash, and Kibana      Responsibilities may include configuring and optimizing Elasticsearch for efficient data indexing and searching, creating Logstash pipelines to ingest and process log data, and designing Kibana dashboards for effective visualization and analysis      Strong knowledge of distributed systems, data parsing, and troubleshooting skills are typically required        ",1.60E+11,16-02-2024,16-05-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Troubleshooting, Distribution system",-,9am-6pm,"Full Time, Permanent",Qualitykiosk Technologies,Organization,Qualitykiosk Technologies,https://img.naukimg.com/logo_images/groups/v1/4620909.gif,"Mumbai, Navi Mumbai","Mumbai, Navi Mumbai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Sr. Azure Cloud Engineer,"   Must have Skills :-          - EventHub         - Azure Data lake, ADSL         - Azure Digital Twins         - Excellent understand and experience in Python         - Azure SQL Server         - Understanding of Web hooks and MQTT protocol         - Experience in Restful API         - Experience in SQL         - Experience in Authorization and Authentication System in Azure, AzureAD         - Nice to have experience in IoT and Security in Development                 ? ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Adsl, Web technologies, Cloud, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Staff Software Engineer, Infrastructure","   8+ years of experience in tech-based companies and preferably start-ups.      Hands-on experience and deep understanding of working with large-scale datasets (10s of Millions of documents), highly scalable and available system architectures      Experience with in-memory cache eg. Redis, and distributed NoSql stores like Elastic search, Cassandra, Hbase, MongoDB etc      Experience in one of the languages like Java, Python, Scala is preferred.      Ability to work with complex business flows and dealing with huge amounts of data.      Experience in building microservices and distributed systems preferred.          Our dual missions one for the world, one for us         For the world: Improve transparency and trust in the B2B ecosystem      For ourselves: Lead fulfilling, impactful lives. Our core values (how we act)      Have Empathy      Continuously push the barrier      Make data-driven decisions      Take smart risks      Have fun at work   ",1.20E+11,12-01-2024,11-04-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Selection process, B2B Marketing, B2B, Sales, NoSQL, Machine learning, Predictive modeling, Distribution system, Python",-,9am-6pm,"Full Time, Permanent",6sense,Organization,6sense,https://img.naukimg.com/logo_images/groups/v1/3714740.gif,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Quality Engineer,"     We are seeking a highly skilled and motivated Data Quality Engineer to join our dynamic team. The ideal candidate is someone who loves to learn, is detail oriented, has exceptional critical thinking and analytical skills. As a Data Quality Engineer, you will play a critical role in ensuring the accuracy, consistency, and completeness of our data across the enterprise data platform. You will be responsible for designing, developing, and implementing data quality processes, standards, and best practices across various data sources and systems to identify, resolve data issues. This role offers an exciting opportunity to learn, collaborate with cross-functional teams, including data engineers, data scientists, and business analysts, to drive data quality improvements and enhance decision-making capabilities.                    Responsibilities              The incumbent will be responsible for (but not limited to) the following:              Designing and implementing data quality checks, identifying, and resolving data quality issues, collaborating with data engineers and product team to improve data pipelines, and developing data quality standards and best practices.              Collaborate with data engineers, analysts, and business stakeholders to understand data requirements and quality expectations.              Analyze architecture and designs to identify test automation strategies to provide full test coverage for new and/or existing data quality checks. Identify and articulate gaps in architecture or design which would lead to data quality issues. Goal here is to validate, automate and maintain high-quality data that supports informed decision-making and business operations.              Monitor and analyze data quality metrics to proactively identify issues and opportunities for improvement.              Establish data quality monitoring processes as part of CI/CD and tools to ensure ongoing compliance with quality standards.              Investigate root causes of data quality issues and work with relevant teams (upstream, downstream & Analyst) to implement corrective actions.              Provide technical guidance and support to ensure data quality standards are integrated into all data-related projects and initiatives.              Work closely with Data Operations & Support, Monitoring team to capture production data quality exceptions and mitigate business impact/risk              Document data quality processes, procedures, and standards for reference and training purposes.                  Business Qualifications              Bachelors degree in Computer Science, Information Systems, or a related field; Masters degree preferred.              Proven experience as a Data Quality Engineer or similar role, with a minimum of 5 years in data quality management or data governance.              Solid understanding of data management concepts, including data profiling, data cleansing, and data integration.              Proficiency in SQL for data querying and manipulation. Develop and execute automated data quality tests using tools like SQL, Python (Pyspark), and data quality frameworks.              Experience with working in a cloud environment (AWS, GCP) preferred.              Experience with data warehousing solutions (Snowflake, Databricks, Redshift) preferred.              Experience with data integration and transformation tools (e.g. Snaplogic, Prophecy, dbt, Talend, etc.) a plus.              Experience with data quality monitoring tools (Acceldata, Tricentis) a plus.              Working knowledge of DevOps principles and CI/CD pipelines preferred.              Strong analytical skills with the ability to identify patterns, trends, and outliers in data.              Strong problem-solving skills and attention to detail.                  Other Qualifications              Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.              Proven ability to prioritize and manage multiple tasks in a fast-paced environment.              Certification in relevant technologies or data management disciplines is a plus.              Analytical mindset with the ability to think strategically and make data-driven decisions.          ",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Manager Quality Assurance, Data management, GCP, Analytical, Data quality, Analytics, Downstream, SQL, Python",-,9am-6pm,"Full Time, Permanent",Workday Inc,Organization,Workday Inc,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AI Platform Engineer,"         DLP (Data Loss Prevention) for PII/sensitive data protection    AutoML, MLOps    Kedro, Airflow, Luigi    Apache Open Whisk    Kubernetes, OpenFaas, Graphx    Version control (i.e. git, gitlab, svn, etc.)    Docker and Docker registry    Front-end technologies ( JavaScript, jQuery, Ajax, Angular)    Server less technologies    Remuneration commensurate with experience            Employee benefits package example              Traditional Benefits                        Retirements & Financial Benefits                        Non-Traditional Employee Paid Benefits                    ",2.20E+11,22-03-2024,20-06-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Business process, Process automation, jQuery, Front end, Version control, GIT, HP data protector, Javascript, Apache, Ajax",-,9am-6pm,"Full Time, Permanent",Artiscien Software Solutions,Organization,Artiscien Software Solutions,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform and Support Engineer,"     We are actively seeking a skilled Data Platform and Support Engineer who will play a pivotal role in ensuring the smooth functioning of our data infrastructure, enabling self-service analytics, and empowering analytical teams across the organization. As a Data Platform and Support Engineer, you will oversee the management of our enterprise data hub, working alongside a team of dedicated data and software engineers to build and maintain a robust data ecosystem that drives decision-making at scale for internal analytical applications.         You will play a key role in ensuring the availability, reliability, and performance of our data infrastructure and systems. You will be responsible for monitoring, maintaining, and optimizing data systems, providing technical support, and implementing proactive measures to enhance data quality and integrity. This role requires advanced technical expertise, problem-solving skills, and a strong commitment to delivering high-quality support services.     The team is responsible for supporting Data Services, Data Warehouse, Analytics, Data Quality and Advanced Analytics/ML for multiple business functions including Sales, Marketing, Services, Support and Customer Experience. We leverage leading modern cloud platforms like AWS, Reltio, Tableau, Snaplogic, MongoDB in addition to the native AWS technologies like Spark, Airflow, Redshift, Sagemaker and Kafka.               Job Responsibilities   :                Monitor the health and performance of data systems, including databases, data warehouses, and data lakes.             Respond to data-related incidents, troubleshoot issues, and resolve them in a timely manner, including participation in on-call rotation shifts.             Conduct root cause analysis and implement corrective actions to prevent recurrence of issues.             Manage and optimize data infrastructure components such as servers, storage systems, and cloud services.             Develop and implement data quality checks, validation rules, and data cleansing procedures.             Implement security controls and compliance measures to protect sensitive data and ensure regulatory compliance.             Design and implement data backup and recovery strategies to safeguard data against loss or corruption.             Optimize the performance of data systems and processes by tuning queries, optimizing storage, and improving ETL pipeline efficiency.             Maintain comprehensive documentation, runbooks, and troubleshooting guides for data systems and processes.             Collaborate with cross-functional teams, including data engineers, data scientists, business analysts, and IT operations.             Lead or participate in data-related projects, such as system migrations, upgrades, or expansions.             Provide training and mentorship to junior team members, sharing knowledge and best practices to support their professional development.             Participate in rotational shifts to provide 24/7 support coverage for data systems, including weekends and holidays as required.             Hands-on experience with source version control, continuous integration and experience with release/change management delivery tools.                 About You           Qualifications:            6+ years of experience designing and building scalable and robust data pipelines to enable data-driven decisions for the business.             Prior experience with CRM systems (e.g. Salesforce) is desirable             Experience building analytical solutions to Sales and Marketing teams.             Experience with very large-scale data warehouse and data engineering projects.             Experience developing low latency data processing solutions like AWS Kinesis, Kafka, Spark Stream processing.             Should be proficient in writing advanced SQLs, Expertise in performance tuning of SQLs             Experience working with AWS data technologies like S3, EMR, Lambda, DynamoDB, Redshift etc.             Strong experience in one or more programming languages for processing of large data sets, such as Python, Scala.             Good to have experience working on Snowflake and/or Databricks.             Ability to create data models, STAR schemas for data consuming.             Extensive experience in troubleshooting data issues, analyzing end to end data pipelines and working with users in resolving issues             BS/MS in computer science or equivalent is required         ",1.60E+11,16-04-2024,15-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Change management, Manager Quality Assurance, Analytical, Data processing, Data quality, IT operations, Technical support, Monitoring, CRM, Business operations",-,9am-6pm,"Full Time, Permanent",Workday Inc,Organization,Workday Inc,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Data Specialist,"       In this role you get to be in the centre of Business Stakeholders, Engineering, Product, and Business Analytics teams to understand, facilitate and maintain data pipelines based on dynamic requirements       You are an integral part of an agile team that works to enhance, build, and deliver data collection, storage, access, and analytics solutions in a secure, stable, and scalable way     As a core technical contributor, you are responsible for maintaining critical data pipelines and architectures across multiple technical areas within various business functions in support of the org s business objectives                Collaborate with and influence business and engineering stakeholders to ensure data infrastructure and products meets the evolving requirements           Lead cross-functional initiatives and collaborate with engineers, product managers, and technical program managers across teams           Work with analysts to productionize analytics and reporting prototypes            Perform analysis on existing data storage system and develop of data solutions in    Snowflake         Leverage SQL and Snowflake to extract, transform, and load (ETL) data from various sources into a structured format for analysis         Improve upon the data ingestion models and ETL jobs to maintain data integrity and data availability of ingest structured and unstructured data         Develop and optimize SQL queries for efficient data retrieval and manipulation         Utilize Python for data analysis, automation, and scripting tasks         Communicate effectively with technical and non-technical stakeholders across multiple business units         What you'll Need to be Successful             Minimum 5 years of enterprise-class experience with large scale cloud solutions in data engineering projects and DevOps. AWS is highly desirable. Google Cloud and Azure are a plus         Understanding of common container and orchestration technologies such as Docker, Container, CI/CD and Kubernetes         Experience architecting complex data marts leveraging DBT and Airflow         Demonstrated ability to architect and build data solutions that leverage data quality and anomaly detection best practices         Experience building production analytics using the Snowflake data platform         Experience in DevOps or DataOps, using Git or Terraform         Intermediate level expertise in Python         Excellent experience in AWS data engineering ecosystem and Snowflake tools and services         Strong experience with data visualization (Power BI, Tableau, Hex, etc) and data storytelling         Good to have AWS Certified Solutions Architect or equivalent     ",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Software Product,"Automation, Data analysis, Business analytics, Data collection, Agile, Data quality, data integrity, data visualization, Analytics, Python",-,9am-6pm,"Full Time, Permanent",Avalara,Organization,Avalara,https://img.naukimg.com/logo_images/groups/v1/4581917.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Sr AWS Engineer," Should have min 10 years of hands-on experience In the AWS cloud at least with S3, EC2, MSK, Glue, DMS and Sagemaker.     Development/work experience in Python, Docker & containerizing.     Should be troubleshooting the problem, reviewing the design, and coding the solution.     An AWS-certified candidate is preferred. ",50324500294,05-03-2024,03-06-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"MIN, Coding, Cloud, Dms, Troubleshooting, AWS, Python",-,9am-6pm,"Full Time, Permanent",Intone Networks,Organization,Intone Networks,https://img.naukimg.com/logo_images/groups/v1/556636.gif,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Analyst - Technology," Data Analysis:      Collect, clean, and analyze data from various sources to identify patterns, trends, and insights relevant to technology initiatives.    Utilize statistical techniques and data visualization tools to present findings and recommendations to stakeholders.    Technology Research:      Stay abreast of emerging technologies, industry trends, and best practices relevant to our organization's technological landscape.    Conduct research and analysis on technology solutions, vendors, and platforms to support decision-making and strategic planning.    Process Improvement:      Identify opportunities to streamline processes, improve efficiency, and enhance the effectiveness of technology systems and workflows.    Collaborate with cross-functional teams to implement process improvements and optimize technology usage across the organization.    Project Support:      Provide analytical support for technology projects, including requirements gathering, solution design, testing, and implementation.    Assist in project planning, resource allocation, and timeline management to ensure successful project delivery.  ",10424501080,01-04-2024,30-06-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Banking,"Analyst, Cloud, Manager Technology, .Net",-,9am-6pm,"Full Time, Permanent",CPP Investments,Organization,CPP Investments,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
data / devops eng(spark + databricks),"   Core skill - Data bricks platform Or Snowflake platform, Python/Java/Scala, Airflow, CI/CD (Devops) process, Building ETL data pipelines, Object Storage in AWS/Azure, Delta lake/Datalake, Kafka.      Exp level - 6+ years                Job Responsibilities:                 Developing latest reusable data processing component libraries based on Data mesh design pattern     Designing implement a modern highly responsive factory approach inner source the components for enterprise use     Translating designs and wire frames into high-quality code     Should have extensive experience building data integration and warehouses in cloud (Azure and AWS)     Strong hands-on experience in python and Java.     Extremely strong SQL skills on OLAP and OLTP technologies.     Ability to do Data models Using Data vault and Dimensional Models     Understand and use CI/CD with on AWS/Azure Kubernetes.     Learn and understand new tools and techniques on Databricks, snowflake, no-sql Databases, data governance and data quality,     Optimizing components for maximum performance across a vast array of data processing and data consumption patterns.   ",3.01E+11,30-08-2023,28-11-2023,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SCALA, data governance, OLAP, Data processing, Data quality, Business solutions, OLTP, AWS, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,remote,remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Cloud (Sr. Engg),"       We are looking for below mentioned Skills.          Very good experience on Azure Cloud environment.      Very good ETL development experience in ADF (Azure Data Factory)      Very good ETL development experience in Azure Databricks using Python, Pyspark, and SQL.            Experience in ingesting data from a variety of data sources, Kubernetes services              ",3.01E+11,30-06-2023,28-09-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Cloud, SQL, Python",-,9am-6pm,"Full Time, Permanent",Hexaquest Global,Organization,Hexaquest Global,https://img.naukimg.com/logo_images/groups/v1/6010528.gif,Mysuru,Mysuru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer Azure Dev,"       Bachelors degree in Computer Science, Engineering, or a related    field.      Fluent in Python for automation & Proven experience as a software    developer with a focus on Python Azure development.      Strong understanding of Azure services and architecture.      Specializing in Azure implementation and management with    expertise in cloud-native design and troubleshooting.      Designing scalable, high-availability Azure solutions, security    enforcement, Azure governance, and monitoring setupProficient in developing and optimizing SQL queries and databases.      Fluent in Terraform IaC for automated provisioning.      Excellent communication and collaboration skills.      Familiarity with Azure DevOps for version control and CI/CD.      Knowledge of security best practices in cloud environments.      Familiarity with Agile/Scrum methodologies.      Azure certifications (e.g., AZ-204, AZ-303, AZ-304) are a plus.      Ability to adapt to a fast-paced and evolving work environment          Responsibilities                  Design, implement, and optimize Azure solutions, ensuring    scalability, performance, and reliability.      Work with Azure App Services for web hosting, Azure Functions for    serverless computing, and Azure SQL Database for data storage.      Implement and integrate Azure services such as Azure API    Management, Azure Logic Apps, and Azure Cognitive Services.      Collaborate with cross-functional teams to define requirements,    analyze technical options, and implement solutions that meet    business objectives.      Utilize Azure DevOps for version control, continuous integration,    and continuous deployment.      Implement and adhere to security best practices, including    authentication, authorization, and data encryption.      Monitor and troubleshoot applications using Azure Monitor,    Application Insights, and other logging tools.      Collaborate with Azure Active Directory for identity and access    management.      Stay current with Azure updates, tools, and best practices to ensure    the adoption of the latest technologies.   ",2.90E+11,29-02-2024,29-05-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Software Product,"Engineering services, Telecom, Automation, Product engineering, Cloud, Troubleshooting, Principal, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Afour Technologies,Organization,Afour Technologies,https://img.naukimg.com/logo_images/groups/v1/4575893.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Monitoring Engineer,"     As a Monitoring Engineer, you will be responsible for monitoring and reporting telecom operations data      You will design dashboards to monitor system health and capture key performance metrics      You will also analyze data to identify traffic patterns and build anomaly detection and predictive models      There will be rotational on-call work thats required for this position                      What we re looking for            6+ years of work experience as a real-time monitoring engineer or ETL developer or big data analyst          Have experience in ELK, Grafana, Splunk and other monitoring/ scripting tools          Have expertise in SQL and Python (Pandas, Jupyter Notebook)          Have proficiency in cloud platforms and technologies including AWS, Snowflake, Databricks, GCP etc          Be self-motivated and can work with minimal supervision          Demonstrate good communication skills and interpersonal skills          Know SIP trunking and SBC technologies                                                Benefits              As part of our award-winning workplace culture and commitment to delivering happiness, our benefits program offers a variety of perks, benefits, and options to help employees maintain their physical, mental, emotional, and financial health; support work-life balance; and contribute to their community in meaningful ways      ",2.90E+11,29-01-2024,28-04-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Interpersonal skills, telecom infrastructure, GCP, Finance, big data analyst, SIP, Operations, Supervision, SQL, Python",-,9am-6pm,"Full Time, Permanent",Zoom Start India,Organization,Zoom Start India,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Snowflake Professional,"     Must Have             1. Previously played a role as a Platform service engineer for one or more of Data engineering platforms services, especially Snowflake and have a good understanding of Cloud services and SaaS and used good governance practices, standards and industry best practices with an enterprise mindset     2. Administered enterprise platform services focussed on Role / Policy / Tag based Access provisioning control, environments configuration management, SSO - Single Sign-On, Authentication Authorization, Platforms connectivity, CI / CD - Continuous Integration Deployment - For ex., Jenkins, Source code management - For ex., GIT / Bitbucket, service request automation, proactive service monitoring alerting, data lifecycle management     3. Good exposure to RDBMS concepts and experience working with SQL programming to query Snowflake metadata to gather insights and visualize on Platform usage, adoption, service operational status alerts, governance observability; Metadata management reporting; Business application Data products jobs cost and performance optimization, management tuning guidelines                     ?         Nice to Have             4. Awareness of GxP and SOX compliant platforms and services, and understand the importrance of data security protection, masking, anonymization, logging auditing, row column level access controls; Backup Disaster recovery.     5. Good experience and exposure to ITIL service management Concepts [Configuration, Knowledge, Service Catalog, Service Levels, Incident, Problem, Request, Release, Change] and have examples of where you have been able to embed best practices in this area through appropriate day to day operational ways of working           ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Service management, Automation, metadata, GIT, RDBMS, data security, Configuration management, Disaster recovery, Monitoring, SQL",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Engineer,"     As a Cloud Engineer at Zecotok, youll be the architect of scalable, resilient, and secure cloud solutions       Dive into the world of AWS, Azure, or Google Cloud Platform to design, deploy, and manage cloud infrastructure     Collaborate with development and operations teams to ensure seamless integration and efficient cloud operations     Proficiency in cloud technologies, automation tools, and containerization empowers you to build digital ecosystems that elevate business performance       ",2.61E+11,26-12-2023,25-03-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Architect, Automation tools, Cloud, Infrastructure, Deployment, Management, AWS",-,9am-6pm,"Full Time, Permanent",Zecotok,Organization,Zecotok,-,Panchkula,Panchkula,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Support Solution Engineer,"   Understanding of shell scripting,python, scala code, Linux, Elastic search     Have understanding of Big data and hands on experience with Apache and Cloudera     Knowledge working in Mysql setup and understanding of sql     Hands on deployment and configuration of any products in previous companies     Working knowledge of monitoring & scheduler tools such as datadog, airflow etc.     Installation and configuration of Apache, Tomcat, Jboss     3-5 years of experience with big data application stack including HDFS, YARN, Spark, Hive and Hbase.     3+ years of experience in Cloud Technologies (AWS/ GCP/ Azure)     3+ years of experience in Big data Enterprise Cluster installation and product installation     2-3 years experience in ETL in a Big Data hadoop environment (Hive/Hbase knowledge).     3+ years years of experience in shell scripting or python.     4+ years of experience in Hadoop Big Data projects Experience with     developing, tuning and debugging code, python/shell scripts loading into Hive, Mariadb, mysql.     Knowledge of python and hands-on coding experience is a plus.     PySpark understanding and experience.     Experience setting up enterprise security solutions including setting up active directories, firewalls, SSL certificates, Kerberos KDC servers, etc.     Experience working with automation tools like Terraform, CI/CD, Jenkins and various test reports and coverage     Experience defining and automating the build, versioning and release processes for complex enterprise products.     Strong experience in Java/J2EE frameworks and its related technologies     Hands on experience in Spring/Spring boot     Good experience in working with Angular development     Good experience in ORM tools like Hibernate/JPA.     Good experience working with SQL/MySQL database     Agile working exposure     Good written and verbal communication skills   ",2.61E+11,26-08-2023,24-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Hibernate, Linux, JBoss, Coding, MySQL, Shell scripting, Debugging, Agile, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Support Solution Engineer,"   Demonstrated experience and proven track record of effectively managing clients in a highly technical and B2B software product and / or solution.     Experience in a fast-paced startup/scale-up environment     Experience in working with remote deployment teams         What you will be doing         Respond to support requests from customers about the Tookitaki Platform     Triage problems related to the Tookitaki Platform and the customer enterprise.     Work with Tookitaki subject matter experts, data scientists, and engineers to resolve customer issues.     Assist support management to develop support models that enable fanatical support.     Design support strategies and services to enable customers to gain optimal value from the platform.     Learn Tookitaki Platform and keep learning and creating new products, features, and techniques.     Document customer problems and resolutions     Monitor production job schedules where required and attend to failures.     Meet client SLA and communicate via Freshdesk tools.     Provide work around solutions for defects raised.     Develop a career in solution architecture.         What you won't be doing         You will not commit roadmap deliverables to clients.     You will not commit and take change requests from clients.     You will not pass customer communications to other teams.     Requirements     Understanding of shell scripting,python, scala code, Linux, Elastic search     Have understanding of Big data and hands on experience with Apache and Cloudera     Knowledge working in Mysql setup and understanding of sql     Hands on deployment and configuration of any products in previous companies     Working knowledge of monitoring & scheduler tools such as datadog, airflow etc.     Installation and configuration of Apache, Tomcat, Jboss     3-5 years of experience with big data application stack including HDFS, YARN, Spark, Hive and Hbase.     3+ years of experience in Cloud Technologies (AWS/ GCP/ Azure)     3+ years of experience in Big data Enterprise Cluster installation and product installation     2-3 years experience in ETL in a Big Data hadoop environment (Hive/Hbase knowledge).     3+ years years of experience in shell scripting or python.     4+ years of experience in Hadoop Big Data projects      Experience with?developing, tuning and debugging code, python/shell scripts loading into Hive, Mariadb, mysql.     Knowledge of python and hands-on coding experience is a plus.     PySpark understanding and experience.     Experience setting up enterprise security solutions including setting up     active directories, firewalls, SSL certificates, Kerberos KDC servers, etc.     Experience working with automation tools like Terraform, CI/CD, Jenkins and various test reports and coverage     Experience defining and automating the build, versioning and release processes for complex enterprise products.         Compensation & Job Perks         Opportunity to work for the fastest growing AML (Anti Money Laundering)     Compliance company serving the fastest and biggest economies     Flexible working hours and remote working   ",2.61E+11,26-08-2023,24-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Solution architecture, Linux, JBoss, Coding, MySQL, Shell scripting, Debugging, SSL, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Engineer,"   Develop, test, and implement Data Platform projects using GCP Components such as BigQuery, Dataflow, Dataproc, DLP, BigTable, Pub/Sub, and Compos.     Ensure data engineering and lifecycle management, including non-functional requirements and operations.     Collaborate with client teams to design and implement scalable data solutions using new and emerging technologies from the Google Cloud Platform.     Automate manual processes to improve delivery speed.     Deploy code from lower environments to production.         Candidate Qualifications:           5+ years of overall experience in developing, testing, and implementing Data Platform projects using GCP Components.     Good understanding of data structures.     Experience working with large datasets and solving complex analytical problems.     Experience using GIT for source code management.     Understanding of data pipeline (batch and streaming) and data governance.     Excellent communication skills to understand business requirements.         Required Skills:           GCP Components (e.g., BigQuery, Dataflow, Dataproc, DLP, BigTable, Pub/Sub, Compos)     Data structures     Large dataset handling and complex analytical problem-solving   ",2.61E+11,26-08-2023,24-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"GIT, GCP, Analytical, data governance, Data structures, Deployment, Management, Testing",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Backend Engineer,"   Ships high-quality code and believe in TDD     Has deep experience with one or more programming languages such as Java, Scala and Python     Has experience designing micro-services or service-oriented architectures     Has experience working with and understanding of Relational Databases     Has experience working with one or more cloud providers, preferably AWS     Enjoys digging into open-source projects and contributing back     Is well versed with tools of the trade - Git, IDEs and build tools     Has a keen desire to learn and grow          The candidate would stand out if he/she has:          Prior experience with one or more distributed NoSQL stores     Prior experience with Hadoop and Spark ecosystem          ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Backend, GIT, NoSQL, TDD, spark, SCALA, Hadoop, Programming, Open source, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data bricks,"   Maintain and support the application. Development of data ingestion pipelines. Databricks background required.          Develop and integrate software applications using suitable development methodologies and standards, applying standard architectural patterns, taking into account critical performance characteristics and security measures.          Evaluate new features and refractors existing code.          Must be willing to flex work hours accordingly to support application launches and manage production outages if necessary          Ensures to understand the requirements thoroughly and in detail and identify gaps in requirements          Ensures that detailed unit testing is done, handles negative scenarios and document the same          Work with QA and automation team.          Works on best practices and documenting the process          code merges and releases (Bitbucket)          Collaborate with Business Analysts, Architects and Senior Developers to establish the physical application framework (e.g. libraries, modules, execution environments).          Good data analysis skills                Must have following experience:          Python/PySpark          SCALA          SQL          Spark/Spark Streaming          Databricks                Preferred to have following experience:          Java/ C #          Azure          Kafka          Azure Data Factory          Big Data Tool Set          Linux    ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Automation, Data analysis, Linux, Architecture, SCALA, Flex, Application development, Unit testing, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer,"     In this position you will apply your skills to manage BI platforms those can be scalable and reliable across regions        You will continue evaluating industrial trends, establish best practices and optimize the BI to deploy analytics solutions at scale      You will be closely working with project managers, visualization developers, architects, vendor partners business unit representatives and platform teams from the different organization to establish the foundation to create value for diverse business functions such as supply chain, pricing, Industrial IOT digital factory implementation, and will impact business units that span through multiple geographic areas      Primary Responsibilities Analytics Platform Administration in tools like Alteryx (primary), Ui-Path(secondary), Collibra (good to have) AWS Cloud (good to have) - - manage the platforms EMR, EC2, Redshift, Aurora, basic tasks, connectivity, storage etc Exposure to BI Platform monitoring tool like Data Dog (primary good to have), Metaminer (good to have), Control-M (good to have) etc Provide a stable production environment that meets performance needs and scalability      Triage production systems in a timely manner including on-call support as needed and maintain audit/security      Working with data scientists, BI team and customer to plan and manage      Lead and execute BI system implementations and upgrades, license management, security, and future/additional product evaluations by working with data scientists, architects and business units Practice Root Cause Analysis to determine the scope and scale of issue impact      Create epics/stories and construct automation to prevent problem recurrence      Communicate status of production issues with both the business and the Data Insights and Analytics team in a timely manner      Proactively develop, coordinate, and provide on-going training, functional, and technical direction to the super-users (content developers)      Define and document application best practices, required standards, processes, and guidelines regarding the use and administration of the Analytics tools      Install and configure various components of Data platform and maintain their integrity Setting up the connectivity between on premise systems and aws cloud services/other data sources /targets Work closely with the database team, network team, BI application teams to make sure that all the BI platforms/applications are highly available and performing as expected      Research functionality and technical configuration within the BI systems supported by the BI Applications group and prepare recommendations for system improvements whenever possible      Maintain and monitor all BI environments including development, test, production, and disaster recovery      Vendor Interaction as per need          Must Have:          8+ years of experience in Platform Administration      3+ years of experience working as a Alteryx Admin      4+ years of experience in health check monitoring and producing user statistics and forensics      1+ years of experience in any alternate BI platform, preferably Tableau/Ui-Path      Strong experience in Linux environment shell scripting to support the hosted application      Experience in Install and upgrading various applications (Alteryx, Ui-Path, collibra etc) including end to end plan and actual execution      Experience is Server and Application Housekeeping activities      Experience setting up servers in a clustered environment      Experience with User and application security configuration Experience Troubleshooting connectivity issues      Experience setting up standard backup and recovery of contents      Experience with multiple implementations of end-to-end application server configurations , including Network, Security, and firewall knowledge      Experience setting up Single Sign-on and Kerberos authentication      Setting up new connectivity to new databases and installing new drivers Troubleshoot application errors and provide permanent fix      Proactive monitoring of the environment      Bachelor s degree in computer science, information technology, data science, data analytics, interactive media, or related field      Experience working on Agile projects and Agile methodology in general      Troubleshoot errors through system logs      Ability to design and understand data processes including loading, transformations, mapping, etc      We Value AWS cloud experience is a big plus      Knowledge about Data Virtualization technologies like Denodo is a plus      Experience working as an administrator on multiple data and analytics environment is a plus (Alteryx, Collibra, Tableau, Informatica, Business Objects, Data Dog, AWS Cloud services like EMR, Redshift, Aurora, EC2, S3, etc)    ",2.31E+11,23-11-2023,21-02-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Supply chain, Automation, Linux, Shell scripting, Disaster recovery, Network security, Informatica, Information technology, Virtualization, Firewall",-,9am-6pm,"Full Time, Permanent",Mindtrilogy,Organization,Mindtrilogy,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud DATA ATA,"Compensation & Benefit-     * Competitive salary on par with industry benchmarks.   * Long Service awards and Retirement Benefits.   * Opportunity to work on cutting-edge technologies.   * Unlimited Career Progression pathways with continual performance management   * Opportunity to work on Cutting Edge Technology Job Description:     * Minimum 8+ years of hands on experience in designing, building and    operationalizing large scale on-premises & cloud enterprise data warehouse    solutions.   * Practical hands-on experience on Azure/AWS/Google cloud services (like    Azure Data Factory, Synapse Analytics, ADLS Gen2, Key Vault, Power BI, Azure    Analysis Services, Azure Cosmos DB, Azure SQL, Azure Purview, Azure Stream    Analytics, Azure Event Hub, AWS S3, AWS Redshift, AWS Glue, AWS EMR, AWS Data    Pipeline, AWS Kinesis, AWS Step function, AWS Lambda, AWS DMS, AWS DynamoDB,    Google Dataflow, BigQuery, Google Cloud Store, Firestore, Bigtable, Dataproc,    Pub/Sub etc.)   * Proven experience with the following areas of DW/BI: ETL, Data Analysis,    Data Modelling/Architecture, Reporting, DW Quality Assurance, Master & Metadata    Management, Data Governance, Data Lineage, Big Data, Data Security, Performance    Tuning, BI/ DW Solution Architecture and BI/DW infrastructure.   * Hands-on experience with modern enterprise data architectures and data    toolsets (ex: data warehouse, data marts, data lake, data lakehouse, 3NF, data    vault, NoSQL and dimensional models, modelling tools, profiling tools)   * Experience with Big Data technologies such as Hadoop/Hive/Spark/Databricks.    * Good knowledge of designing and building data pipelines from ingestion to    consumption within a big data architecture using Java / Python / Scala.   * Proven experience in analysing, re-architecting and migrating on premise    data warehouses to data platforms on Azure/AWS/Google cloud using cloud native    or 3rd party services.   * Excellent problem-solving skills, strong analytical skills with    demonstrated ability to analyse and make recommendations on solutions.   * Ability to learn and adapt quickly as well as ability to conceptualize and    articulate ideas clearly and concisely.   * Excellent communication, presentation and interpersonal skills with    experience of successfully winning over audiences with compelling and    persuasive presentations (Pre-sales). Salary: Best in the industry     Location: Pune/Remote/Hybrid",2.31E+11,23-09-2023,22-12-2023,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Performance tuning, Data analysis, metadata, Manager Quality Assurance, Performance management, Agile, Presales, Dms, Analytics, Python",-,9am-6pm,"Full Time, Permanent",Bitwise Solutions,Organization,Bitwise Solutions,https://img.naukimg.com/logo_images/groups/v1/2755446.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer - BigQuery,"Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have Skills : Google BigQuery Good to Have Skills : No Technology Specialization Job Requirements : Key Responsibilities : 1: Assists with the data platform blueprint and design, encompassing the relevant data platform components. 2: Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. 3: The Data Engineer performs tasks such as data modeling, data pipeline build, data lake build, scalable programming frameworks Technical Experience : 1: Should have exp in Data Engineering Skills like - Data Vault Modeling,Database Architecture,Data Architecture Principles,Technology Architecture Blueprint ,Roadmap Definition 2: Should have strong knowledge in SQL, Python,Apache Airflow,Data Architecture,Data Modeling,Machine Learning 3: Google Cloud Platform stack experience : BigQuery,Composer,Dataproc,Dataflow,Cloud Storage, Cloud SQL,Pub,Sub Professional Attributes : 1: Good communication 2: Good Leadership skills and team handling skills 3: Analytical skills, presentation skills, ability to work under pressure 4: Should be able to work in shifts whenever required",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Accounting / Auditing,"BigQuery, Cloud Storage, Cloud SQL, Apache Airflow, Data Engineering, Data Platform Engineer, Data Architecture, Data Modeling, Dataflow, Machine Learning, Python",-,9am-6pm,"Full Time, Permanent",Edgesoft,Organization,Edgesoft,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Qlik SaaS,"   Collaborate with stakeholders to understand data requirements and objectives.  Design and develop interactive dashboards and reports using data visualization tools such as Qlik SaaS and Looker.  Create visually appealing charts, graphs, and infographics to communicate complex data in a clear and concise manner.  Conduct dashboard reviews to ensure accuracy and consistency of visualizations.  Demonstrate dashboard capabilities to key stakeholders through written reports and presentations.  Stay current on industry trends and best practices in data visualization and analytics.",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"GCP, QlikSaas, Qlik Sense Development, Looker",-,9am-6pm,"Full Time, Permanent",Kezan India,Organization,Kezan India,-,Chennai,Chennai,-,-,-,10-18 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
MLOps Engineer,"       Design, develop, and maintain scalable ML pipelines for model training, deployment, and monitoring         Collaborate with data scientists, machine learning engineers, and software developers to understand model requirements and implement production-ready solutions         Implement continuous integration and continuous deployment (CI/CD) processes for machine learning models         Develop and implement data versioning, model versioning, and model performance tracking mechanisms         Optimize and automate infrastructure for ML experimentation, model training, and inference         Ensure the security, scalability, and reliability of ML systems in production         Monitor and troubleshoot production ML systems to ensure high availability and performance         Stay up to date with the latest trends and technologies in MLOps, and identify opportunities for improvement and innovation             Requirements:             Bachelors or Masters degree in Computer Science, Engineering, or a related field         3+ years of experience in machine learning, software engineering, or DevOps, with a focus on MLOps         Strong programming skills in languages such as Python, Java, or Scala         Proficiency in ML frameworks such as TensorFlow, PyTorch, or scikit-learn         Experience with cloud platforms such as AWS, Azure, or Google Cloud         Strong understanding of DevOps principles and CI/CD processes         Familiarity with containerization technologies such as Docker and orchestration tools like Kubernetes         Experience with data engineering tools and frameworks (e.g., Apache Spark, Apache Airflow)         Excellent problem-solving and analytical skills         Strong communication and collaboration skills       ",2.11E+11,21-08-2023,19-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Analytical skills, orchestration, data science, spark, devops, Machine learning, Programming, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Caminosoft Ai,Organization,Caminosoft Ai,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Ai/ Ml Engineer,   JD 7 Years of experience in Python AI/ML Gen AIWork on Data     Ingestion to Graph DBWork on AI/GenAI models for given use cases     Deploy and Test LLMs in Linux and Cloud environment      Experience Required    ,1.90E+11,19-03-2024,17-06-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"Linux, Cloud, Deployment, Python, Testing",-,9am-6pm,"Full Time, Permanent",Nityo Infotech,Organization,Nityo Infotech,https://img.naukimg.com/logo_images/groups/v1/76234.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Cloud Engineer,"       Should have min. 5 years of hands-on experience                  In the AWS cloud at least with S3, EC2, MSK, Glue, DMS and Sagemaker. Development/work experience in Python, Docker containerizing.                  Should be troubleshooting the problem, reviewing the design, and coding the solution. An AWS-certified candidate is preferred.                  should have a minimum of 5 years of hands-on experience in data engineering, which includes data model defining, creating, debugging, and reviewing.                  Should be able to write good coding for the database, such as function, procedure, and other components.                  Should have high proficiency in the Snowflake database.                  Should have a strong foundation of database concepts and work experience in OLTP and OLAP.        ",1.90E+11,19-02-2024,19-05-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Coding, Data modeling, Debugging, Cloud, OLAP, Dms, Troubleshooting, OLTP, AWS, Python",-,9am-6pm,"Full Time, Permanent",First Soft Solutions,Organization,First Soft Solutions,-,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Redshift_CBS professional,"   As an AWS Redshift_CBS, you will be responsible for:         Designing and building scalable, extendable, and maintainable data services using modern cloud data architecture principles     Troubleshooting and analyzing performance impact of AWS data services     Working with AWS Redshift, DynamoDB, Aurora, Kinesis, Spark streaming, Spark SQL, Hadoop, RedshiftSnowflake, Kafka, and Clear water Data Lake build out     Utilizing AWS core services such as EC2, S3, Redshift, EMR, ECR, CloudWatch, CloudFormation, DynamoDB, SQS, SNS, Lambda, Step Functions, API Gateway, and IAM     Experience with Kinesis and other AWS streaming techniques           Candidate Qualifications:     The ideal candidate should possess the following qualifications:         Hands-on experience in AWS Redshift, AWS SES, Python Spark programming, APImicroservice architecture, and Data model architecture     Strong problem-solving and analytical thinking abilities to address complex AWS data related technical challenges     Working experience on Hadoop, Spark & Kafka Environment     Working experience with Kafka and other streaming sources     Knowledge of AWS Data Architecture patterns           Required Skills:     The successful candidate should have expertise in the following skills:         AWS Redshift     DynamoDB     Aurora     Kinesis     Spark streaming   ",1.81E+11,18-09-2023,17-12-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, spark, Data modeling, Analytical, Hadoop, Troubleshooting, AWS, SQL, Python, Data architecture",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,New Delhi,New Delhi,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Backend Engineer,"  If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.  Please click on the link to apply to our Career Page: https://us241.dayforcehcm.com/CandidatePortal/en-US/acv/Site/IndiaCandidatePortal/Posting/View/12699 Who we are:    ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with datasolutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of theindustry and we want you to join us on our journey. ACVs network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.  ACV Auctions is opening its new India Development Center in Chennai, India, and were looking for talented individuals to join our team. As we expand our platform, were offering a wide range of exciting opportunities across various roles. At ACV, we put people first and believe in the principles of trust and transparency. If you are looking for an opportunity to work with the best minds in the industry and solve unique business and technology problems? Look no further!  Join us in shaping the future of the automotive marketplace! At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer industry leading benefits and wellness programs. Who are we looking for: ACV is looking for a Software Engineer with Python backend development experience. You will play a critical role in the design, development, optimization, and maintenance of many critical applications and services (Python, FastAPI, Vue.js, Kafka, AWS, GCP, etc.). You will work with a fast-moving, multidisciplinary and distributed team to deliver high-quality, robust, and scalable solutions that will empower our customers and deliver the best experience for them.  What you will do: Actively and consistently support all efforts to simplify and enhance the customer experience. Lead the architectural design and technical decision-making process, ensuring alignment with product goals and engineering standards. Collaborate with product managers, designers, and other engineers to define requirements and translate them into technical specifications. Mentor and guide junior engineers, providing technical expertise, code reviews, and best practices to foster professional growth. Hold software development to high technical standards. Help onboard new team members, or developers across other teams while providing technical expertise, code reviews, and best practices to foster professional growth. Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required. Ensure that new development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema designs. Collaborate and partner with other platform and product teams to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products. Architect and build entire services, including but not limited to: customer onboarding, leveraging risk models, developing audit solutions, integrating and supporting various services through the use of message brokers and interfaces. Perform additional duties as assigned Work in office, 3 days a week is required What you will need: BS in Computer Science or a related technical discipline, or equivalent practical experience  5-15 years relevant experience Ability to read, write, speak and understand English.  Deep understanding of modern tech stacks Strong experience scaling applications and services with high-traffic/high-availability requirements  Strong experience with Python, FastAPI, Kafka, AWS, Kubernetes, Vue/React   (similar experience in other languages/stacks may be acceptable) Experience working in distributed teams and delivering high-quality projects on time Great written and verbal communication skills Experience using containerization technologies (Docker, Kubernetes, containerd) Some experience with cloud services (AWS/GCP)   Our Values  Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.  ",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Software Product,"Strong experience with Python, Vue/React, Kafka, FastAPI, AWS, Kubernetes",-,9am-6pm,"Full Time, Permanent",ACV Auto Auctions India,Organization,ACV Auto Auctions India,https://www.naukri.com/hotjobs/images/v3/acvauctions_apr24.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Viz Engineer,"     We are seeking an Viz Engineer with 2+ years of cloud analytics experience to join our team of data experts in building innovative AI technology.          At Syntasa, we leverage big data, machine learning, and real-time streaming to turn billions of records into actionable insights, driving improvements in customer acquisition, conversion, and retention.                    Responsibilities:                    Design and develop data solutions for data intake, preparation, and analysis          Apply statistical models and data science techniques to deliver business-impactful analytics          Create flexible tools, dashboards, and reports to support data-driven decision-making          Develop clear and impactful visualizations to communicate analytical findings to stakeholders            Qualifications:            Bachelor's degree in Computer Science, Statistics, Business Analytics, or a related field (Masters degree preferred)          2+ years of experience with cloud analytics (AWS, GCP, or Azure)          Strong programming skills in Python/Scala and experience with SQL and BigQuery          Knowledge of machine learning libraries such as Pandas/Scikit          Familiarity with BI tools such as Data Studio, PowerBI, or Looker      ",1.80E+11,18-03-2024,16-06-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Software Product,"Computer science, Customer acquisition, GCP, Business analytics, Analytical, Machine learning, Programming, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Syntasa,Organization,Syntasa,-,"Noida, Pune, Gurugram","Noida, Pune, Gurugram",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Solutions Engineer,"     We are looking for a SingleStore Solutions Engineer who is passionate about removing data bottlenecks for their customers and enabling real-time data capabilities to some of the most difficult data challenges in the industry      In this role, you will work directly with our sales teams, and channel partners to identify prospective and current customer pain points where SingleStore can remove those bottlenecks and deliver real-time capabilities      You will provide value-based demonstrations, presentations, and support proof of concepts to validate proposed solutions      As a SingleStore solutions engineer, you must share our passion for real-time data, fast analytics, and simplified data architecture      You must be comfortable in both high executive conversations and being able to deeply understand the technology and its value-proposition                    Responsibilities              Engage with both current and prospective clients to understand their technical and business challenges      Present and demonstrate SingleStore product offering to Fortune 500 companies.      Enthusiastic about the data analytics and data engineering landscape      Provide valuable feedback to product teams based on client interactions      Stay up to date with database technologies and the SingleStore product offerings                Qualifications              Minimum of 3 years experience in a technical role      Excellent presentation and communication skills, with experience presenting to large corporate organizations      Ability to communicate complex technical concepts to non-technical audiences.      Strong team player with interpersonal skills      Broad range of experience within large-scale database and/or data warehousing technologies      Experience with data engineering tools Apache Spark, Apache Flink, Apache Airflow      Demonstrated proficiency in ANSI SQL query languages      Demonstrated proficiency in Python, Scala, or Java      Understanding of private and public cloud platforms such as AWS, Azure, GCP, VMware    ",1.51E+11,15-12-2023,14-03-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Product management, VMware, SAN, SQL database, GCP, ANSI, Analytics, Recruitment, Python, Data architecture",-,9am-6pm,"Full Time, Permanent",Singlestore,Organization,Singlestore,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Engineer (GCP),"         We are seeking a skilled and experienced Cloud Engineer to join our dynamic team      As a Cloud Engineer, you will play a crucial role in implementing and managing cloud architectures for our clients software applications      Your strong expertise in Google Cloud Platform (GCP) implementations, programming languages, and cloud ecosystem design will contribute to the success of our cloud-based solutions                       We are offering a highly competitive salary commensurate with industry standards.                    Minimum Qualifications:            - Demonstrated experience in implementing cloud architecture for software applications.            - Extensive expertise in GCP implementations.            - Proficiency in at least one of the programming/scripting languages.            - Proficient in using Linux CLI commands and Google Cloud SDK.            - Ability to design holistic cloud ecosystems with a focus on Google Cloud Platform capabilities and features.            - Familiarity with Cloud Shell and GCP commands such as gcloud and gsutil.            - Hands-on experience with GCP IaaS services such as GCE, GAE, GKE, VPC, DNS, Interconnect VPN, CDN, Cloud Storage, FileStore, Firebase, Deployment Manager, and Stackdriver.            - Familiarity with GCP services including Cloud Endpoints, Dataflow, Dataproc, Datalab, Dataprep, Cloud Composer, Pub/Sub, and Cloud Functions.                  Responsibilities:            - Troubleshoot issues, actively seeking out problems, and providing effective solutions.            - Implementing HA and DR solutions            - Be an active participant in the running of the team, fostering a great place to work.            - Engage with the wider business to identify opportunities for future work for the team.            - Experiment with new technologies to help push the boundaries of what the team is building.                            Requirements        - Professional certifications related to cloud platforms, specifically Google Cloud Platform.            - Experience with other cloud platforms such as AWS or Azure.            - Knowledge of containerization technologies (e.g., Docker, Kubernetes).            - Familiarity with DevOps practices and tools.            - Understanding basic network and security principles in cloud environments.            - Experience with automation and infrastructure-as-code tools, preferably terraform            - Excellent problem-solving and analytical skills.            - Strong communication and collaboration abilities.                    Benefits        Health Insurance for a worry-free lifestyle.            Flexible work hours for better work-life balance.            Informal dress code to express your individuality.            Enjoy a 5-day work week to pursue your passions outside of work.            Indulge in free snacks and beverages for those energy boosts!          ",1.51E+11,15-09-2023,14-12-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SAN, Automation, Linux, GCP, VPN, SOC, SMS, DNS, SDK, CRM",-,9am-6pm,"Full Time, Permanent",Transcloud Labs,Organization,Transcloud Labs,-,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Cloud (Sr. Engg),"HexaQuESt Global - India  is looking for Azure Cloud (Sr. Engg)  to join our dynamic team and embark on a rewarding career journey    you will be responsible for the design, implementation, and optimization of cloud solutions on the Microsoft Azure platform     This role involves hands-on technical expertise, collaboration with cross-functional teams, and ensuring the effective use of Azure services to meet business requirements       Key Responsibilities:       Azure Cloud Implementation:Design, implement, and maintain cloud infrastructure components on Microsoft Azure, including Virtual Machines, Storage, Networking, and Identity services     Collaborate with solution architects to translate business requirements into scalable and secure Azure solutions     Azure Services Integration:Integrate and optimize various Azure services, such as Azure App Services, Azure SQL Database, Azure Functions, and others, to support application needs     Leverage Azure PaaS services to enhance application scalability and efficiency     Automation and Scripting:Implement Infrastructure as Code (IaC) using tools like Azure Resource Manager templates, Terraform, or other automation tools     Develop and maintain automation scripts (PowerShell, Python, etc) for provisioning and managing Azure resources     Security and Compliance:Implement and enforce security best practices for Azure solutions, ensuring compliance with security policies and industry standards     Collaborate with security teams to address vulnerabilities and implement security measures     Performance Optimization:Optimize Azure resources for performance, cost-effectiveness, and scalability     Monitor and analyze Azure resource usage, identifying opportunities for improvement     Monitoring and Logging:Implement monitoring solutions for Azure resources to proactively identify and address issues     Set up logging and alerting mechanisms for effective troubleshooting     Collaboration:Collaborate with development, operations, and other cross-functional teams to align Azure cloud initiatives with overall business goals     Participate in Agile/Scrum processes and contribute to sprint planning and retrospectives   ",1.40E+11,14-02-2024,14-05-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, iac, azure devops, sql, docker, azure app service, azure functions, sql azure, git, devops, powershell, paas, jenkins, cloud computing, python, azure paas, microsoft azure, azure cloud, virtual machines, scrum, cloud infrastructure, terraform, agile, iaas, aws",-,9am-6pm,"Full Time, Permanent",Hexaquest Global,Organization,Hexaquest Global,https://img.naukimg.com/logo_images/groups/v1/6010528.gif,Mysuru,Mysuru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Big Data -Database Professionals," Global Pharma Tek is looking for Big Data -Database to join our dynamic team and embark on a rewarding career journey        you will be responsible for managing and optimizing large-scale databases within a Big Data environment      This role involves designing, implementing, and maintaining robust database architectures, ensuring data integrity, and collaborating with other teams to support data-driven initiatives        Key Responsibilities:        Database Design:Design, implement, and maintain scalable and efficient database architectures for Big Data platforms      Collaborate with data architects to ensure database designs align with business and data requirements      Database Administration:Perform day-to-day administration of Big Data databases, including monitoring, backup and recovery, and access controls      Implement and enforce security measures to protect sensitive data      Data Modeling:Develop and maintain data models for Big Data databases, ensuring consistency and optimal performance      Collaborate with data scientists and analysts to understand data modeling requirements      Performance Optimization:Optimize database performance by tuning queries, indexing, and implementing caching strategies      Conduct regular performance assessments and address bottlenecks      Data Integration:Integrate data from various sources into Big Data databases, ensuring proper data flow and transformation      Collaborate with ETL developers and data engineers for seamless data integration      Backup and Recovery:Develop and implement robust backup and recovery strategies to safeguard critical data      Conduct regular tests to ensure data recoverability      Collaboration:Work closely with data engineers, analysts, and other stakeholders to understand data requirements and support data-driven initiatives      Provide technical support and guidance to cross-functional teams    ",1.30E+11,13-02-2024,13-05-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"modeling, indexing, performance tuning, engine calibration, transformation, database administration, engine testing, sql server, sql, database design, data modeling, integration, performance optimization, etl, big data, data integration, architecture",-,9am-6pm,"Full Time, Permanent",Global Pharma Tek,Organization,Global Pharma Tek,https://img.naukimg.com/logo_images/groups/v1/1012164.gif,Vadodara,Vadodara,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer,"   You are a dynamic individual who can manage their own workload, understands customer communication and delivering a quality service for our user base     Has good inter-personal skills     Is well organised and practical, with a logical, analytical approach to problem solving     Pays careful, close attention to detail     Is skilled in installing one or more items of hardware and software         Day-to-day, you will:         Installation/Maintain the ESX Infrastructure      Installation/Maintain/Patch VMWare Hypervisor      Maintain Storage for the VMWare estate      Manage the IaaS service for customers      Ensure the replication software is functioning      Perform Upgrades where applicable      Work on assigned Service Requests and Incidents within the documented SLA s      Work within an ITIL framework to reduce customer impacting Incidents      Review workload to drive efficiency, move processes and technology towards automation and self-service      Be part of the On-Call support rota             Your skills and experiences might also include:          Managing an enterprise sized VMWare estate      Experience of Active Directory management      Capability to produce/amend scripts      Must be self-motivated and passionate about service delivery to a high standard.      Ability to learn new and varied skills within the Access Group support environment.      Ability to coach other members of staff and impart knowledge in an effective manner      Ability to work well under pressure and within given timescales.      To work within a team environment.      Be able to work outside core working hours when required to apply important updates      Be on call in rotation to handle priority 1 incidents if they occur     ",1.00E+11,10-04-2024,09-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Software Product,"Service delivery, VMware, Hospitality, Automation, Interpersonal skills, Software installation, Analytical, hypervisor, Active directory, Service quality",-,9am-6pm,"Full Time, Permanent",Hackajob,Organization,Hackajob,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Journeyman Data Ingest Platform Engineer," The    Data Ingest Platform Engineer    responsibilities include conducting full development lifecycle of data that includes requirements from DHS, other OMB initiatives, and provide support for the whole program. This position also requires building a new data automation practice on the program to address our client s most pressing needs with Cyber Security Threats and Data. The successful candidate will bring a consultative approach to data to improve the value of the data that s being collected by our customers. This position is also a thought leader in the practice of Big Data in solving our clients cyber security problems, coupled with demonstrated experience designing and developing enterprise data solutions for large clients by providing a new approach to the team, presenting white papers and other solutions.           Responsibilities include, but are not limited to:         Design, implement, and support a highly available and fault tolerant distributed Cribl LogStream architecture     Develop standards and governance policies for management of the extended Cribl architecture     Develop disaster recovery plan and implement cloud-native capabilities to ensure the Cribl-based Data Ingest component remains available at all times     Develop automated deployments that support IaaS, container, and Kubernetes     Develop and maintain tool integration packages consisting of a number of endpoint types to include REST APIs, Database connections, and proprietary connections like Splunk or ServiceNow.     Manage a centralized and curated registry of pre-packaged tool integrations ready for Agency consumption.     Develop data processing strategies to ensure efficient collection, aggregation, and transport of relevant cybersecurity data.             Basic Qualifications:         Bachelors Degree complete or in progress preferably in applied mathematics, statistics, computer science, data science, electrical engineering, physics, or closely related field     A minimum of (6) six years of overall experience     Familiar with JavaSrcript     Experience with JSON parsing and YAML     Experience interacting with RESTful APIs     Experience with scripting languages like Python, Bash, and PowerShell     Experience with TLS/SSL to secure data in transit.     Experience collaborating with US Government Agencies, state or local governments, or commercial entities to develop IT service program maturity in accordance with Federal IT mandates and best practices.             Preferred Qualifications:         Demonstrated ability to investigate data and present findings to internal teammates and client audiences.     Experience in conducting assessments of an Enterprise by reviewing technical documentation, conducting interviews and workshops to identify gaps and develop a tailored solution is highly desired.             Clearance Requirements:         Must be a US citizen and pass a background investigation.     Able to obtain and maintain a DHS Suitability/Entry on Duty (EOD)   ",1.00E+11,10-04-2024,09-07-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Software Product,"Automation, Powershell, Disaster recovery, Data processing, splunk, JSON, SSL, Python, Technical documentation",-,9am-6pm,"Full Time, Permanent",Hackajob,Organization,Hackajob,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Programmer (IT) Professionals,"   Coders Brain Pvt Ltd is looking for - Programmer (IT) 5 to 7 Years Pune_Varsha to join our dynamic team and embark on a rewarding career journey      Develop software applications based on business requirements and technical specifications      Write clean, efficient, and maintainable code      Test and debug software applications to ensure they meet quality standards      Collaborate with cross-functional teams to design, develop, and deploy new features      Troubleshoot and resolve software defects and issues      Optimize software performance and ensure scalability      Conduct code reviews and provide constructive feedback to team members      Stay up-to-date with industry trends and emerging technologies      Document software designs, code, and technical specifications      Participate in agile development processes and contribute to continuous improvement        ",61223501641,06-12-2023,05-03-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Quality standards, Scalability, Agile development, Debugging, Deployment, Continuous improvement, Application software, Troubleshooting, Testing",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Universal Windows Platform (UWP) Engineer,"           Must    have strong working experience with UWP.       Must    have strong knowledge of C#/WPF/.NET     Minimum 3 years experience in Windows desktop applications using the .NET technology stack and WPF, C#.     Experience in design, development, and support in the latest DOT NET technologies     Ability to design, develop, and maintain high-performance and high-availability systems.     Strong technical expertise in WPF/XAML     Experience using MVC, MVVM architecture     Experience in developing and consuming RESTful web services     Expertise in using object-oriented concepts     Experience creating architectural design documents and other technical documentation         ?     ",60923501122,06-09-2023,05-12-2023,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Web services, MVVM, Architectural design, Design development, .Net, Windows, MVC, WPF, Technical documentation",-,9am-6pm,"Full Time, Permanent",Finoit Technologies,Organization,Finoit Technologies,https://img.naukimg.com/logo_images/groups/v1/4594973.gif,remote,remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Tech Lead - Cloud Migration Engineer,"   Lead a team of software developers and provide technical direction and guidance      Collaborate with project managers, product managers, and other stakeholders to define, prioritize and plan technical projects      Design, develop and implement complex software systems, ensuring high quality and timely delivery      Mentor and coach team members, providing technical and professional development opportunities      Participate in the requirements understanding, analysis and implementation      Test and deploy applications and systems      Debugging and Bug fix in existing code      Develop documentation throughout the software development life cycle (SDLC)    ",31023500525,03-10-2023,01-01-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Debugging, Cloud, Software development life cycle, Technical Lead, Mentor, Deployment, SDLC, Testing",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : as per accenture standards Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements.  Must have Skills :DevOps, SSI: NON SSI:Good to Have Skills :SSI:No Function Specialization NON SSI :Job Requirements :Key Responsibilities :Dev/Ops Container consultantTerrafrom Scripting, GCP deployments Technical Experience :Experience in scripting languages, eg PowerShell, Bash or PythonExperience with Docker and Kubernetes Knowledge of any of main Public Cloud platforms Azure, GCP, AWS Professional Attributes :Good Communication and leadership skill Educational Qualification:as per accenture standardsAdditional Info : Qualification as per accenture standards",10524910295,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, ssi, microsoft azure, devops, powershell, python, investment banking, trade settlements, capital market, client onboarding, javascript, reconciliation, docker, application development, sql, derivatives, java, kyc, gcp, static data, bash, mysql, html, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Cloud Infrastructure Good to have skills : NA Minimum  18  year(s) of experience is required Educational Qualification : 15 full years of education  Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Cloud Infrastructure. Your typical day will involve working with various cloud technologies and collaborating with cross-functional teams to deliver high-quality solutions.  Roles & Responsibilities: Design, develop, and maintain cloud-based applications using Cloud Infrastructure. Collaborate with cross-functional teams to identify and prioritize application requirements. Configure and deploy applications to meet business process and application requirements. Ensure the scalability, availability, and security of cloud-based applications. Stay updated with the latest advancements in cloud technologies and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Proficiency in Cloud Infrastructure. Good To Have Skills:Experience with DevOps tools like Jenkins, Ansible, and Docker. Strong understanding of cloud-based application development and deployment. Experience with cloud-based databases like AWS RDS, Azure SQL, and Google Cloud SQL. Experience with cloud-based storage solutions like AWS S3, Azure Blob Storage, and Google Cloud Storage. Additional Information: The candidate should have a minimum of 18 years of experience in Cloud Infrastructure. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality cloud-based solutions. This position is based at our Pune office. Qualification 15 full years of education",10524909730,01-05-2024,30-07-2024,EducationalOccupationalCredential,216,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"docker, ansible, azure blob storage, jenkins, cloud infrastructure, c#, cloud sql, python, google, amazon rds, sql server, javascript, application development, sql, microservices, plsql, spring, sql azure, java, devops, google analytics, mysql, html, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Senior Software Developer - Python, API, AWS","           ?         Ability to understand complex architecture design         Deliver solutions within a multi-functional Agile team         Develop expertise in our proprietary enterprise software products         Set and maintain a level of excitement in using various technologies to develop, support, and iteratively deploy real enterprise level software         Achieve an understanding of customer environments and their use of the products         Adhere apply software engineering practices and implement automation across all elements of solution delivery         Technical Leads will be expected to build solutions, architecture, algorithms, and designs which scale to the customers enterprise/global requirements                 What we are looking for.             Basic Qualifications   :         Bachelors degree in Computer Science or Equivalent     4+ years related experience     Passionate, smart, and articulate developer     Strong Python/PySpark and SQL Skills     Exposure to AWS services with strong hands on experience     Exposure to Data Engineering Big Data technologies like Hadoop, Spark/Scala, Airflow ETL     Experience in implementing REST APIs using Python as Flask and Django     Able to demonstrate strong OOP skills     Able to work well individually and with a team     Strong problem-solving skills     Good work ethic, self-starter, and results-oriented     Interest and experience in Environmental and Sustainability content is a plus     Agile/Scrum experience a plus               Preferred Qualifications     :       Experience on Docker is a plus     Experience on Databricks is a plus     Experience with large scale messaging systems such as Kafka or RabbitMQ or commercial systems.         ",3.00E+11,30-01-2024,29-04-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Financial Services,"Supply chain, Automation, Equity, HTTP, Automotive, Analytics, SQL",-,9am-6pm,"Full Time, Permanent",Osttra,Organization,Osttra,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Java Developer,"     Develop detailed design specifications, unit test plans, and high-quality code     Work with a team of talented software engineers through the entire development cycle, from design through deployment and monitoring,     Work with Architecture, Development, QA, and other engineering groups to define, build, and evolve into a world-class software development organization,     Provide on-going product maintenance and enhancements to production applications,Proactively identify opportunities for improvement           Expertise Qualification          Experience with Cloud technologies like AWS, docker,,Experience with messaging systems like Kafka, RabbitMQ,Working knowledge of Python/GO/shell     ",3.00E+11,30-04-2024,29-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"spring boot, Tomcat, NoSQL, Architecture, Cloud, Deployment, Unit testing, AWS, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Nityo Infotech,Organization,Nityo Infotech,https://img.naukimg.com/logo_images/groups/v1/76234.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Developer,"   Responsible Business Leadership,Diversity Inclusion,excel,Azure Data factory,Azure Synapse,Data Bricks,Data Flows,HDInsight,Azure Analysis,Data Management,Data Modelling        Roles Responsibilities          The team is focused on developing a broader portfolio with solutions for Risk Consulting, Management Consulting, Technology Consulting, Strategy Consulting, Forensics as well as vertical specific solutions,      You will collaborate with and receive support from a network of people to achieve your goals,      We will also provide you with global leadership development frameworks and the latest in digital technologies to learn and excel in your career,Apply to us if you believe PwC is the place to be      ,The candidate should have strong experience in Azure cloud using Storage services (Blob, ADLS, PaaS SQL),,Strong in planning and organization skills,lead on an agile team and provide automated cloud solutions,      This content is for general information purposes only and should not be used as a substitute for consultation with professional advisors            Expertise Qualification          Experience in Hadoop and Spark is mandatory Should have expertise in extracting data from different source systems like flat files, XML sources, JSON, Parquet files, Big data appliances, RDBMS, etc,          Good knowledge of Big Data frameworks and related technologies,          Experience in Hadoop and Spark is mandatory          Should have clear understanding of DW lifecycle and contributed in preparing          Detail Level Design document Unit Test plans          Code review reports          Must have strong sql, understand joins, groups, functions Stored procedures          Must have experience in Unit and Integration Test creation and verification          Proficient in creating documentation related to Project Architecture, Project end User Manual and Operations Hand off guide          Should have good understanding of Data Migration processes, methods and project lifecycle          Good in Unix Shell Scripting skills Strong analytical skills and troubleshooting skills          Should be well versed with quality processes and implementation          Excellent communication skills          Strong understanding of Cloud data migration processes, methods and project lifecycle          Good knowledge of Application DevOps tools (Git, CI/CD Frameworks,Experience in Jenkins or Gitlab with rich experience in source code management like Code Pipeline, Code Build and Code Commit Strong Experience in data conversion projects          Desired Knowledge / Skills          Experience in building stream-processing systems, using solutions such as Storm or Spark-Streaming Experience in Azure functions          Experience in Power Bi reporting tool Knowledge in Python Professional and Educational Background        ",3.00E+11,30-04-2024,29-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"Data migration, RDBMS, XML, Agile, JSON, Stored procedures, Troubleshooting, Unix shell scripting, SQL, Python",-,9am-6pm,"Full Time, Permanent",Nityo Infotech,Organization,Nityo Infotech,https://img.naukimg.com/logo_images/groups/v1/76234.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Aws Developer,"   Responsible Business Leadership,Diversity Inclusion,excel,Enterprise Data lake,Data hub,AWS cloud technology,Amazon Web Services,AWS        Roles Responsibilities          The team is focused on developing a broader portfolio with solutions for Risk Consulting, Management Consulting, Technology Consulting, Strategy Consulting, Forensics as well as vertical specific solutions     You will collaborate with and receive support from a network of people to achieve your goals,     We will also provide you with global leadership development frameworks and the latest in digital technologies to learn and excel in your career,     Apply to us if you believe PwC is the place to be,Strong in planning and organization skills,lead on an agile team and provide automated cloud solutions,     This content is for general information purposes only and should not be used as a substitute for consultation with professional advisors           Expertise Qualification          Candidates with 2-,Experience in architecting and delivering highly scalable, distributed, cloud-based enterprise data solutions          Strong expertise in end-to-end implementation of Cloud data engineering solutions like Enterprise Data lake, Data hub in AWS          Proficient in Lambda or Kappa Architectures          Should be aware of Data Management concepts and Data Modelling          Strong AWS hands-on expertise with a programming background preferably Python/Scala          Good knowledge of Big Data frameworks and related technologies,         Experience in Hadoop and Spark is mandatory Strong experience in AWS compute services like AWS EMR, Glue and storage services like S3, Redshift Dynamodb          Good experience with any one of the AWS Streaming Services like AWS Kinesis, AWS SQS and AWS MSK Troubleshooting and Performance tuning experience in Spark framework - Spark core, Sql and Spark Streaming          Good knowledge of Application DevOps tools (Git, CI/CD Frameworks,Experience in Jenkins or Gitlab with rich experience in source code management like Code Pipeline, Code Build and Code Commit Experience with AWS CloudWatch, AWS Cloud Trail, AWS Account Config, AWS Config Rules          Good knowledge in AWS Security and AWS Key management          Strong understanding of Cloud data migration processes, methods and project lifecycle Good analytical problem-solving skills          Good communication and presentation skills Desired Knowledge / Skills,         Experience in building stream-processing systems, using solutions such as Storm or Spark-Streaming Experience in Big Data ML toolkits, such as Mahout, SparkML, or H2O          Knowledge in Python Worked in Offshore / Onsite Engagements         Experience in one of the flow tools like Airflow, Nifi or Luigi         Experience in AWS services like STEP Lambda Experience in Azure Cloud Service Professional and Educationa     ",3.00E+11,30-04-2024,29-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"Performance tuning, Leadership development, Data migration, Data management, Analytical, Management consulting, Agile, Troubleshooting, SQL, Python",-,9am-6pm,"Full Time, Permanent",Nityo Infotech,Organization,Nityo Infotech,https://img.naukimg.com/logo_images/groups/v1/76234.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Aws Developer,"     Responsible Business Leadership,Diversity Inclusion,excel,Enterprise Data lake,Data hub,AWS cloud technology,Amazon Web Services,AWS      Roles Responsibilities        The team is focused on developing a broader portfolio with solutions for Risk Consulting, Management Consulting, Technology Consulting, Strategy Consulting, Forensics as well as vertical specific solutions,         You will collaborate with and receive support from a network of people to achieve your goals         We will also provide you with global leadership development frameworks and the latest in digital technologies to learn and excel in your career         Apply to us if you believe PwC is the place to be       Strong in planning and organization skills,lead on an agile team and provide automated cloud solutions    This content is for general information purposes only and should not be used as a substitute for consultation with professional advisors           Expertise Qualification        Candidates with 2-,Experience in architecting and delivering highly scalable, distributed, cloud-based enterprise data solutions          Strong expertise in end-to-end implementation of Cloud data engineering solutions like Enterprise Data lake, Data hub in AWS Proficient in Lambda or Kappa Architectures          Should be aware of Data Management concepts and Data Modelling Strong AWS hands-on expertise with a programming background preferably Python/Scala          Good knowledge of Big Data frameworks and related technologies,Experience in Hadoop and Spark is mandatory          Strong experience in AWS compute services like AWS EMR, Glue and storage services like S3, Redshift Dynamodb          Good experience with any one of the AWS Streaming Services like AWS Kinesis, AWS SQS and AWS MSK Troubleshooting and         Performance tuning experience in Spark framework - Spark core, Sql and Spark          Streaming Good knowledge of Application DevOps tools (Git, CI/CD Frameworks,Experience in Jenkins or Gitlab with rich experience in source code management like Code Pipeline, Code Build and Code Commit Experience with AWS CloudWatch, AWS Cloud Trail, AWS Account Config, AWS Config Rules Good knowledge in AWS Security and AWS          Key management Strong understanding of Cloud data migration processes, methods and project lifecycle          Good analytical problem-solving skills Good communication and presentation skills          Desired Knowledge / Skills,         Experience in building stream-processing systems, using solutions such as Storm or Spark-Streaming          Experience in Big Data ML toolkits, such as Mahout, SparkML, or H2O          Knowledge in Python Worked in Offshore / Onsite          Engagements Experience in one of the flow tools like Airflow, Nifi or Luigi          Experience in AWS services like STEP Lambda Experience in Azure Cloud Service Professional and Educational     ",3.00E+11,30-04-2024,29-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"Performance tuning, Leadership development, Data migration, Data management, Analytical, Management consulting, Agile, Troubleshooting, SQL, Python",-,9am-6pm,"Full Time, Permanent",Nityo Infotech,Organization,Nityo Infotech,https://img.naukimg.com/logo_images/groups/v1/76234.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Full Stack Python Developer - E0,"   Nisum is a leading global digital commerce firm headquartered in California,  with services spanning digital strategy and transformation,  insights and analytics,  blockchain,  business agility,  and custom software development.  Founded in 2000 with the customer-centric motto     Building Success Together     ,   Nisum has grown to over 1, 800 professionals across the United States,  Chile, Colombia,  India,  Pakistan and Canada.  A preferred advisor to leading Fortune 500 brands,  Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in today s world,  with immersive and seamless experiences across digital and physical channels.          What You'll Do       Actively interface with other teams to gather requirements,  design,  code,  debug,  document,  implement solutions using cloud tools     Be able to self-manage and required to lead projects with some client facing responsibilities     Highly motivated self-starter,  detail and quality oriented able to work independently Champion and manage new initiatives to completion to improve and streamline operational processes and maximize system resources     Resolve complex problems that have implications beyond your own area     Capable of working proficiently at both the strategic and tactical aspects of a project     Strong communication skills when responding to internal and external partners     Provide support of applications in the production environment,  as needed.      Monitor and manage the back-up and Disaster Recovery processes     Create key performance metrics,  measuring the utilization,  performance and overall health of the environment.      Report utilization and performance metrics to user communities     Develop and maintain operational best practices for smooth operation of the data analytics environment                What You Know       A minimum of 5-8 years  experience working with Big Data using Spark/Scala     Should be well versed in Core Java,  and Java frame works     Has mentored junior software developers on design patterns,  development best practices and DevOps trade-offs     Experienced with all ancillary technologies necessary for Internet applications: HTTP,  TCP/IP,  POP/SMTP,  etc.      High scalability projects involving cloud-based infrastructure design and implementation     Working knowledge of object-oriented design and development skills     Successful track record of developing quality software products and shipping production ready software     Proficient in Spark,  Scala,  Python,  AWS Cloud technologies     3+ years of experience across multiple Hadoop / Spark technologies such as Hadoop,     MapReduce,  HDFS,  HBase,  Hive,  Flume,  Sqoop,  Kafka,  Scala     Good understanding of Web Services protocols such as REST,  SOAP and API design for extensibility and portability     Experience debugging distributed systems with high data loads     Deep understanding of distributed data model     Should have excellent communication and presentation skills     Should have experience in AWS: Lambda,  EC2.      Knowledge in AWS EMR or Cloudera is a great plus.      Good problem solving and troubleshooting abilities     Ability to multitask in a fast-changing environment     Should be involved in working in an onsite / offshore model     Should have worked on all the stages of SDLC from Design to System Testing,  implementation and post implementation               Education       BS required in Computer Science or relevant area required.  MS in Computer Science or Information Systems preferred                Benefits       In addition to competitive salaries and benefits packages,  Nisum India offers its employees some unique and fun extras:      Continuous Learning -   Year-round training sessions are offered as part of skill enhancement certifications sponsored by the company on a need basis.  We support our team to excel in their field.       Parental Medical Insurance   - Nisum believes our team is the heart of our business and we want to make sure to take care of the heart of theirs.  We offer opt-in parental medical insurance in addition to our medical benefits.       Activities -  From the Nisum Premier League's cricket tournaments to hosted Hack-a-thon,  Nisum employees can participate in a variety of team-building activities such as skits,  and dances performance in addition to festival celebrations.       Free Meals -   Free snacks and dinner is provided on a daily basis,  in addition to subsidized lunch.                 Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.       ",2.10E+11,21-03-2024,19-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SMTP, System testing, Core Java, HTTP, Troubleshooting, Medial insurane, Disaster reovery, SDLC, Python, Computer siene",-,9am-6pm,"Full Time, Permanent",Nisum,Organization,Nisum,https://img.naukimg.com/logo_images/groups/v1/4657475.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Immediate Hiring For the Skill of Python Fullstack Developer,"  Role: Python with AWS Developer Experience:  6 T0 8 Years Notice Period : Immediate to 15 Days Location: Hyderabad, Pune, Coimbatore Job Type : Permanent Shift Timings: 1:30 PM -10:30 PM Mode of work: Hybrid Job Description: Primary Skill  : Python, AWS Skills(Good Understanding on Lambda, deployments in AWS, Cloud Watch, SNS,API Gateway)  DB Skills. If Anyone Interested, please drop a Mail to  vineetha.n@kanarystaffing.com  ",1.30E+11,13-04-2024,12-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Develop, Aws Lambda, Python, AWS, Lambda, DB, SQL",-,9am-6pm,"Full Time, Permanent",Kanary Staffing,Organization,Kanary Staffing,-,"Hyderabad, Pune, Coimbatore","Hyderabad, Pune, Coimbatore",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer,"         What Youll Do        Design build backend components of our MLOps platform on AWS.     Collaborate with geographically distributed cross-functional teams.     Participate in on-call rotation with the rest of the team to handle production incidents.               What You Know        A minimum of 8+ years of professional backend web development experience with Python.     Strong working experience in python programming.     Expertise with one of the python frameworks - pyspark,     Must have experience On AWS glue and Pyspark     strong experience with using pandas , numpy, joblib and other popular libraries.     Good working experiance with parallel batch processing with python     Good working experience On AWS Batch and Step functions     Good experience on aws ecs, ecr, docker      Good knowledge on cloud watch, athena and other aws services     Should have the expertise to write an effective, scalable, highly performable code     Should be in a position to test and debig the programs     Should have knowledge to implement security and data protection as part of the programing     Good to have terraform knowledge     Should have been implemented 2 or more large scale projects and be a part of end to end system implementation.     Should be a team player and should have the ability to coordinate and work with different teams     Should be a self starter and should be willing to learn the new skill set if required     Good analytical and problem-solving skills               Education        BS in Computer Science or related fields; MS preferred               Benefits        In addition to competitive salaries and benefits packages, Nisum India offers its employees some unique and fun extras:       Continuous Learning -    Year-round training sessions are offered as part of skill enhancement certifications sponsored by the company on a need basis. We support our team to excel in their field.       Parental Medical Insurance    - Nisum believes our team is the heart of our business and we want to make sure to take care of the heart of theirs. We offer opt-in parental medical insurance in addition to our medical benefits.       Activities -   From the Nisum Premier Leagues cricket tournaments to hosted Hack-a-thon, Nisum employees can participate in a variety of team-building activities such as skits, and dances performance in addition to festival celebrations.       Free Meals -    Free snacks and dinner is provided on a daily basis, in addition to subsidized lunch.                   ",1.10E+11,11-01-2024,10-04-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Backend, HP data protector, Analytical, Web development, Medical insurance, AWS, Team building, Analytics, Python",-,9am-6pm,"Full Time, Permanent",Nisum,Organization,Nisum,https://img.naukimg.com/logo_images/groups/v1/4657475.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Google BigQuery Good to have skills : NA Minimum  10  year(s) of experience is required Educational Qualification : Fulltime 15 years qualification Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have Skills :Google BigQueryGood to Have Skills : No Technology SpecializationJob Requirements :Key Responsibilities :1:Data Proc, Pub,Sub, Data flow, Kalka Streaming, Looker, SQL - No FLEX2:Proven track record of delivering data integration, data warehousing soln3:Strong SQL And Hands-on, Pro in BigQuery SQL language,Exp in Shell Scripting, Python - No FLEX4:Exp with data integration and migration projects ,Oracle, SQL5:understanding on cloud native services :bucket storage, GBQ, cloud function, pub sub, composer, and Kubernetes6:Exp in cloud solutions, mainly data platform services , GCP Certifications Technical Experience :1:Expert in Python - NO FLEX. Strong hands-on- knowledge in SQL - NO FLEX, Python programming using Pandas, NumPy, deep understanding of various data structure dictionary, array, list, tree etc, experiences in pytest, code coverage skills2:Exp with building solutions using cloud native services:bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes NO FLEX3:Pro with tools to automate AZDO CI CD pipelines like Control-M , GitHub, JIRA, confluence , CI CD Pipeline Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:Fulltime 15 years qualificationAdditional Info :",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sql, shell scripting, bigquery, python, google, kubernetes, css, confluence, ci/cd, data warehousing, numpy, spring, java, gcp, flex, mysql, html, jira, github, oracle, pytest, warehouse, sql server, javascript, dataproc, application development, pandas, gbq, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Google BigQuery Good to have skills : NA Minimum  2  year(s) of experience is required Educational Qualification : 15 years of full-time education or above Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have Skills :Google BigQueryGood to Have Skills : No Technology SpecializationJob Requirements :Key Responsibilities :a:Strong hands-on and strong knowledge in SQL, Python programming using Pandas, NumPy, deep understanding of various data structure dictionary, array, list, tree etc, experiences in pytest, code coverage skills are preferred b:Strong hands-on experience with building solutions using cloud native services:bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes etc c:Good knowledge of Big Data querying tools Technical Experience :a:Good knowledge of Big Data querying tools and Experience with integration of data from multiple data sources b:Good understanding of ETL tools like composer etc c:Proficiency with tools to automate AZDO CI CD pipelines - Control-M , GitHub, JIRA, confluence , CI CD Pipeline d:Open mindset, ability to quickly adapt new technologies e:Performance tuning of BigQuery SQL scripts f:GCP Certified preferred g:Working in agile environment Professional Attributes :1:Must have good communication skills 2:Must have ability to collaborate with different teams and suggest solutions 3:Ability to work independently with little supervision or as a team 4:Good analytical problem solving skills 5:Good team handling skills Educational Qualification:15 years of full-time education or aboveAdditional Info :Level and Across Accenture Location Facilities",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, sql, bigquery, python, google, confluence, numpy, microservices, spring, java, gcp, mysql, big data, etl, jira, c#, rest, github, code coverage, performance tuning, pytest, cloud native, application development, pandas, spring boot, agile, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Databricks Unified Data Analytics Platform-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Databricks Unified Data Analytics Platform  Good to have skills :Microsoft Azure Data Services Minimum 5 year(s) of experience is required  Educational Qualification : Key Reponsibilities :Experience in developing Spark applications using Spark-SQL in Databricks for data extraction, transformation, and aggregation from multiple file formats for Analyzing transforming the data to uncover insights into the customer usage patterns 2 Extract Transform and Load data from sources Systems to GCP Data Storage services using a combination of GCP Data Lake components Data ingestion to one or more GCP services GCP BQ, GCP BQ ML, DataProc, etc and processing the data in GCP Databricks Technical Experience : 1 Overall, 5 years of experience In Industry including 4Years of experience As Developer using Big Data Technologies like Databricks/Spark and Hadoop Ecosystems 2 Hands on experience on Unified Data Analytics with Databricks, Databricks Workspace User Interface, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL 3 Good understanding of Spark Architecture with Databricks, Structured Streaming 4 Setting Up cloud platform with Databricks, Databricks Workspace Professional Attributes :1 Team player 2 Good communication skill",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"hadoop, data services, data analytics, microsoft azure, data bricks, hive, hibernate, jquery, sql, spark programming, data extraction, java, spring mvc, spark, gcp, j2ee, json, html, ml, python, oracle, sql server, javascript, application development, mapreduce, sqoop",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google Cloud Data Services-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Google Cloud Data Services  Good to have skills :NA Minimum 10+ year(s) of experience is required  Educational Qualification : Key Reponsibilities :1 Designing, implementing, and maintaining data infrastructure and pipelines on the Google Cloud Platform GCP 2 Strong knowledge of GCP services, especially Big Query, data warehouse concepts 3 Proficiency in SQL and experience with Data security and Optimization 4 Familiarity with programming languages such as Python 5 Understanding of data security and complian Technical Experience : A purely data delivery expert min 18-19yrs of exp B data exp min 14yrs 1 Proven experience as a Google cloud Platform Engineer, preferably with a focus on Google Cloud Platform infra, IaaC, networking, IAM 2 Strong knowledge of GCP services such as Cloud Storage, Big Query, Dataflow, Dataproc, Cloud Composer, Pub/Sub, Airflow, DAG etc Professional Attributes :A story telling B manage team of 40 people C preferrably working in client location",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, google, sql, data warehousing concepts, bigquery, hive, python, data security, cloud orchestration, airflow, data warehousing, networking, cloud platform, dataproc, java, iam, gcp, spark, cloud storage, hadoop, aws, big data, data flow, pubsub",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : PySpark Good to have skills : Databricks Unified Data Analytics Platform Minimum  8  year(s) of experience is required Educational Qualification : Any technical Graduation Role:Application Developer  Project Role Description:Design, build and configure applications to meet business process and application requirements.  Must have Skills :PySpark, SSI: NON SSI:Good to Have Skills :SSI:Databricks Unified Data Analytics Platform NON SSI :Job Requirements:'', Key Responsibilities:Work on client projects to deliver AWS, PySpark, Databricks based Data engineering & Analytics solutions  Build and operate very large data warehouses or data lakes.  ETL optimization, designing, coding, & tuning big data processes using Apache Spark.  Build data pipelines & applications to stream and process datasets at low latencies.  Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data. Technical Experience:Minimum of 1 years of experience in Databricks engineering solutions on AWS Cloud platforms using PySpark  Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture & delivery.  Minimum 2 year of Experience in one or more programming languages Python, Java, Scala  Experience using airflow for the data pipelines in min 1 project  1 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform Professional Attributes: Ready to work in B Shift (12 PM 10 PM)  A Client facing skills:solid experience working in client facing environments, to be able to build trusted relationships with client stakeholders  Good critical thinking and problem-solving abilities  Health care knowledge  Good Communication Skil Educational Qualification:Any technical Graduation Additional Info:NA Qualification Any technical Graduation",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, pyspark, aws cloud microservices, etl, big data, kubernetes, python, data analytics, scala, docker, application development, data bricks, coding, java, git, aws cloud, spark, jenkins, data warehousing concepts, shell scripting, terraform, hadoop, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google Cloud Platform Architecture-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Google Cloud Platform Architecture  Good to have skills :Google BigQuery,Google Cloud Data Services Minimum 5 year(s) of experience is required  Educational Qualification :BE or BTech Key Reponsibilities :Work along-side customer team to deliver BQ migrations leveraging best-in-breed technology capability Build GCP data driven solutions for enterprise data warehouse and data lakehouse Support existing GCP Data Migration implementations Identify an opportunity for an automation and build optimal solution to reduce manual intervention Implement, Enhance the data pipelines in GCP to onboard additional data sources Technical Experience : 2-4 Years of relevant professional experience on GCP Proven track record of delivering data integration, data warehousing solutions Experience with data integration and migration projects; ETL/data integration tools Proficient in Big Query SQL language Experience working with cloud solutions, mainly data platform services Willingness to get GCP Certifications Required Preferred work experience in Shell Scripting, Python, Teradata, Oracle, SQL Server Professional Attributes :1 Experience in requirements analysis, gap analysis, requirements elicitation, requirements management and communication 2 Manages individual and team quality of work toward meeting or exceeding SLA targets 3 Strong analytical and prioritization skills",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"google, cloud platform, prioritization, sql, platform architecture, kubernetes, data services, python, oracle, gap analysis, data warehousing, microsoft azure, data migration, sql server, docker, java, gcp, shell scripting, bigquery, etl, requirement analysis, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Apache Spark Good to have skills : Data Warehouse ETL Testing, Microsoft SQL Server, Unix to Linux Migration Minimum  4  year(s) of experience is required Educational Qualification : A:15 years of Full Time education Role:Application Developer  Project Role Description:Design, build and configure applications to meet business process and application requirements.  Must have Skills :Apache Spark, SSI: NON SSI:Good to Have Skills :SSI:Data Warehouse ETL Testing, Microsoft SQL Server, Unix to Linux Migration NON SSI :Job Requirements:'', Key Responsibilities:A:Will be responsible for various transformations and actions, spark configuration and tuning techniques B:Must have the Ability to work on Hadoop architecture; execution engines, frameworks, applications tools C:Should be able to work on Pyspark using Spark MLlib library D:Must be able to deliver Data warehousing concepts methods  Technical Experience:A:Should have 4-5 years of experience using PySpark with Spark RDDs Spark SQL DataFramesB:Should have 2-3 years of experience in AWS Sagemaker and AWS Glue C:Should have 3-4 years of experience in data wrangling and data analysis with Pandas and Numpy D:Should have 3-4 years of Working experience with ML algorithms like Random Forest, Linear Regression, Logistic Regression, Decision Trees, K Means etc  Professional Attributes:A:Should have good communication and analytical skills  Educational Qualification:A:15 years of Full Time education Additional Info:A :No Specific Shift Timings",2.10E+11,21-04-2024,20-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, pyspark, spark, data warehousing concepts, hadoop, algorithms, data analysis, aws sagemaker, data warehousing, numpy, sql server, sql, application development, pandas, aws glue, etl testing, ml algorithms, apache, linux, spark mllib, aws, data wrangling, unix, ml",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Databricks Unified Data Analytics Platform Good to have skills : NA Minimum  2  year(s) of experience is required Educational Qualification : Fulltime 15 years qualification Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have Skills :Databricks Unified Data Analytics PlatformGood to Have Skills : No Technology SpecializationJob Requirements :Key Responsibilities :Exp in implementation of Operational Data Model or Data Mart using Clinical Operational data Experience in Databricks Data Engineering to create Data Lake solutions using AWS services Knowledge of Databricks cluster and SQL warehouse, Exp in Delta and Parquet file handling Knowledge of Databricks Unity Catalog and Consumption patterns Knowledge of GitHub and CI CD Pipelines Extensive Exp in SQL, entity relationship, Join, Aggregation function and DBT, Python, Data frames and Spark Technical Experience : Responsible for authoring SQL and Python scripts on Databricks and Data Build tool to create ODM model.  Responsible for identifying data set relationship, join criteria and implement it in code for ODM model development.  Responsible for creation of Delta Lake for ODM model and setup of consumption pattern using Databricks Unity catalog  Responsible for creation of Data Pipelines for Data processing of Delta files into ODM format for downstream data consumptionProfessional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever requiredEducational Qualification:Fulltime 15 years qualificationAdditional Info :",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sql, spark, python, data grids, er, web services, ci/cd, hibernate, jquery, java, asp.net, json, mysql, html, c#, github, data analytics, oracle, data processing, data engineering, sql server, javascript, application development, data bricks, angular, aws, angularjs",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Apache Spark Good to have skills : Unix to Linux Migration, Data Warehouse ETL Testing, Microsoft SQL Server Minimum  3  year(s) of experience is required Educational Qualification : A:15 years of Full Time education Role:Application Developer  Project Role Description:Design, build and configure applications to meet business process and application requirements.  Must have Skills :Apache Spark, SSI: NON SSI:Good to Have Skills :SSI:Unix to Linux Migration, Data Warehouse ETL Testing, Microsoft SQL Server NON SSI :Job Requirements:'', Key Responsibilities:A:Will be responsible for various transformations and actions, spark configuration and tuning techniques B:Must have the Ability to work on Hadoop architecture; execution engines, frameworks, applications tools C:Should be able to work on Pyspark using Spark MLlib library D:Must be able to deliver Data warehousing concepts methods  Technical Experience:A:Should have 4-5 years of experience using PySpark with Spark RDDs Spark SQL DataFrames B:Should have 2-3 years of experience in AWS Sagemaker and AWS Glue C:Should have 3-4 years of experience in data wrangling and data analysis with Pandas and Numpy D:Should have 3-4 years of Working experience with ML algorithms like Random Forest, Linear Regression, Logistic Regression, Decision Trees, K Means etc  Professional Attributes:A:Should have good communication and analytical skills  Educational Qualification:A:15 years of Full Time education Additional Info:A :No Specific Shift Timings Qualification A:15 years of Full Time education",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, pyspark, spark, data warehousing concepts, hadoop, algorithms, data analysis, aws sagemaker, data warehousing, numpy, sql server, sql, application development, pandas, aws glue, etl testing, ml algorithms, apache, linux, spark mllib, aws, data wrangling, unix, ml",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Microsoft Azure Analytics Services-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Microsoft Azure Analytics Services  Good to have skills :NA Minimum 7 year(s) of experience is required  Educational Qualification :Minimum 15 years of fulltime education Key Reponsibilities :A Function as the Junior Data Architect for a small, simple project or proposal or as a team lead for small or medium sized project B Discuss specific data architecture and data related issues with data architect or team in area of expertise C Analyze and assess the impact of the requirements on the data and its lifecycle D Lead Big data architecture and design of small implementation of Lambda architecture Technical Experience : A Strong experience in Azure is preferred with hands-on experience in two or more of these skills :Azure Synapse Analytics, Azure HDInsight, Azure Databricks with PySpark or Scala or SparkSQL, Azure Analysis Services B Experience in one or more Real-time or Streaming technologies including:Azure Stream Analytics, Azure Data Explorer, Azure Time Series Insights, etc C Candidate must have 3-5 years of IT experience and around 1-3 years of extensive Big data experience Professional Attributes :A Team player B Should have excellent client communication skills C Should have good analytical and problem-solving skills",1.30E+11,13-04-2024,12-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"analytics services, azure analytics, microsoft azure, azure analysis services, big data, azure databricks, azure data lake, scala, azure synapse, pyspark, azure data factory, azure hdinsight, spark, azure analysis, azure data explorer, azure stream analytics",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Kolkata,Kolkata,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Lead customer Success Engineer III (AWS),"     Lead and facility troubleshooting and resolve complex cloud-related issues       Provide guidance to customer on deploy and migrate applications and data to the cloud     Provide best practice for monitor and optimize cloud infrastructure     Ensure data security and compliance in the cloud     Interact with clients and provide cloud support and guidance     Collaborate with teams to design and implement cloud-based solutions     Ensure customers realize the value of Rackspace and Fanatical Support through pro-active architecture reviews and consultancy work     Attend and participate in monthly/quarterly service review meetings     Identify opportunities for growth and pass leads to a Business Development Consultant     Mentor and assist in the development of other technical staff     Requirements:Bachelor s degree in Compuer Science, Engineering, or equivalent working experience     At least 5 years of experience in cloud operation support and deployment     AWS ertifications preferred     Proficient in cloud platforms, tools, and services     Experience in cloud architecture, development, and administration     Proven Troubleshooting and analytical skills     Tenacious problem solver, will own issues until full resolution     Excellent communication skills, both written and verbal with great attention to detail     Strong rapport and relationship building skills with both internal departments and external customers     A good level of business awareness and commercial acumen     Understanding of IT industry working practices / methodologies - ITIL foundation certification desirable but not essential   ",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"operational support, data security, Relationship building, Cloud, Manager Technology, Troubleshooting, rackspace, AWS",-,9am-6pm,"Full Time, Permanent",Rackspace Technology,Organization,Rackspace Technology,https://img.naukimg.com/logo_images/groups/v1/3080116.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Data Engineering-D&A - Senior Associate,"   A career in our Advisory Acceleration Centre is the natural extension of PwCs leading class global delivery capabilities     We provide premium, cost effective, high quality services that support process quality and delivery capability in support for client engagements     As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution         PwC Professional skills and responsibilities for this management level include but are not limited to:         Use feedback and reflection to develop self awareness, personal strengths and address development areas.     Delegate to others to provide stretch opportunities, coaching them to deliver results.     Demonstrate critical thinking and the ability to bring order to unstructured problems.     Use a broad range of tools and techniques to extract insights from current industry or sector trends.     Review your work and that of others for quality, accuracy and relevance.     Know how and when to use tools available for a given situation and can explain the reasons for this choice.     Seek and embrace opportunities which give exposure to different situations, environments and perspectives.     Use straightforward communication, in a structured way, when influencing and connecting with others.     Able to read situations and modify behavior to build quality relationships.     Uphold the firms code of ethics and business conduct.                   Minimum Degree Required:    Bachelors degree in Engineering               ?         Certification(s) Preferred:    Amazon Web Services (AWS) certifications (AWS certified Data Engineer is recommended).            ?         Desired Skills                       Minimum 2 years experience in design and development of data pipelines and processing of data at scale using technologies like EMR, Kinesis, Lambda, Glue, Athena, Redshift, Step Functions.             Experience in design and development of applications using Python (must have) or Java.              Experience in Big Data technologies and tools such as PySpark/Spark, Hadoop, Hive, Kafka etc             Experience with building data pipelines in streaming and batch mode.             Experience in optimizing big data pipelines on AWS.             Good understanding of data warehousing concepts and Expert level skills in writing and optimizing SQL.             Experience in development using message queues, stream processing, highly available & fault tolerant applications             Minimum 1 year of experience in using AWS services like EC2, Cloud IAM, VPC, S3 etc and good understanding of architectural best practices.             Proficiency in using SDKs for interacting with native AWS services             Good understanding of big data design patterns and performance tuning              Experience with Git and CI/CD pipelines to deploy cloud applications             Experienced in roles conducting requirements gathering, writing user stories, creating application design and using design patterns             Excellent communication skills with the ability to influence client business and IT teams             Experience with Agile software development                 Ability to work independently and across multiple teams.       ",2.70E+11,27-03-2024,25-06-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Accounting / Auditing,"Performance tuning, Leadership development, Architecture, Product innovation, Agile, big data, AWS, SQL, Python",-,9am-6pm,"Full Time, Permanent",PwC Service Delivery Center,Organization,PwC Service Delivery Center,https://img.naukri.com/logo_images/v3/1428182.gif,"Mumbai, Hyderabad, Bengaluru","Mumbai, Hyderabad, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Data Encryption and Cryptography Automation Expert,"   The Security Services team at ITO, Ford Business Solutions, India is seeking a passionate and creative Cloud Data Encryption and Cryptography Automation Expert who can build and maintain critical security tools used to protect Ford s most sensitive security systems. The interested candidate should have experience in Public Key Infrastructure (PKI), Certificate Management, key management, EKCM, cryptography, data encryption and managing SaaS platforms such as Venafi, Cloud KMS as well as cloud computing (GCP, AWS, Azure), and automation.       WHAT YOUR SKILLSET LOOKS LIKE:         A relevant Bachelors or Master s Degree in engineering/computer applications     3+ years of experience as a Security Engineer with a focus on using tools like Venafi, Cloud KMS     Experience in design and implementation of Windows Active Directory Certificate Services, CAs (both root and enterprise CAs).     Experience in configuring Online Responder service (OCSP) and Certificate Revocation List (CRL) servers.     Experience in working with external CAs like DigiCert, GlobalSign etc.     1+ years of experience in application/API development (Python Flask or FastAPI or Java or Node JS)     Basic experience in JavaScript, HTML5,CSS, etc.     Knowledge of GCP or Azure or AWS and configuring infrastructure using infrastructure-as-a-code libraries like Terraform, Ansible, etc.     Experience working in an Agile development environment     Understanding of Dev(Sec)Ops best practices highly advantageous     In-depth knowledge of cybersecurity principles, practices, and technologies.     Strong communication skills     The courage to promise and commitment to deliver, whatever it takes. Keen attention to detail.     Pro-active, independent, resourceful, able to work in a team environment and work independently with minimal supervision     Positive and passionate personality, with a zest for life outside of work (we truly mean it. And measure it too!)           WOULD BE GREAT IF YOU ALSO BRING:         Security Certifications:      Certified Information Systems Security Professional (CISSP) or      Venafi Security Administrator or     GCP: Professional Cloud Security Engineer             YOUR TYPICAL DAY HERE WOULD BE:         Automate and orchestrate certificate management using Venafi     Collaborate with Platform vendors to coordinate distribution of regular updates/patches to clients and platform.     Test and implement vendor product upgrades.     Utilize GCP Cloud services, including KMS and ReCaptcha security services, to implement robust encryption, key management, and application security solutions     Act as an advocate and liaison with Cyber D fense analysts to understand their requirements and use cases, and to design, develop, and automate security solutions.     Work with application teams to adopt and modernize data security.     Manage nCipher HSM for key management, ensuring the security and integrity of cryptographic keys.     Automate repeatable tasks and workflows to improve process efficiency by developing APIs/scripts and deploying to the cloud.     Implement SRE for Platform services, capabilities/features to achieve availability and reliability.      Implement ZeroTrust capabilities across all device pillars to reduce and maintain non-compliance devices.     Research and evaluate new security technologies and make recommendations for implementation.     Stay up-to-date on the latest security trends and developments and maintain a high level of technical expertise in the field of endpoint security.     ",2.11E+11,21-12-2023,20-03-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Auto Components,"Cloud computing, Automation, hsm, endpoint security, data security, Security services, Application security, Business solutions, Cryptography, Python",-,9am-6pm,"Full Time, Permanent",Ford,Organization,Ford,https://img.naukimg.com/logo_images/groups/v1/247012.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Tibco/AWS/JAVA Developer," We are seeking an experienced Tibco/AWS/JAVA Developer with at least 5 years of experience to join our team. The successful candidate will develop custom solutions using technology tools such as TIBCO, AWS native technologies such as EC2, EKS, Lambda, AmazonMQ, API Gateway, DynamoDB, DocumentDB, etc. This will include designing new and existing critical software systems and/or applications to improve the customer journey and operational efficiency. In addition, the candidate will work on serverless technologies, services, and container technologies such as Docker and Kubernetes.   Key Responsibilities:      Develop custom solutions powered by technology tools such as TIBCO, and AWS native technologies such as EC2, EKS, Lambda, AmazonMQ, API Gateway, DynamoDB, DocumentDB, etc.     Design new and existing critical software systems and/or applications to improve the customer journey and operational efficiency.     Design and develop advanced applications using JAVA and other open-source technologies.     Work on serverless technologies, services, and container technologies such as Docker and Kubernetes.     Requirements:      Bachelors degree is required.     5-8 years of experience developing custom solutions.     Experience with TIBCO, AWS, and JAVA is mandatory.     Experience with AWS services such as EC2, EKS, Lambda, AmazonMQ, API Gateway, DynamoDB, and DocumentDB is mandatory.     Experience with serverless technologies and container technologies such as Docker and Kubernetes is preferred.   ",2.61E+11,26-08-2023,24-11-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Networking, Customer satisfaction, IT consulting, TIBCO, Manager Technology, Open source, Business solutions, Operations, AWS",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,"Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud IaaS Developer," Ideal candidate has experience with developing applications in the cloud; knows Python, frameworks like Django but also knows how to deploy an application on the cloud.      We re hiring Cloud Development Engineer / hands-on Cloud Architect for developing our IaaS platform. Specifically skills in developing one-click deployment of VMs, GPUs and applications on public cloud is preferred.       Responsibilities       Be proficient in public cloud technologies from authentication, deployment, database selection, backend integration and front end UI. Unique opportunity to work on a whole application than small chunks of cloud development.       Qualifications       1-3 years experience with Oracle Cloud, AWS, Google Cloud and/or Azure. GPU Cloud experience is an advantage.     Must have prior demonstrable hands-on cloud development experience. Start-up speed and energy required. ",2.01E+11,20-09-2023,19-12-2023,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Backend, Front end, Django, Cloud, cloud architect, Deployment, Oracle, VMS, AWS, Python",-,9am-6pm,"Full Time, Permanent",Qubrid,Organization,Qubrid,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud_Engineer,"Your Role and Responsibilities As an cloud Engineer, you would be responsible for  An AWS cloud engineer is responsible for designing, building, and managing cloud-based applications and services using Amazon Web Services (AWS). Typical job duties include: Planning and designing the cloud infrastructure with AWS Maintaining and deploying the cloud applications Troubleshooting and resolving issues with the cloud infrastructure Developing new cloud-based solutions Required Technical and Professional Expertise 3+ years' experience in cloud computing Experience managing common AWS services (EC2, RDS, S3, VPC, CloudFormation etc.) or AZURE OR GCP Equivalent Experience with containerization technologies such as Docker and Kubernetes Experience in basic administration of MySQL, MSSQL, Postgres, DynamoDB, MongoDB and/or ElasticSearch Knowledgeable about networking and security Knowledgeable about configuration management frameworks such as Puppet, Terraform, Chef or Ansible Effective communication skills Good understanding of Linux tools, with an emphasis on curl, stack strace and using git Various authentication such as LDAP (MS Active Directory and OpenLDAP), SAML 2.0 and OIDC. Hands on UNIX/Linux experience for setting up application environments. Experience in deploying and managing web technology frameworks including API's & micro-services. Scripting, automation, and application monitoring tools. Strong troubleshooting skills and a passion for problem solving and investigation Deliver the deployment based on customer requirements Preferred Technical and Professional Expertise Python (web frameworks), Angular.js/ React (web frameworks), java and RESTful APIs Upstream involvement in open source projects (used commonly in watsonx.data) (example postgresql, mysql, db2, netezza) Having knowledge on visualizations and dashboards, data catalogues, ETL tools, Jupyter notebooks environments will be an added advantage Analyze and organize raw data, Build data systems and pipelines, Evaluate business needs and objectives, Interpret trends and patterns, Conduct complex data analysis and report on results Knowledge of support systems and tools is considered an advantage An appreciation and passion for Open Source software would be an advantage",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"gcp, troubleshooting, mysql, aws, mongodb, kubernetes, curl, microsoft active directory, dynamo db, openldap, microsoft azure, amazon rds, aws cloudformation, sql server, ansible, docker, puppet, elastic search, git, postgresql, ldap, linux, terraform, cloud computing",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,"Hyderabad, Pune, Bengaluru","Hyderabad, Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Designer,"Project Role : Application Designer Project Role Description : Assist in defining requirements and designing applications to meet business process and application requirements.  Must have skills : Amazon Web Services (AWS) Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of fulltime education Summary :As an Application Designer, you will be responsible for assisting in defining requirements and designing applications to meet business process and application requirements using Amazon Web Services (AWS). Your typical day will involve working with cross-functional teams, analyzing business requirements, and designing scalable and secure applications on AWS.  Roles & Responsibilities: Design and develop scalable, secure, and highly available applications on AWS. Collaborate with cross-functional teams to analyze business requirements and design solutions that meet business process and application requirements. Ensure the application design is aligned with AWS best practices and security standards. Provide technical guidance and mentorship to junior team members. Stay updated with the latest advancements in AWS and cloud computing technologies. Professional & Technical Skills: Must To Have Skills:Strong experience in Amazon Web Services (AWS). Good To Have Skills:Experience in other cloud platforms such as Microsoft Azure or Google Cloud Platform. Experience in designing and developing scalable and secure applications on AWS. Strong understanding of AWS services such as EC2, S3, Lambda, and RDS. Experience in designing and implementing AWS security and compliance controls. Experience in DevOps practices and tools such as Jenkins, Git, and Docker. Additional Information: The candidate should have a minimum of 3 years of experience in Amazon Web Services (AWS). The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering scalable and secure applications on AWS. This position is based at our Bengaluru office. Qualification 15 years of fulltime education",1.21E+11,12-05-2024,10-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"aws iam, microsoft azure, amazon ec2, gcp, aws, python, project management, software development, web services, application design, amazon rds, hibernate, aws cloudformation, docker, ansible, git, java, lambda expressions, devops, linux, jenkins, jira, amazon cloudwatch",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Designer,"Project Role : Application Designer Project Role Description : Assist in defining requirements and designing applications to meet business process and application requirements.  Must have skills : Amazon Web Services (AWS) Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of fulltime education Summary :As an Application Designer, you will be responsible for assisting in defining requirements and designing applications to meet business process and application requirements using Amazon Web Services (AWS). Your typical day will involve working with cross-functional teams, analyzing business requirements, and designing scalable and secure applications on AWS.  Roles & Responsibilities: Design and develop scalable, secure, and highly available applications on AWS. Collaborate with cross-functional teams to analyze business requirements and design solutions that meet business process and application requirements. Ensure the application design is aligned with AWS best practices and security standards. Provide technical guidance and mentorship to junior team members. Stay updated with the latest advancements in AWS and cloud computing technologies. Professional & Technical Skills: Must To Have Skills:Strong experience in Amazon Web Services (AWS). Good To Have Skills:Experience in other cloud platforms such as Microsoft Azure or Google Cloud Platform. Experience in designing and developing scalable and secure applications on AWS. Strong understanding of AWS services such as EC2, S3, Lambda, and RDS. Experience in designing and implementing AWS security and compliance controls. Experience in DevOps practices and tools such as Jenkins, Git, and Docker. Additional Information: The candidate should have a minimum of 3 years of experience in Amazon Web Services (AWS). The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering scalable and secure applications on AWS. This position is based at our Bengaluru office. Qualification 15 years of fulltime education",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"aws iam, microsoft azure, amazon ec2, gcp, aws, python, project management, software development, web services, application design, amazon rds, hibernate, aws cloudformation, docker, ansible, git, java, lambda expressions, devops, linux, jenkins, jira, amazon cloudwatch",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
MLOps Engineer - Cloud Apps,"- Databricks ML Ops Engineers (Admin) for the leaders in analytics and cloud applications, headquartered in San Francisco and with offices in India.  Our client provides strategic guidance and technology systems for clients wishing to solve their most com",90524910268,09-05-2024,07-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Cloud Apps, Azure, MLOps, Google Cloud Platform, Azure Databricks, AWS, Data Catalogue",-,9am-6pm,"Full Time, Permanent",iMindYourBusiness,Organization,iMindYourBusiness,-,"Hyderabad, Pune, Bengaluru","Hyderabad, Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
DataOps Engineer - Analytics & Cloud Apps,"We are hiring data operations engineers for the leaders in analytics and cloud applications, headquartered in San Francisco with offices in India. Our client provides strategic guidance and technology systems for clients wishing to solve their most complex and interesting business challenges involving cloud applications, big data, business intelligence, and data discovery. They work with both corporate technology departments and corporate business units to develop value-added decision support solutions, not just new technology deployments. Exp Level: 5 - 8 years 5+ years experience with 1+ years experience in CI/CD with Gitlab and Github. Required: CI/CD, Gitlab, Github, bash scripts/yaml files. Preferred: Snowflake, Airflow, DataOps experience for Snowflake and Airflow, Azure - Design and development of data engineering assets and scalable engineering frameworks to support various Business unit's data demands and internal data analytics activities - Evaluate data importance and manage production of data pipelines - Code, test, and document new or modified data models and ETL/ELT tools to create robust and scalable data assets for reporting and analytics - Build data engineering framework to support Data Migration to AWS Cloud technologies and deliver new projects as per new target architecture within AWS data cloud service - Expand and increase data platform capabilities to resolve new data problems and challenges by identifying, sourcing, and integrating new data - Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, redesigning infrastructure for greater scalability, etc - Peer review code and promote dev ops culture within the data team. ",90524909529,09-05-2024,07-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Analytics / KPO / Research,"DataOps, Apache Airflow, Azure, Cloud Apps, Data Migration, Data Pipeline, ETL Tools, Data Infrastructure, CI/CD, Snowflake DB",-,9am-6pm,"Full Time, Permanent",iMindYourBusiness,Organization,iMindYourBusiness,-,"Hyderabad, Pune, Bengaluru","Hyderabad, Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS LEX - Chatbot Developer with MNC _ Only Immediate Joiners,"Conversational AI development experience using Amazon lex(Minimum 3 year) Integration with Amazon connect AWS services experience(Lambda function) 3+ years of exp in software development, with successfully launching commercial chatbot products. Required Candidate profile Exp with Jira, Confluence & SharePoint. Bonus points for Figma and VoiceFlow AWS certifications or equivalent exp in cloud technologies Contribute to the overall improvement of chatbot functionalities Perks and benefits Notice Period: 0 - 10 Days only.",2.70E+11,27-04-2024,26-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Amazon Lex, Chatbot, LEX, Aws Lambda, conversational AI, Chatbot Development",-,9am-6pm,"Full Time, Permanent",Wize Careers Consultants,Organization,Wize Careers Consultants,-,"Hyderabad, Bengaluru, Mumbai (All Areas)","Hyderabad, Bengaluru, Mumbai (All Areas)",-,-,-,7-14 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Microsoft Azure Modern Data Platform, Python (Programming Language), Databricks Unified Data Analytics Platform Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform, ensuring cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services, collaborating with Integration Architects and Data Architects, and utilizing your expertise in data platform components.  Roles & Responsibilities: Assist with the blueprint and design of the data platform, encompassing the relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Implement and maintain data platform components using Microsoft Azure Data Services. Develop and maintain data pipelines, ensuring data quality and integrity. Troubleshoot and resolve data platform issues, working with cross-functional teams as needed. Professional & Technical Skills: Must To Have Skills:Expertise in Microsoft Azure Data Services. Good To Have Skills:Experience with Microsoft Azure Modern Data Platform, Databricks Unified Data Analytics Platform, and Python (Programming Language). Strong understanding of data platform components and architecture. Experience with data pipeline development and maintenance. Ability to troubleshoot and resolve data platform issues. Solid grasp of data quality and integrity best practices. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of education",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, python, microsoft azure, data engineering, data quality, hive, data analytics, data warehousing, data architecture, machine learning, sql server, sql, nosql, amazon ec2, java, data modeling, spark, kafka, mysql, hadoop, sqoop, aws, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Microsoft Azure Modern Data Platform, Python (Programming Language), Databricks Unified Data Analytics Platform Minimum  7.5  year(s) of experience is required Educational Qualification : Minimum 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform, ensuring cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services, collaborating with Integration Architects and Data Architects, and utilizing your expertise in Python, Databricks Unified Data Analytics Platform, and Microsoft Azure Modern Data Platform.  Roles & Responsibilities: Assist with the blueprint and design of the data platform, encompassing the relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Microsoft Azure Data Services. Design and implement data storage solutions using Microsoft Azure Modern Data Platform. Utilize your expertise in Python and Databricks Unified Data Analytics Platform to develop and maintain data processing and analysis tools. Professional & Technical Skills: Must To Have Skills:Expertise in Microsoft Azure Data Services. Good To Have Skills:Proficiency in Python, Databricks Unified Data Analytics Platform, and Microsoft Azure Modern Data Platform. Strong understanding of data modeling and database design principles. Experience with data integration and ETL processes. Familiarity with data warehousing and business intelligence concepts. Additional Information: The candidate should have a minimum of 7.5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of education",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, data services, information technology, microsoft azure, data modeling, hive, data analytics, data warehousing, data architecture, machine learning, sql, database design, data bricks, java, spark, design patterns, design principles, etl, aws, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer-GCP - Senior Associate 2,"                 Experience in GCP .Google Cloud Platform.Experience in implementing solutions leveraging Google Cloud products such as Cloud BigQuery, Cloud DataFlow, Kubernetes, Cloud Pub/Sub, Cloud Big Table and TensorFlow will be highly preferred.                       Experience in delivering artifacts such as scripts (Python), dataflow components. Strong experience with SQL                         Experience in building and optimizing large scale data pipeline systems.                         Experience in Big Data Analytic frameworks and query tools such as Spark Hadoop Hive Impala etc                         Experience working with SQL and NoSQL databases like MongoDB Cassandra.                         Good understanding of Big data design patterns and performance tuning.                         Experience with data pipeline and workflow management tools: Azkaban Oozie etc                         Strong Data Warehousing experience building operational ETL or ELT pipelines comprising several sources.                         Streaming Data Integration experience with Sqoop and Kafka                         Experience in working on projects using NIFI                         Experience with Migration of Workloads from on-prem/other public clouds to Google Cloud Platform.                         Knowledge or working experience on 1-2 projects in an alternate cloud environment such as AWS or Azure will be a bonus                         Knowledge or working experience in GitHub for source code maintenance will be a plus.                         Good understanding of Data Modeling and Data Architecture       ",2.80E+11,28-02-2024,28-05-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Accounting / Auditing,"Performance tuning, Data modeling, Cloud, Process quality, Data analytics, big data, Data warehousing, SQL, Data architecture, Python",-,9am-6pm,"Full Time, Permanent",PwC Service Delivery Center,Organization,PwC Service Delivery Center,https://img.naukri.com/logo_images/v3/1428182.gif,"Hyderabad, Bengaluru","Hyderabad, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Migration Engineer,"Project Role : Cloud Migration Engineer Project Role Description : Provides assessment of existing solutions and infrastructure to migrate to the cloud. Plan, deliver, and implement application and data migration with scalable, high-performance solutions using private and public cloud technologies driving next-generation business outcomes.  Must have skills : AWS CloudFormation Good to have skills : AWS Athena, AWS Redshift Minimum  3  year(s) of experience is required Educational Qualification : Job  Summary :We are seeking an experienced AWS Subject Matter Expert (SME) with expertise in Lake Formation to join our team. The successful candidate will be responsible for designing, building, and maintaining our company's data lakes on AWS. This includes developing and implementing tagging, access role simplifications, internal access system integrations and governance.Responsibilities:* Design and implement data lakes on AWS using Lake Formation by using Terraform IaC*Design central Lake Formation management approach for overall in data space to manage data authorizations and aws resource management* Involve on tagging implementation for the various AWS services such as EMR, Redshift and S3 and data governance policies* Collaborate with data analysts and data scientists to understand business requirements and optimize data authorization controls* Optimize and improve access management by using Lake formation* Stay up-to-date with new AWS features and best practices for Lake FormationRequirements:* 3+ years of experience with AWS and Lake Formation* Strong knowledge of AWS services, including S3, Glue, and Lake Formation* Familiarity with data warehousing and data governance principles* Excellent problem-solving skills and attention to detail*Experience with infrastructure as code tools like Terraform or CloudFormationNice to Have:* Experience with other AWS services, such as Redshift, Athena, and Quicksight* Knowledge of data architecture and design principles* Familiarity with security and compliance requirements for data storage and processing* Certification in AWS  Additional Information: The candidate should have a minimum of 5 years of experience in AWS CloudFormation. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering successful cloud migration projects. This position is based at our Bengaluru office.",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"aws iam, amazon redshift, aws cloudformation, athena, aws, kubernetes, python, cloud migration, vmware, data warehousing, microsoft azure, aws lambda, docker, ansible, sql, amazon ec2, git, spark, devops, microsoft windows, linux, jenkins, terraform, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,"Hyderabad, Bengaluru","Hyderabad, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Migration Engineer,"Project Role : Cloud Migration Engineer Project Role Description : Provides assessment of existing solutions and infrastructure to migrate to the cloud. Plan, deliver, and implement application and data migration with scalable, high-performance solutions using private and public cloud technologies driving next-generation business outcomes.  Must have skills : AWS CloudFormation Good to have skills : AWS Athena, AWS Redshift Minimum  5  year(s) of experience is required Educational Qualification : Job  Summary :We are seeking an experienced AWS Subject Matter Expert (SME) with expertise in Lake Formation to join our team. The successful candidate will be responsible for designing, building, and maintaining our company's data lakes on AWS. This includes developing and implementing tagging, access role simplifications, internal access system integrations and governance.Responsibilities:* Design and implement data lakes on AWS using Lake Formation by using Terraform IaC*Design central Lake Formation management approach for overall in data space to manage data authorizations and aws resource management* Involve on tagging implementation for the various AWS services such as EMR, Redshift and S3 and data governance policies* Collaborate with data analysts and data scientists to understand business requirements and optimize data authorization controls* Optimize and improve access management by using Lake formation* Stay up-to-date with new AWS features and best practices for Lake FormationRequirements:* 3+ years of experience with AWS and Lake Formation* Strong knowledge of AWS services, including S3, Glue, and Lake Formation* Familiarity with data warehousing and data governance principles* Excellent problem-solving skills and attention to detail*Experience with infrastructure as code tools like Terraform or CloudFormationNice to Have:* Experience with other AWS services, such as Redshift, Athena, and Quicksight* Knowledge of data architecture and design principles* Familiarity with security and compliance requirements for data storage and processing* Certification in AWS  Additional Information: The candidate should have a minimum of 5 years of experience in AWS CloudFormation. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering successful cloud migration projects. This position is based at our Bengaluru office.",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"aws iam, amazon redshift, aws cloudformation, athena, aws, kubernetes, python, cloud migration, vmware, data warehousing, microsoft azure, aws lambda, docker, ansible, sql, amazon ec2, git, spark, devops, microsoft windows, linux, jenkins, terraform, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,"Hyderabad, Chennai","Hyderabad, Chennai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Harness Engineer,"  Position: Google Cloud Platform (GCP) Harness Engineer Experience: 5-8 years Responsibilities: 1. Design, implement, and maintain CI/CD pipelines using GCP Harness for efficient software delivery. 2. Collaborate with development and operations teams to streamline the deployment process and ensure continuous integration and continuous deployment practices. 3. Configure and manage deployment strategies, rollbacks, and canary releases within the GCP Harness platform. 4. Implement and maintain infrastructure as code (IaC) practices using tools like Terraform or Deployment Manager for GCP environments. 5. Work closely with cross-functional teams to troubleshoot and resolve deployment issues and optimize release processes. 6. Define and enforce best practices for version control, branching, and code promotion in collaboration with development teams. 7. Implement and manage monitoring and alerting solutions for deployment pipelines to ensure system reliability and availability. 8. Collaborate with security teams to integrate security practices into the CI/CD pipelines, including vulnerability scanning and compliance checks. 9. Stay updated on the latest GCP and DevOps trends, providing recommendations for improvements and optimizations.",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"GCP Harness platform., Ci/Cd, Devops",-,9am-6pm,"Full Time, Permanent",Tata Consultancy Services (TCS),Organization,Tata Consultancy Services (TCS),https://img.naukimg.com/logo_images/groups/v1/223346.gif,"Kolkata, Hyderabad, Bengaluru","Kolkata, Hyderabad, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer - Gen AI - Associate 2,"   A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics      We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology      We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge      As part of our Analytics and Insights Consumption team, you ll analyze data to drive useful insights for clients to address core business issues or to drive strategic outcomes      Youll use visualization, statistical and analytics models, AI/ML techniques, Modelops and other techniques to develop these insights                        As an Associate, youll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:            Invite and give in the moment feedback in a constructive manner.      Share and collaborate effectively with others.      Identify and make suggestions for improvements when problems and/or opportunities arise.      Handle, manipulate and analyse data and information responsibly.      Follow risk management and compliance procedures.      Keep up-to-date with developments in area of specialism.      Communicate confidently in a clear, concise and articulate manner - verbally and in the materials I produce.      Build and maintain an internal and external network.      Seek opportunities to learn about how PwC works as a global network of firms.      Uphold the firms code of ethics and business conduct.                                Job Description: GenAI Data Engineer - Associate                      PwC US - Acceleration Center is seeking an enthusiastic and emerging GenAI Data Engineer to join our team at the Associate level        This role is ideal for individuals passionate about data engineering and eager to develop their skills in GenAI technologies      As an Associate GenAI Data Engineer, you will contribute to developing and maintaining data pipelines, implementing machine learning models, and optimizing data infrastructure for our GenAI projects under the guidance of more experienced team member               Responsibilities:                  Assist in the design, development, and maintenance of data pipelines and ETL processes for GenAI projects.                  Work closely with data scientists and software engineers to support the implementation of machine learning models and algorithms.                  Help optimize data infrastructure and storage solutions to ensure efficient data processing.                  Gain experience with event-driven architectures to enable real-time data processing and analysis.                  Learn and assist with containerization technologies like Kubernetes and Docker for deployment and scalability.                  Support the development and maintenance of data lakes for managing large volumes of structured and unstructured data.                  Contribute to the integration of LLM frameworks (such as Langchain and Semantic Kernel) for advanced language processing and analysis.                  Collaborate with cross-functional teams to support the design and implementation of solution architectures for GenAI projects.                  Utilize cloud computing platforms such as Azure or AWS under supervision for data processing, storage, and deployment.                  Participate in monitoring and troubleshooting of data pipelines and systems to ensure smooth operations.                  Stay informed about the latest advancements in GenAI technologies and assist in recommending innovative solutions to enhance data engineering processes.                  Engage with cross-functional teams to help understand business requirements and contribute to translating them into technical solutions.                  Assist in documenting data engineering processes, methodologies, and best practices.                  Requirements:                  Bachelor s degree in Computer Science, Data Science, or a related field.                  1-3 years of relevant experience, ideally with some exposure to GenAI projects.                  Basic programming skills in Python.                  Familiarity with data processing frameworks like Apache Spark or similar.                  Understanding of SQL and basic database management systems.                  Some knowledge of event-driven architectures and real-time data processing.                  Exposure to containerization technologies like Kubernetes and Docker.                  Awareness of data lakes and basic data lake management principles.                  Some familiarity with LLM frameworks such as Langchain and Semantic Kernel.                  Experience with cloud computing platforms such as Azure or AWS is a plus.                  Strong analytical and problem-solving skills.                  Good communication and collaboration abilities.                  Ability to work in a fast-paced and dynamic environment.                  Nice to Have Skills:                  Exposure to additional technologies such as Databricks, Azure AI Search, Azure OpenAI, Azure Event Hub, Azure Data Lake Storage, AWS Open Search, AWS Bedrock, AWS Event Bridge, AWS S3, Azure Key Vault,DataDog, and Splunk.          ",1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Accounting / Auditing,"Computer science, Assurance, Data management, Analytical, Data processing, Troubleshooting, Risk management, Business intelligence, Monitoring, SQL",-,9am-6pm,"Full Time, Permanent",PwC Service Delivery Center,Organization,PwC Service Delivery Center,https://img.naukri.com/logo_images/v3/1428182.gif,"Hyderabad, Bengaluru","Hyderabad, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer: Data Platforms-Google,"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe. You'll work with visionaries across multiple industries to improve the hybrid cloud and Al journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience. Your Role and Responsibilities Work Time : 2pm to 11pm  6+ years of industry work experience  Experience extracting data from a variety of sources, and a desire to expand those skills  Worked on Google Looker tool  Worked on Big Query and GCP technologies  Strong SQL and Spark knowledge  Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark  Knowledge of Financial Accounting is a bonus  Work independently with cross functional team and drive towards the resolution  Experience with Object oriented programming using python and its design patterns  Experience handling Unix systems, for optimal usage to host enterprise web applications GCP certifications preferred.  Payments Industry Background good to have  Candidate who has been part to google Cloud Migration is an ideal Fit""  Required Technical and Professional Expertise Intuitive individual with an ability to manage change and proven time management Proven interpersonal skills while contributing to team effort by accomplishing related results as needed Up-to-date technical knowledge by attending educational workshops, reviewing publications Preferred Technical and Professional Expertise 6+ years of industry work experience  Experience extracting data from a variety of sources, and a desire to expand those skills  Worked on Google Looker tool. ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data analysis, sql, spark, object oriented programming, hadoop, hive, python, power bi, data warehousing, google, machine learning, data engineering, business intelligence, sql server, tableau, gcp, financial accounting, bigquery, data visualization, etl, aws, unix",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Staff Engineer,"     You have an analytical mind and a passion for the craft of software engineering, and you love solving problems and learning new things     Youre passionate about software quality and the opportunity to make a meaningful, widespread impact through the work you do every day is important to you     You are looking for an opportunity to work for a company whose software is utilized by every Fortune 500 company and significantly impacts every industry     Our product is used across verticals including education, finance, retail, government, airlines, etc, enabling critical line of business use cases     You want to be a part of a collaborative environment whose teams care about the product they are creating, how they create it, and the impact it has on customers business objectives                 What is the primary need, technical challenge, or most significant problem for this role               Workspace ONE is a digital workspace platform that simply and securely delivers and manages any app on any device by integrating access control, application management, and multi-platform endpoint management     It is available as a cloud service or for on-premises deployment     Our software scales to support millions of devices across multiple device platforms     We have a high bar for quality, performance, scale, availability and supportability - our customers are counting on us to keep their business running     We support complex use cases that span across VMware and services/capabilities provided by OS vendors     We integrate with customers enterprise infrastructure to provide a seamless management experience     You will be the senior most architect on a team that is solely focused on ensuring we meet the high quality standards that our product demands     This team will be responsible for building tools and services that enable us to automate the complex use cases that we support     Responsibilities for this team will also include ensuring that the product being built is testable including designing and refactoring core platform components to ensure so     Team members are expected to participate actively in product design discussions and work closely with functional engineering teams to ensure that the product is built in parallel with the integrations, services and tooling required to effectively test it in an automated fashion     You will be owning engineering initiatives and champion a culture of high ownership, continuous improvement, and engineering excellence.         Within the first 6-12 months, what are the 2-3 most important performance goals for you to be successful in this role         Product and implement designs that improve test automation in the product     Be very active in code reviews, holding the highest bar of engineering standards     Participate in recruiting activities that help grow the team         What assignments or requirements will you be performing on a regular basis         Architect and build frameworks, tools and services that enable us to automate the product     Automate complex testing scenarios and use cases to reduce test execution and triaging time and improve engineering efficiency     Drive automation efforts; build an automation platform that will enable team to write automation tests quickly and efficiently.     Work on initiatives that improve the testability and supportability of the code     Identify and address areas where test automation can be improved by either refactoring, building or adopting new tooling and technologies     Actively participating in design and code reviews from functional teams     Help grow and mentor a team of engineers             What are the benefits and perks of working at VMware             Employee Stock Purchase Plan     Medical Coverage, Retirement, and Parental Leave Plans for All Family Types     Generous Time Off Programs     40 hours of paid time to volunteer in your community     Rethinks Neurodiversity program to support parents raising children with learning or behavior challenges, or developmental disabilities     Financial contributions to your ongoing development (conference participation, training, course work, etc)     Healthy and local-inspired snacks in all our pantries     This job may require the candidate to travel and/or work from a facility that requires full vaccination prior to entry.     Category : Engineering and Technology     Subcategory: Software Engineering     Experience: Business Leadership   ",2.21E+11,22-11-2023,20-02-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Software Product,"VMware, Automation testing, Analytical, Test execution, HTTP, Product design, software quality, Continuous improvement, Purchase Manager, Principal",-,9am-6pm,"Full Time, Permanent",VMware,Organization,VMware,https://img.naukimg.com/logo_images/groups/v1/162630.gif,"Hyderabad, Ahmedabad, Bengaluru","Hyderabad, Ahmedabad, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Designer,"Project Role : Application Designer Project Role Description : Assist in defining requirements and designing applications to meet business process and application requirements.  Must have skills : Amazon Web Services (AWS) Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of fulltime education Summary :As an Application Designer, you will be responsible for assisting in defining requirements and designing applications to meet business process and application requirements using Amazon Web Services (AWS). Your typical day will involve working with cross-functional teams, analyzing business requirements, and designing scalable and secure applications.  Roles & Responsibilities: Collaborate with cross-functional teams to analyze business requirements and design scalable and secure applications using Amazon Web Services (AWS). Design and implement AWS infrastructure and services, including EC2, S3, RDS, and Lambda, to support application development and deployment. Ensure application security and compliance with industry standards and best practices, including experience with security tools such as AWS IAM, KMS, and CloudTrail. Develop and maintain technical documentation, including architecture diagrams, design documents, and deployment guides. Stay updated with the latest advancements in AWS and cloud computing, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with Amazon Web Services (AWS), including EC2, S3, RDS, and Lambda. Good To Have Skills:Experience with other cloud platforms such as Microsoft Azure or Google Cloud Platform. Strong understanding of cloud computing concepts and principles, including experience with infrastructure as code tools such as Terraform or CloudFormation. Experience with containerization technologies such as Docker and Kubernetes. Solid grasp of software development principles and practices, including experience with programming languages such as Java or Python. Additional Information: The candidate should have a minimum of 3 years of experience in Amazon Web Services (AWS). The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering scalable and secure cloud-based solutions. This position is based at our Bengaluru office. Qualification 15 years of fulltime education",80524911707,08-05-2024,06-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"software development, microsoft azure, amazon ec2, gcp, aws, kubernetes, python, aws iam, web services, application design, amazon rds, aws cloudformation, docker, application development, kms, java, lambda expressions, terraform, cloud computing, cloud trail, jira",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer with Cloud,"Work with cloud platforms to deploy, manage, and scale applications as needed. Troubleshoot and debug issues in production and testing environments. Stay updated with the latest technologies and best practices in Python development. Skills Required: Design, develop, and maintain robust Python-based backend systems and API Strong experience in any cloud deployment with Docker, fastapi, flask, GCP/Azure/AWS Collaborate with cross-functional teams to define, design, and ship new features. Implement and optimize performance-critical code for high-throughput and low-latency applications. Understand and work with the threading limitations of Python and multi-process architecture. Implement and maintain websockets for real-time communication between server and client applications. Utilize object-oriented programming principles to ensure scalability, reusability, and maintainability of codebase. Implement multi-tenancy solutions to support multiple users or tenants securely. Containerize applications using Docker for easy deployment and scalability. Primary Skills: Python, API Development, Data Structures, Algorithms  Docker, fastapi, flask, GCP/Azure/AWS Experience around libraries such as Numpy, Matplotlib, Pandas or similar will be a big benefit though not mandatory",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, algorithms, cloud development, cloud deployment, docker, pandas, amazon ec2, ansible cloud computing, git, matplotlib, data structures, terraform, flask, azure",-,9am-6pm,"Full Time, Permanent",JK Technosoft,Organization,JK Technosoft,https://img.naukimg.com/logo_images/groups/v1/467092.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Microsoft Azure Modern Data Platform, Python (Programming Language), Databricks Unified Data Analytics Platform Minimum  5  year(s) of experience is required Educational Qualification : Minimum 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform, ensuring cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services, collaborating with Integration Architects and Data Architects, and utilizing your expertise in Python and Databricks Unified Data Analytics Platform.  Roles & Responsibilities: Assist with the blueprint and design of the data platform, encompassing the relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Implement and maintain data pipelines using Microsoft Azure Data Services. Develop and maintain data processing and transformation jobs using Databricks Unified Data Analytics Platform. Troubleshoot and optimize data pipelines and data processing jobs for performance and scalability. Professional & Technical Skills: Must To Have Skills:Expertise in Microsoft Azure Data Services. Good To Have Skills:Proficiency in Python, Microsoft Azure Modern Data Platform, and Databricks Unified Data Analytics Platform. Strong understanding of data modeling and database design principles. Experience with data integration and ETL processes. Familiarity with data warehousing and data lake concepts. Knowledge of data security and compliance best practices. Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, data services, information technology, microsoft azure, data modeling, hive, data analytics, data warehousing, data architecture, machine learning, sql, database design, data bricks, java, spark, design principles, hadoop, etl, aws, data integration, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills : Informatica Intelligent Cloud Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : As per accenture standards Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Informatica Intelligent Cloud Services.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Informatica Intelligent Cloud Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data integration workflows using Informatica Intelligent Cloud Services. Troubleshoot and resolve data integration issues in a timely manner. Ensure data quality and integrity by implementing data validation and cleansing processes. Professional & Technical Skills: Must To Have Skills:Experience with Informatica Intelligent Cloud Services. Good To Have Skills:Experience with other data integration tools such as Talend or MuleSoft. Strong understanding of data integration concepts and techniques. Experience with data validation and cleansing processes. Familiarity with cloud-based data platforms such as AWS or Azure. Experience with SQL and relational databases. Additional Information: The candidate should have a minimum of 3 years of experience in Informatica Intelligent Cloud Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification As per accenture standards",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloud services, information technology, sql, informatica, data integration, c#, project management, python, talend, microsoft azure, data warehousing, sql server, docker, data quality, java, data modeling, mule esb, devops, linux, paas, jenkins, aws, cloud computing, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Microsoft Azure Modern Data Platform, Python (Programming Language), Databricks Unified Data Analytics Platform Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform, ensuring cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services, collaborating with Integration Architects and Data Architects, and utilizing your expertise in data platform components.  Roles & Responsibilities: Assist with the blueprint and design of the data platform, encompassing the relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Implement and maintain data platform components using Microsoft Azure Data Services. Develop and maintain data pipelines, ensuring data quality and integrity. Troubleshoot and resolve data platform issues, working with cross-functional teams as needed. Professional & Technical Skills: Must To Have Skills:Expertise in Microsoft Azure Data Services. Good To Have Skills:Experience with Microsoft Azure Modern Data Platform, Databricks Unified Data Analytics Platform, and Python (Programming Language). Strong understanding of data platform components and architecture. Experience with data pipeline development and maintenance. Ability to troubleshoot and resolve data platform issues. Solid grasp of data quality and integrity best practices. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, python, microsoft azure, data engineering, data quality, hive, data analytics, data warehousing, data architecture, machine learning, sql server, sql, nosql, amazon ec2, java, data modeling, spark, kafka, mysql, hadoop, sqoop, aws, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills : Informatica Intelligent Cloud Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : As per accenture standard Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Informatica Intelligent Cloud Services.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Informatica Intelligent Cloud Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data integration workflows using Informatica Intelligent Cloud Services. Troubleshoot and resolve data integration issues in a timely manner. Ensure data quality and integrity by implementing data validation and cleansing processes. Professional & Technical Skills: Must To Have Skills:Experience with Informatica Intelligent Cloud Services. Good To Have Skills:Experience with other ETL tools such as Talend or SSIS. Strong understanding of data integration concepts and techniques. Experience with data validation and cleansing processes. Familiarity with data modeling and database design principles. Additional Information: The candidate should have a minimum of 3 years of experience in Informatica Intelligent Cloud Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification As per accenture standard",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloud services, information technology, talend, informatica, data integration, c#, project management, python, microsoft azure, data warehousing, sql server, sql, docker, data quality, java, data modeling, devops, linux, paas, jenkins, etl, ssis, aws, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Microsoft Azure Modern Data Platform, Python (Programming Language), Databricks Unified Data Analytics Platform Minimum  7.5  year(s) of experience is required Educational Qualification : Minimum 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform, ensuring cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services, collaborating with Integration Architects and Data Architects, and utilizing your expertise in Python, Databricks Unified Data Analytics Platform, and Microsoft Azure Modern Data Platform.  Roles & Responsibilities: Assist with the blueprint and design of the data platform, encompassing the relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Microsoft Azure Data Services. Design and implement data storage solutions using Microsoft Azure Modern Data Platform. Utilize your expertise in Python and Databricks Unified Data Analytics Platform to develop and maintain data processing and analysis tools. Professional & Technical Skills: Must To Have Skills:Expertise in Microsoft Azure Data Services. Good To Have Skills:Proficiency in Python, Databricks Unified Data Analytics Platform, and Microsoft Azure Modern Data Platform. Strong understanding of data modeling and database design principles. Experience with data integration and ETL processes. Familiarity with data warehousing and business intelligence concepts. Additional Information: The candidate should have a minimum of 7.5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, data services, information technology, microsoft azure, data modeling, hive, data analytics, data warehousing, data architecture, machine learning, sql, database design, data bricks, java, spark, design patterns, design principles, etl, aws, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills : Informatica Intelligent Cloud Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : As per accenture standards Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Informatica Intelligent Cloud Services.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Informatica Intelligent Cloud Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data integration workflows using Informatica Intelligent Cloud Services. Troubleshoot and resolve data integration issues in a timely manner. Ensure data quality and integrity by implementing data validation and cleansing processes. Professional & Technical Skills: Must To Have Skills:Experience with Informatica Intelligent Cloud Services. Good To Have Skills:Experience with other data integration tools such as Talend or MuleSoft. Strong understanding of data integration concepts and techniques. Experience with data validation and cleansing processes. Familiarity with cloud-based data platforms such as AWS or Azure. Experience with SQL and relational databases. Additional Information: The candidate should have a minimum of 3 years of experience in Informatica Intelligent Cloud Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification As per accenture standards",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloud services, information technology, microsoft azure, informatica, data integration, c#, project management, python, talend, data warehousing, sql server, sql, docker, data quality, java, data modeling, mule esb, devops, linux, paas, jenkins, aws, cloud computing, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Machine learning Engineer,"Responsibilities Responsible for successful delivery of MLOps solutions and services in client consulting environments; Define key business problems to be solved; formulate high level solution approaches and identify data to solve those problems, develop, analyze/draw conclusions and present to client. Assist clients with operationalization metrics to track performance of ML Models Agile trained to manage team effort and track through JIRA High Impact Communication- Assesses the target audience need, prepares and practices a logical flow, answers audience questions appropriately and sticks to timeline. Technical and Professional Requirements: Technical knowledge- has expertise in cloud technologies, specifically MS Azure, and services with hands on coding to Python Programming - Expert and Experienced - 4 -5 years DevOps Working knowledge with implementation experience - 1 or 2 projects a minimum Hands-On MS Azure Cloud knowledge Understand and take requirements on Operationalization of ML Models from Data Scientist Help team with ML Pipelines from creation to execution List Azure services required for deployment, Azure Data bricks and Azure DevOps Setup Assist team to coding standards (flake8 etc) Guide team to debug on issues with pipeline failures Engage with Business / Stakeholders with status update on progress of development and issue fix Automation, Technology and Process Improvement for the deployed projects Setup Standards related to Coding, Pipelines and Documentation Adhere to KPI / SLA for Pipeline Run, Execution Research on new topics, services and enhancements in Cloud Technologies  Preferred Skills: Technology->Machine learning->data science  Technology->Machine Learning->Python  Additional Responsibilities: Masters degree in Computer Science Engineering, with Relevant experience in the field of MLOps / Cloud Domain experience in Capital Markets, Banking, Risk and Compliance etc. Exposure to US/ overseas markets is preferred Azure Certified DP100, AZ/AI900 Domain / Technical / Tools Knowledge: Object oriented programming, coding standards, architecture & design patterns, Config management, Package Management, Logging, documentation Experience in Test Driven Development and experience in using Pytest frameworks, git version control, Rest APIs Azure ML best practices in environment management, run time configurations (Azure ML & Databricks clusters), alerts. Experience designing and implementing ML Systems & pipelines, MLOps practices Exposure to event driven orchestration, Online Model deployment Contribute towards establishing best practices in MLOps Systems development Proficiency with data analysis tools (e.g., SQL, R & Python) High level understanding of database concepts/reporting & Data Science concepts Hands on experience in working with client IT/Business teams in gathering business requirement and converting into requirement for development team Experience in managing client relationship and developing business cases for opportunities Azure AZ-900 Certification with Azure Architecture understanding is a plus  Educational Requirements Bachelor of Engineering  Service Line Data & Analytics Unit *  Location of posting is subject to business requirements",90524905854,09-05-2024,07-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Python, design patterns, Machine learning, Package Management, Test Driven Development, Object oriented programming, Config management",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://www.naukri.com/hotjobs/images/v3/infosys_nov13.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Informatica Intelligent Cloud Services Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 years of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Informatica Intelligent Cloud Services.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Informatica Intelligent Cloud Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data integration workflows using Informatica Intelligent Cloud Services. Troubleshoot and resolve data integration issues in a timely manner. Ensure data quality and integrity by implementing data validation and cleansing processes.  Professional & Technical Skills: Must To Have Skills:Experience with Informatica Intelligent Cloud Services. Strong understanding of data integration workflows and processes. Experience with troubleshooting and resolving data integration issues. Experience with data validation and cleansing processes. Experience with cloud-based data platforms.  Additional Information: The candidate should have a minimum of 5 years of experience in Informatica Intelligent Cloud Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field. This position is based at our Hyderabad office. Qualification 15 years of full time education",80524911251,08-05-2024,06-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloud services, data engineering, troubleshooting, informatica, data integration, c#, python, project management, microsoft azure, data warehousing, machine learning, sql server, sql, docker, data quality, data modeling, devops, linux, aws, cloud computing, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer: Data Platforms-Google,"Introduction A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe. You'll work with visionaries across multiple industries to improve the hybrid cloud and Al journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience. Your Role and Responsibilities Work Time : 2pm to 11pm  6+ years of industry work experience  Experience extracting data from a variety of sources, and a desire to expand those skills  Worked on Google Looker tool  Worked on Big Query and GCP technologies  Strong SQL and Spark knowledge  Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark  Knowledge of Financial Accounting is a bonus  Work independently with cross functional team and drive towards the resolution  Experience with Object oriented programming using python and its design patterns  Experience handling Unix systems, for optimal usage to host enterprise web applications GCP certifications preferred.  Payments Industry Background good to have  Candidate who has been part to google Cloud Migration is an ideal Fit""  Required Technical and Professional Expertise Intuitive individual with an ability to manage change and proven time management Proven interpersonal skills while contributing to team effort by accomplishing related results as needed Up-to-date technical knowledge by attending educational workshops, reviewing publications Preferred Technical and Professional Expertise 6+ years of industry work experience  Experience extracting data from a variety of sources, and a desire to expand those skills  Worked on Google Looker tool. ",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data analysis, sql, spark, object oriented programming, hadoop, hive, python, power bi, data warehousing, google, machine learning, data engineering, business intelligence, sql server, tableau, gcp, financial accounting, bigquery, data visualization, etl, aws, unix",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Lead,"SUMMARY      We are hiring for  ?a?        Python Lead?       for one of our esteemed clients in    ?          Bangalore Location:        ? ?Location: Bangalore            ? ?Exp - 10+ years                ? ?Below are the Job Details:                  ? ?10+ years of experience in developing complex Python applications.                Experience in Python (and its variants such as Python Spark, Python Flask, Python Django, Python Tensorflow)              Expertise in Large Language Models (LLM) based prompt and response pair generation and validation              Solid experience in programming, data structures, and problem - solving            Good to have NLP, Generative AI, and AI/ML expertise              Excellent communication skills              Hands-on designing and architectural experience          ",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"hlookup, vlookup, artificial intelligence, python flask, tensorflow, java, spark, data structures, html, mysql, programming, ml, communication skills, architecture, python, sumif, natural language processing, large, pivot table, aiml, machine learning, javascript, django, application, countif, flask",-,9am-6pm,"Full Time, Permanent",2coms,Organization,2coms,https://img.naukimg.com/logo_images/groups/v1/467982.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Engineering Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Minimum 15 years of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and implementing data engineering solutions using your expertise in Data Engineering.  Roles & Responsibilities: Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Assist with the blueprint and design of the data platform components. Implement data engineering solutions using your expertise in Data Engineering. Develop and maintain data pipelines, data models, and data integration solutions. Ensure data quality and integrity by implementing data validation and testing procedures. Professional & Technical Skills: Must To Have Skills:Expertise in Data Engineering. Good To Have Skills:Experience with Big Data technologies such as Hadoop, Spark, and Kafka. Strong understanding of data modeling and database design principles. Experience with ETL tools such as Informatica, Talend, or DataStage. Proficiency in programming languages such as Python, Java, or Scala. Additional Information: The candidate should have a minimum of 5 years of experience in Data Engineering. The ideal candidate will possess a strong educational background in Computer Science, Information Technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of full time education",80524913340,08-05-2024,06-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"information technology, data engineering, database design, data modeling, design principles, hive, python, datastage, scala, talend, machine learning, sql, data quality, java, spark, design patterns, kafka, hadoop, big data, etl, aws, informatica, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer: Data Platforms-Google,"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe. You'll work with visionaries across multiple industries to improve the hybrid cloud and Al journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience. Your Role and Responsibilities Work Time : 2pm to 11pm  6+ years of industry work experience  Experience extracting data from a variety of sources, and a desire to expand those skills  Worked on Google Looker tool  Worked on Big Query and GCP technologies  Strong SQL and Spark knowledge  Excellent Data Analysis skills. Must be comfortable with querying and analyzing large amount of data on Hadoop HDFS using Hive and Spark  Knowledge of Financial Accounting is a bonus  Work independently with cross functional team and drive towards the resolution  Experience with Object oriented programming using python and its design patterns  Experience handling Unix systems, for optimal usage to host enterprise web applications GCP certifications preferred.  Payments Industry Background good to have  Candidate who has been part to google Cloud Migration is an ideal Fit""  Required Technical and Professional Expertise Intuitive individual with an ability to manage change and proven time management Proven interpersonal skills while contributing to team effort by accomplishing related results as needed Up-to-date technical knowledge by attending educational workshops, reviewing publications Preferred Technical and Professional Expertise 6+ years of industry work experience  Experience extracting data from a variety of sources, and a desire to expand those skills  Worked on Google Looker tool. ",80524908679,08-05-2024,06-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data analysis, sql, spark, bigquery, object oriented programming, hive, python, power bi, data warehousing, google, machine learning, data engineering, business intelligence, sql server, tableau, gcp, financial accounting, hadoop, data visualization, etl, aws, unix",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Designer,"Project Role : Application Designer Project Role Description : Assist in defining requirements and designing applications to meet business process and application requirements.  Must have skills : Amazon Web Services (AWS) Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of fulltime education Summary :As an Application Designer, you will be responsible for assisting in defining requirements and designing applications to meet business process and application requirements using Amazon Web Services (AWS). Your typical day will involve working with cross-functional teams, analyzing business requirements, and designing scalable and secure applications on AWS.  Roles & Responsibilities: Design and develop scalable, secure, and highly available applications on AWS. Collaborate with cross-functional teams to analyze business requirements and design solutions that meet business process and application requirements. Ensure the application design is aligned with AWS best practices and security standards. Provide technical guidance and mentorship to junior team members. Stay updated with the latest advancements in AWS services and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Strong experience in designing and developing applications on AWS. Good To Have Skills:Experience with other cloud platforms such as Microsoft Azure or Google Cloud Platform. Strong understanding of AWS services such as EC2, S3, Lambda, RDS, and DynamoDB. Experience with AWS deployment tools such as CloudFormation and Terraform. Experience with DevOps practices and tools such as Jenkins, Git, and Docker. Solid understanding of application design patterns and best practices for scalability, security, and performance. Additional Information: The candidate should have a minimum of 3 years of experience in designing and developing applications on AWS. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering scalable and secure applications on AWS. This position is based at our Bengaluru office. Qualification 15 years of fulltime education",30524904433,03-05-2024,01-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"aws iam, application design, microsoft azure, design patterns, aws, python, software development, dynamo db, web services, amazon rds, aws cloudformation, hibernate, docker, amazon ec2, git, java, lambda expressions, gcp, devops, jenkins, terraform, jira",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Designer,"Project Role : Application Designer Project Role Description : Assist in defining requirements and designing applications to meet business process and application requirements.  Must have skills : Amazon Web Services (AWS) Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of fulltime education Summary :As an Application Designer, you will be responsible for assisting in defining requirements and designing applications to meet business process and application requirements using Amazon Web Services (AWS). Your typical day will involve working with cross-functional teams, analyzing business requirements, and designing scalable and secure applications on AWS.  Roles & Responsibilities: Design and develop scalable, secure, and highly available applications on AWS. Collaborate with cross-functional teams to analyze business requirements and design solutions that meet business process and application requirements. Ensure the application design is aligned with AWS best practices and security standards. Provide technical guidance and mentorship to junior team members. Stay updated with the latest advancements in AWS services and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Strong experience in designing and developing applications on AWS. Good To Have Skills:Experience with other cloud platforms such as Microsoft Azure or Google Cloud Platform. Strong understanding of AWS services such as EC2, S3, Lambda, RDS, and DynamoDB. Experience with AWS deployment tools such as CloudFormation and Terraform. Experience with DevOps practices and tools such as Jenkins, Git, and Docker. Solid understanding of application design patterns and best practices for scalability, security, and performance. Additional Information: The candidate should have a minimum of 3 years of experience in designing and developing applications on AWS. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering scalable and secure applications on AWS. This position is based at our Bengaluru office. Qualification 15 years of fulltime education",10524907867,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"aws iam, application design, docker, design patterns, aws, python, software development, dynamo db, web services, microsoft azure, amazon rds, aws cloudformation, hibernate, amazon ec2, git, java, lambda expressions, gcp, devops, jenkins, terraform, jira",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
CLOUD DEVELOPER,"     Responsible for the development of user interfaces written against popular JavaScript     frameworks;     A successful candidate will have a strong software development background;     Responsible for development of RESTful web services that host data to production systems;     Responsible for rapid development and prototyping of analytical processes in support of several     client initiatives;     Responsible for assisting in the monitoring and management of deployed environments;     Senior software developer, will be a member of a larger team to participate in digital     transformation initiatives;     Support and develop new features on production systems;     Develop integrations through APIs;     Delivery to web and cloud platforms;     Support internal business clients on issue resolution in a global operations environment;     Assist in the development of cloud-based and IoT solutions;     Collaborate with vendor team members, stakeholders and software development teams to     develop and support applications.     Requirements:     Container Development - Docker, Kubernetes, Kubernetes as Service (EKS, GKE, PKS, AKS);     Ability to effectively manage several projects simultaneously in a rapidly paced environment;     Proficiency with Amazon Web Services (AWS)/Azure/GCP is required;     Willing to go the extra mile ensuring that time critical requirements are met;     Maintain technology partnerships and drive impactful outcomes;     Linux/Unix systems;     Bachelor s Degree in Computer Science, Computer Engineering or a closely related field;     Excellent knowledge of Customer s software development cycle and OM processes;     Ability to code within AWS environments;     Highly mission-oriented;     Demonstrated experience with Java development;     Has worked with similar technologies listed below;     Excellent writing skills with ability to express strategy, technical knowledge and processes in an easily understood presentation;   ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Accounting / Auditing,"Unix, Linux, Architecture, Data management, GCP, Analytical, Javascript, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",PwC Service Delivery Center,Organization,PwC Service Delivery Center,https://img.naukri.com/logo_images/v3/1428182.gif,Kolkata,Kolkata,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
C++/ Python Developer,"  Position Responsibilities: Assists in oversight of software suppliers in development of airborne systems software Supports review and approval of suppliers software lifecycle data throughout software lifecycle processes; requirements, development/implementation, verification and certification phases Assists showing of compliance in accordance with FAA regulations associated with software development Assist in stage of involvement (SOI) reviews/audits, as well as Requirements Reviews, Design Reviews, and Tracebility Reviews Work collaboratively with suppliers to ensure that each system operates as designed and free of safety issues and quality problems Review and disposition problem reports written by Systems Engineers and Suppliers Assists in writing technical summaries to support compliance finding leading to software certification Works under general supervision Basic Qualifications (Required Skills/Experience): Bachelor, Master degree from an accredited course of study, in Computer Engineering, Software Engineering, Computer Science, or other technical degree 8+ years of experience in software engineering 8+ years of experience with software development using either C++, or Python 5+ years' of experience in RTCA DO-178(B/C)/DO-330 Software Considerations in Airborne Systems and Equipment Certification Preferred Qualifications (Desired Skills/Experience): 8 or more years' related work experience or an equivalent combination of education and experience Experience in activities to develop, document and maintain architectures, requirements, algorithms, interfaces and designs for software systems",70524008533,07-05-2024,05-08-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Defence & Aerospace,"C++, DO-330, Python Development, DO 178, RTCA",-,9am-6pm,"Full Time, Permanent",RGBSI,Organization,RGBSI,https://www.naukri.com/hotjobs/images/v3/rapid_apr22.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Java + Reactive programming,"     Design, cloud native applications using the Google Cloud Platform services and various open-source frameworks     Debug and fix issues arising in Java application with reactive programming and deployment stack deployed on GCP.     Hands on must have experience with containers - Docker, Kubernetes on any cloud technologies is fine     Hands on experience with Google Cloud Platform services like Dataflow, Pub/Sub, Compute engine, Kubernetes Engine, File Store, Cloud SQL, and Bigtable is required.     Experience in SCM technologies such as Git     Experience using databases - eg, SQL (MS SQL Server, Oracle) or NoSQL     Experience working with teams using Agile methodologies, demonstrated ability to quickly reproduce, diagnose and trouble shoot complex problems.     Understanding of lean and agile methods of software delivery and the principles and practices of BDD, ATDD and TDD.     Terraform automation for Infra as code, GitHub CI/CD tooling, and GCP Cloud Build.     Demonstrate knowledge of Scrum/Agile methodology.             Preferred Qualifications         Proven communication, collaboration, reporting, analytical and problem-solving skills.     Ability to understand technical specifications and analyze log files, experience in designing and building automation tools and systems.     Strong analytical skills in assessing functional and technical requirements and identifying high risk and key test areas.     Exhibits strong leadership qualities including excellent judgment, high standards, can dive deep and remain in touch with business details, drives innovation and delivers results     ",10524500144,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"MS SQL, Operational excellence, GCP, Cloud, Developer, Scrum, Oracle, SCM, Open source, Recruitment",-,9am-6pm,"Full Time, Permanent",NCR Corporation,Organization,NCR Corporation,https://img.naukimg.com/logo_images/groups/v1/44664.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Backend Developer - Python,"We are looking for a Backend Engineering lead whose highly talented individuals come from diverse backgrounds and are looking to solve real client problems at scale. We are looking for passionate techies with skills primarily around AWS and the latest tech stack who are aspiring for a fast-track career. Join us if you like to : - Build out a next-gen fintech product from ground 0 - Opportunity to influence the design of the product - Flexible and Hybrid work environment running out of Slack - Flat org structure - Stay up-to-date on industry trends and emerging technologies We'll need you to bring : - Bachelor's degree in Engineering or Master's degree in CS/ IT. - 3 to 4 years of experience  - Clean coding skills around Python - Knowledge of Redis.  - Experienced in SQL with Postgres and Good to have Influx DB. - Knowledge of NGINX or any other API gateway. - Strong AWS skills, techies with certifications from AWS are particularly encouraged to apply - AWS API Gateway, Route53, Lambda, EC2, RDS, SQS, CloudWatch, Cognito, QuickSight - Demonstratable experience around writing testable code, working with git, doing peer-level code review, daily standups, and generally championing software excellence",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Backend Development, RDS, Quick Sight, Route53, Redis, SQL, SQS, EC2, CloudWatch, Cognito, AWS API Gateway, Lambda, Python",-,9am-6pm,"Full Time, Permanent",Ventura Securities,Organization,Ventura Securities,https://img.naukimg.com/logo_images/groups/v1/705036.gif,"Mumbai, Thane, Navi Mumbai","Mumbai, Thane, Navi Mumbai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer / Backend Python Developer - Machine Learning,"Ability to write robust code in Python, Flask, Rest API Proven experience as a Machine Learning Engineer or similar role Understanding of data structures and software architecture Flask, Kubernetes/AWS, Grafana, PostgreSQL, NoSQL. Location: NCR, Banaglore",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Python, software architecture, NoSQL, PostgreSQL, data structures, Rest API, AWS, Grafana, Machine Learning, Kubernetes, Flask",-,9am-6pm,"Full Time, Permanent",4Bell Technology,Organization,4Bell Technology,-,"Mumbai, Navi Mumbai, Pune","Mumbai, Navi Mumbai, Pune",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Python (Programming Language), Microsoft Azure Analytics Services, Synapse Minimum  5  year(s) of experience is required Educational Qualification : Min 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services and other relevant data platform components.  Roles & Responsibilities: Assist with the design and implementation of data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines and ETL processes to support data integration and transformation. Implement data security and privacy measures to ensure compliance with regulatory requirements. Troubleshoot and resolve data platform issues, working with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Must To Have Skills:Strong understanding of data platform components and architecture. Good To Have Skills:Proficiency in Python programming language. Good To Have Skills:Experience with Microsoft Azure Analytics Services and Synapse. Experience with data integration and transformation using ETL processes. Knowledge of data security and privacy regulations. Strong analytical and problem-solving skills. Excellent communication and collaboration skills. Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office.",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, python, information technology, microsoft azure, etl, hive, c#, azure data lake, ssas, azure analytics, power bi, data warehousing, data pipeline, azure data factory, machine learning, sql server, sql azure, data modeling, spark, ssis, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Azure Python Specialist,"Role & responsibilities     Staging data from      ODS (Operational data store) and data process using databricks to prepare      scalable fact and dimension data structure.  Participate in the      development of cloud data warehouses, data as a service, and business      intelligence solutions.  Prepare platform and      build platform from scratch. Ability to provide      solutions that are forward-thinking in data and analytics. Deliver a quality product. Coding complex      U-SQL, Spark (Scala or Python). T-SQL Developing Modern      Data Warehouse solutions using Azure Stack (Azure Data Lake, Azure Data      Factory, Azure Databricks) Document wiki pages      and provide knowledge transition to backup team members. Performing      Operational support post go live activity. Experience in Azure stack (Azure Data Lake, Azure Data Factory, Azure Databricks) --  Mandatory Good understanding of other Azure services like Azure Data Lake Analytics & U-SQL, Azure SQL DW and Azure Synapse Sql (Serverless)  Preferred candidate profile     6+ years experience in Azure + Python",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"AZURE, AWS, Python",-,9am-6pm,"Full Time, Permanent",Orcapod Consulting Services,Organization,Orcapod Consulting Services,https://img.naukri.com/logo_images/v3/602389.gif,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,7.5-12 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Big Data(Scala Spark AWS),"Hii, Opprtunity With Big 4 _ Permanent Payroll Location - Hyderabad Experience -6 to 10yrs for senior consultant role Mandatory skills-  Bigdata ,Spark, Scala  , AWS (Must) NP-30 Days (Strictly 30days, not considering more than 30days NP) Interested kindly share CV on mail  varsha.akiwate@pratyin.com  ",20524008060,02-05-2024,31-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"AWS, SCALA, Big Data, Spark",-,9am-6pm,"Full Time, Permanent",Pratyin Infotech,Organization,Pratyin Infotech,https://img.naukimg.com/logo_images/groups/v1/4619807.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Development Engineer,"Project Role : Software Development Engineer Project Role Description : Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work. Must have skills : Data Science Good to have skills : PySpark, Python (Programming Language) Minimum  7.5  year(s) of experience is required Educational Qualification : 1:Minimum 15 years of full time education Project Role :Software Development Engineer Project Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work. Must have Skills :Data ScienceGood to Have Skills : AWS CloudFormation, Python (Programming Language), PySparkJob Requirements :Key Responsibilities :1:Provide analytics insights and solutions to solve business problems that are specific to client requirements Leverage analytics within transformation projects in operations through maturity assessments 2:one-time analytics projects Support operations in fact-based decision making, RCA and strategic initiatives through data analysis, text analysis and statistical modeling Follow multiple approaches for project execution Technical Experience :1:Must have hands on experience on Data Science Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:1:Minimum 15 years of full time educationAdditional1:Should be able to work in any shiftAdditional Info :",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pyspark, data science, python, software development, aws cloudformation, c++, dbms, sql, docker, ansible, coding, java, git, spark, devops, linux, jenkins, html, mysql, data structures, mongodb, c, javascript, sql server, amazon ec2, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS DevOps and Kafka Developer,"Responsibilities A day in the life of an Infoscion As part of the Infosys consulting team, your primary role would be to lead the engagement effort of providing high-quality and value-adding consulting solutions to customers at different stages- from problem definition to diagnosis to solution design, development and deployment.  You will review the proposals prepared by consultants, provide guidance, and analyze the solutions defined for the client business problems to identify any potential risks and issues.  You will identify change Management requirements and propose a structured approach to client for managing the change using multiple communication mechanisms.  You will also coach and create a vision for the team, provide subject matter training for your focus areas, motivate and inspire team members through effective and timely feedback and recognition for high performance.  You would be a key contributor in unit-level and organizational initiatives with an objective of providing high-quality, value-adding consulting solutions to customers adhering to the guidelines and processes of the organization. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional Requirements: Primary skills:Cloud->AWS Administration(AWS IAAS),Devops->AWS DevOps,Devops->Azure  Preferred Skills:  Devops->Azure Cloud->AWS Administration(AWS IAAS) Devops->AWS DevOps  Additional Responsibilities: Good knowledge on software configuration management systems Strong business acumen, strategy and cross-industry thought leadership Awareness of latest technologies and Industry trends Logical thinking and problem solving skills along with an ability to collaborate Two or three industry domain knowledge  Understanding of the financial processes for various types of projects and the various pricing models available Client Interfacing skills Knowledge of SDLC and agile methodologies  Project and Team management  Educational Requirements Master Of Comp. Applications,Master Of Science,Master Of Technology,Master Of Engineering,Bachelor Of Comp. Applications,Bachelor Of Science,Bachelor Of Technology,Bachelor of Engineering  Service Line Engineering Services *  Location of posting is subject to business requirements ",60524904745,06-05-2024,04-08-2024,EducationalOccupationalCredential,108,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"DevOps, AWS DevOps, Azure Cloud, Project management, aws administration, AWS IAAS, software configuration management, SDLC",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://www.naukri.com/hotjobs/images/v3/infosys_nov13.gif,Thiruvananthapuram,Thiruvananthapuram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer Lead,"Hi Job seeker, Urgent Open Demand For Python Backend Developer in Apexon Orgnization. Job Location: Ahmadabad, Bengalore, Chennai, Hyderabad, Mumbai, Pune and    Coimbatore JD:   Proficiency in Python 7+ Years Experince Strong experience in backend development using both languages. Familiarity with functional programming principles in Python. API Development Knowledge of RESTful API design and implementation Proficiency with libraries/frameworks like Python, Django, and Django Rest Framework Identity and Access Management Configuration Implementation of SSO, Authorization, and Permission policy Database Skills: Proficiency in working with databases (SQL or NoSQL). Understanding of data modelling and query optimization. Collaboration and Communication Ability to work in a team environment. Effective communication skills. DevOps and Deployment: Experience with deployment tools (Docker, Kubernetes). AWS services including Amazon S3, EC2, ECS/EKS, Lambda, DynamoDB, and GCP Version Control: Proficient with Git and collaborative workflows. Problem-solving skills: Analytical mindset and ability to troubleshoot issues. Team Leader: Excellent communication skills and a cross-functional collaborative attitude. Excellent Delegating tasks and setting deadlines, tracking progress, and risk management. Facilitate technical team, removing impediments, ensuring product quality and continuous improvement. Bonus Skills (Not Required, but Desirable): Experience with R Frontend Exposure: Familiarity with front-end technologies (HTML, CSS, JavaScript). Understanding of how backend and frontend components interact. Knowledge of cloud platforms GCP and Azure Knowledge of Keycloak configuration with Okta /Azure SSO Interested candidate please reach out Us-  9014401520  OR Email the Update CV -  Nagendrababu.r@apexon.com",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Api Gateway, Dynamo Db, Aws Iam, Microservices, Python, Aws Cloud, Docker, Django, SQL, Kubernetes",-,9am-6pm,"Full Time, Permanent",Apexon,Organization,Apexon,https://www.naukri.com/hotjobs/images/v3/APEXON_Jan23.gif,"Pune, Ahmedabad, Bengaluru","Pune, Ahmedabad, Bengaluru",-,-,-,22.5-30 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Python Developer,"Exp in AWS, such as Lambda, RDS, and DynamoDB, with a strong grasp of cloud computing concepts & architectural best practices Leverage Python to build platforms and apps Exp in developing and integrating RESTful APIs & contribute to system arch Required Candidate profile 3-5 y AWS/Azure, 2-3y of exp in Python Python Generators &async operations multithreading, context managers, decorators, descriptors Exp to microservices, Redis caching, CI/CD pipelines & environments",2.10E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Python, NoSQL, Aws Cloud, Azure Functions, RDBMS, Azure Cloud, Django, Microsoft Azure, Node.Js, MongoDB, AWS, Microservices",-,9am-6pm,"Full Time, Permanent",Numino Labs,Organization,Numino Labs,https://img.naukimg.com/logo_images/groups/v1/4615451.gif,"Bengaluru,Karnataka","Bengaluru,Karnataka",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Microservices Developer,"         Java,  open-source technologies and hands on experience in micro services using advance Java concepts.           Experience in Spring boot and Spring cloud frameworks          Experience in cloud technologies like Kubernetes,  Docker etc          Experience in writing unit test cases using Junit,  TestNG          Familiar with agile development lifecycle and worked on the design/development projects          All code delivered adheres to secure coding practices and with adequate code coverage at a minimum of 70          Should have experience in the usage of version control tools like  git .            Experience in collaborative tools like GitHub,  Bitbucket etc.           Exposure/Experience in Google,  Amazon/Microsoft cloud computing platform          Experience/Exposure to NoSQL datastore(nice to have)               2-7 years of software development experience            Required to complete their coding work as per the design spec with the highest quality in the scheduled time.           Work under the guidance of a Technical Architect/Lead engineer            Technical Expertise required across the eCommerce Search engineering roles  :                    Hands on experience in any one of SOLR,  Elasticsearch,  Endeca platforms          Should have built multi-faced search and must have provided input on search index specific data modelling          Having search relevancy and ranking experience using AI/ML is an added plus          Exposure to building data pipelines and cloud native data processing is nice to have          Java,  open-source technologies and hands on experience in micro services using advance Java concepts.           Experience in Spring boot and Spring cloud frameworks          Experience in cloud technologies like Kubernetes,  Docker etc.           Experience in writing unit test cases using Junit,  TestNG          Familiar with agile development lifecycle and worked on the design/development projects          Good Knowledge in messaging systems such as Kafka/RabbitMQ/Tibco EMS          All code delivered adheres to secure coding practices and with adequate code coverage at a minimum of 70          Should have experience in the usage of version control tools like  git .            Experience in collaborative tools like GitHub,  Bitbucket etc.           Exposure/Experience in Google,  Amazon/Microsoft cloud computing platform          Experience/Exposure to NoSQL datastore(nice to have)      ",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Auto Components,"Cloud computing, Version control, GIT, Coding, E-commerce, Data processing, Test cases, microsoft, Open source, endeca",-,9am-6pm,"Full Time, Permanent",Ford,Organization,Ford,https://img.naukimg.com/logo_images/groups/v1/247012.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Lead Lead Software Developer,"Tao Digital India Private Limited is looking for Lead Lead Software Developer to join our dynamic team and embark on a rewarding career journey      Modifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.      Directing system testing and validation procedures.      Directing software programming and documentation development.      Consulting with departments or customers on project status and proposals.      Working with customers or departments on technical issues including software system design and maintenance.      Analyzing information to recommend and plan the installation of new systems or modifications of an existing system.      Consulting with engineering staff to evaluate software hardware interfaces and develop specifications and performance requirements.      Designing and developing software systems using scientific analysis and mathematical models to predict and measure outcomes and design consequences.      Preparing reports on programming project specifications, activities, or status.      Conferring with project managers to obtain information on limitations or capabilities    ",1.40E+11,14-03-2024,12-06-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"c#, algorithms, css, python, software development, hibernate, ajax, javascript, jquery, sql server, software programming, spring, java, asp.net, j2ee, .net, php, json, html, data structures, mysql, programming",-,9am-6pm,"Full Time, Permanent",Tao Digital,Organization,Tao Digital,https://img.naukimg.com/logo_images/groups/v1/8250187.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Lead - Java, Python, Big data, GCP","     Overall Responsibilities:        Lead the development and implementation of projects using emerging technologies    Mentor and guide team members to ensure the successful delivery of projects    Identify and evaluate new technology solutions to improve business processes    Collaborate with cross-functional teams to ensure alignment with the organization's overall strategy    Stay up-to-date with the latest technological advancements and industry trends            Skills:            Java,  Python,  Big data,  GCP          Bachelor s degree in engineering or computer science or equivalent    4 to 8 years  experience is required    Expert in Java,  Python programming,  python frameworks like Django,  Flask    Experience on Big Data (Spark Core,  Hive,  Airflow)    Experience in SQL    Familiar with GCP offerings,  experience building data pipelines on GCP a plus    Experience on Hadoop Architecture having knowledge on Hadoop,  Map Reduce,  HDFS.     UNIX shell scripting experience    Experience of defining and using CI/CD pipelines    Experience in caching and queuing stacks (Redis,  Kafka)    Exposure to logging systems such as ELF & visualization tools like Tableau,  a nice to have    Shown ability to develop and document technical and functional specifications and analyze software and system processing flows    Creative problem solving (Innovative)    Willingness to understand the business and participate in discussions about project requirements              Experience:        At least 4-8 years of experience in software development and leading technology projects    Proven track record of delivering projects using emerging technologies    Experience in mentoring and guiding junior team members    Experience in working with cross-functional teams            Day-to-Day Activities:        Manage the development and delivery of projects using emerging technologies    Provide technical guidance and mentorship to junior team members    Collaborate with cross-functional teams to ensure alignment with the organization's overall strategy    Evaluate and recommend new technology solutions to improve business processes    Stay up-to-date with the latest technological advancements and industry trends            Qualification:        Bachelor's or Master's degree in Computer Science,  Information Technology,  or related field    Relevant certifications in emerging technologies            Soft Skills:        Strong communication and leadership skills    Ability to work well under pressure and meet tight deadlines    Excellent interpersonal and team-working skills    Ability to effectively communicate technical information to non-technical stakeholders    Passionate about technology and a desire to stay up-to-date with the latest advancements.           S                                   YNECHRON S DIVERSITY & INCLUSION STATEMENT                                                              Diversity & Inclusion are fundamental to our culture,  and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer.  Our Diversity,  Equity,  and Inclusion (DEI) initiative  Same Difference  is committed to fostering an inclusive culture - promoting equality,  diversity and an environment that is respectful to all.  We strongly believe that a diverse workforce helps build stronger,  successful businesses as a global company.  We encourage applicants from across diverse backgrounds,  race,  ethnicities,  religion,  age,  marital status,  gender,  sexual orientations,  or disabilities to apply.  We empower our global workforce by offering flexible workplace arrangements,  mentoring,  internal mobility,  learning and development programs,  and more.                                                           All employment decisions at Synechron are based on business needs,  job requirements and individual qualifications,  without regard to the applicant s gender,  gender identity,  sexual orientation,  race,  ethnicity,  disabled or veteran status,  or any other characteristic protected by law                          .          Candidate Application Notice   ",2.70E+11,27-04-2024,26-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Financial Services,"Unix shell sripting, Tehnology solutions, Information tehnology, Manager Tehnology, GCP, Equity, big data, SQL, Python, Computer siene",-,9am-6pm,"Full Time, Permanent",Synechron,Organization,Synechron,https://img.naukimg.com/logo_images/groups/v1/419316.gif,"Gurugram, Bengaluru","Gurugram, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Warehouse ETL Testing Good to have skills : NA Minimum  2  year(s) of experience is required Educational Qualification : any graduate Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and performing Data Warehouse ETL Testing.  Roles & Responsibilities: Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Assist with the blueprint and design of the data platform components. Perform Data Warehouse ETL Testing to ensure data quality and integrity. Develop and maintain ETL processes and scripts for data integration and data transformation. Troubleshoot and resolve issues related to data integration and data transformation. Professional & Technical Skills: Must To Have Skills:Strong experience in Data Warehouse ETL Testing. Good To Have Skills:Experience with ETL tools like Informatica, Talend, or DataStage. Experience with SQL and database technologies like Oracle, SQL Server, or MySQL. Experience with scripting languages like Python or Perl. Experience with data modeling and data integration techniques. Strong understanding of data warehousing concepts and methodologies. Additional Information: The candidate should have a minimum of 2 years of experience in Data Warehouse ETL Testing. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification any graduate",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"trading, investment banking, capital market, derivatives, trade processing, python, oracle, datastage, talend, data warehousing, sql server, sql, data quality, data modeling, mysql, perl, data transformation, etl, informatica",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Warehouse ETL Testing Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and performing Data Warehouse ETL Testing.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Perform Data Warehouse ETL Testing. Develop and maintain data integration solutions. Ensure data quality and integrity through testing and validation. Professional & Technical Skills: Must To Have Skills:Data Warehouse ETL Testing. Good To Have Skills:Experience with data integration solutions. Strong understanding of data modeling and database design. Experience with ETL tools such as Informatica or Talend. Experience with SQL and scripting languages such as Python or Perl. Additional Information: The candidate should have a minimum of 3 years of experience in Data Warehouse ETL Testing. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field. This position is based at our Hyderabad office. Qualification Minimum 15 years of education",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"information technology, data warehouse etl testing, database design, data modeling, data integration, python, test management, automation testing, talend, data warehousing, sql, etl testing, data quality, perl, hadoop, agile, etl, big data, informatica, unix",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Engineer - WLAN Firmware / Platform stability,"           The person will support customers to debug WLAN Windows Snapdragon based products and will drive the WLAN customer issues from design phase to the mass production.        This person will help customers solve any issues found on all layers including WLAN SoC chip firmware, driver, protocol stack, WOS framework and applications.      In addition, this person will provide trainings to customers, review customer customization design and implementation and support marketing and sales team to win designs.      This person will work closely with worldwide development engineering team, the customer program managers and application engineering team.      This person can learn and master new WOS WLAN technology and QC software implementations very quickly.      Travels to customer site and other regional QC offices are needed.      It is a very challenging position requiring the candidate to have strong technical capability, solid customer skills and initiative to drive things forward.                                 Minimum Qualifications:                               Key words : C, WLAN, Windows Drivers       Strong C/C++ programming skills, Windows Driver experience     Familiar with 802.11 protocol and IP networking highly desirable.     Familiar with WinDBG debugging, Debugview, Windows NetadapterCx, WiFi Cx, 802.11 protocol and IP networking highly desirable.     Debugging skills in embedded environment and WLAN driver/firmware, WLAN protocol analysis     Experience on WLAN driver and Firmware development and customization is highly preferred     Experience on Windows device driver is a plus     Prior experience with Android/Windows Mobile is a plus     Experience on WLAN SoC firmware development is a plus - Knowledge of TCP/IP network is a plus     Effective communication and interpersonal skills are preferred     Ability to learn new technologies quickly; Self-motivated and work alone with high pressure from customers.                           Minimum Qualifications:                               Bachelors degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Applications Engineering, Software Development experience, or related work experience.         OR       Masters degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Applications Engineering, Software Development experience, or related work experience       OR       PhD in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Applications Engineering, Software Development experience, or related work experience.                               1+ year of any combination of academic and/or work experience with Programming Language such as C, C++, Java, Python, etc             1+ year of any combination of academic and/or work experience with debugging techniques.             Minimum 2-5 years of relevant work experience                   Minimum Qualifications:           Bachelors degree in Engineering, Information Systems, Computer Science, or related field.      Required: Bachelors degree in Computer Science and/or Electrical Engineering   ",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Telecom / ISP,"Electrical engineering, Staffing, SOC, Windows mobile, Protocol stack, WiFi, Application software, Python, Android",-,9am-6pm,"Full Time, Permanent","Qualcomm Technologies, Inc",Organization,"Qualcomm Technologies, Inc",https://img.naukimg.com/logo_images/groups/v1/356782.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"GCP Data Architect 

","  GCP Data Architect- Job Description   SN   Required Information   Details   1  Role  GCP Data Architect  2  Desired Experience Range  10  16 Years  4  Location of Requirement  Hyderabad (Primary)   Indore| Nagpur|Ahmedabad|Kolkata|Bhubaneshwar (Secondary)  Desired Skills -Technical/Behavioral   Primary Skill   ? Work experience as a GCP Data Architect or similar role   Experience in cloud architecture or systems engineering   Experience with identity management and networking segmentation across multiple VPC networks and projects   Experience leading large-scale network migrations to GCP   Experience establishing technical strategy and architecture at the enterprise level   Experience with one or more third-party networking products   Experience leading GCP Cloud project delivery   Experience working with infrastructure as code tools such as Ansible, Chef, or Terraform   Expertise in GCP services   Knowledge of security, identity, and access management   Knowledge of networking protocols such as TCP/IP and HTTP/S   Knowledge of network security design including segmentation, encryption, logging, and monitoring   Knowledge of network topologies, load balancing, and segmentation   Knowledge of programming languages including Python, Ruby, and JavaScript   Strong communication, presentation, and leadership skills ",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Java Script Ajax, GCP Data Architect, Ruby, Gcp Cloud, Python",-,9am-6pm,"Full Time, Permanent",Tata Consultancy Services (TCS),Organization,Tata Consultancy Services (TCS),https://img.naukimg.com/logo_images/groups/v1/223346.gif,"Kolkata, Hyderabad, Ahmedabad","Kolkata, Hyderabad, Ahmedabad",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : Min 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and utilizing Microsoft Azure Data Services to deliver impactful data-driven solutions.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Utilize Microsoft Azure Data Services to deliver impactful data-driven solutions. Develop and maintain data pipelines, data models, and data integration solutions. Design and implement data security and privacy measures to ensure data protection and compliance. Professional & Technical Skills: Must To Have Skills:Proficiency in Microsoft Azure Data Services. Strong understanding of data integration, data modeling, and data security. Experience with data pipeline development and maintenance. Experience with data integration solutions. Experience with data security and privacy measures. Additional Information: The candidate should have a minimum of 7.5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science or a related field. This position is based at our Hyderabad office.",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data modeling, data services, data security, microsoft azure, data integration, hive, amazon redshift, data warehousing, data pipeline, sql, java, spark, mysql, hadoop, big data, etl, sap hana, python, machine learning, sql server, nosql, amazon ec2, kafka, sqoop, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google BigQuery-Data Platform Engineer,"Project Role :Data Platform Engineer  Project Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills :Google BigQuery  Good to have skills :NA",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"machine learning, sql, data modeling, gcp, bigquery, hive, python, oracle, data analysis, airflow, data warehousing, sql server, plsql, tableau, java, spark, mysql, hadoop, etl, big data, aws, unix",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior IT Technologist,"       In this role, you will play a critical part in designing, implementing, and optimizing our data architecture and data engineering needs relating to our API Gateway and data mesh service               Your responsibilities will focus in four areas where integration and interoperability with our API Gateway data mesh are most important:                 interoperability with our data lake;      interoperability with some remaining independent data repositories in our business units;      interoperability with data used by our various product development teams across Medtronic who need to share and integrate their data from other parts of the business; and            interoperability with application-to-application communication through the Gateway.         This role requires a deep understanding of both software and data engineering principles, as well as the ability to architect scalable and efficient data integration and interoperability solutions.             A Day in the Life                                       Collaborate with cross-functional teams to understand business requirements, data integration complexities, and security access needs and translate these into a scalable, secure, and performant data mesh architecture and service.                             Develop and maintain a comprehensive understanding and evolving data map of data sources, formats, and integration points within the organization.                              Work closely and effectively with the MDM, the data lake, and the data platform teams to develop the most efficient, performant, secure, and scalable API-driven data mesh for Medtronic s diverse and patient-critical business needs.                              Actively participate in prototypes, spikes, POCs and rapid development and full production services with both designs and hands-on implementation of data engineering solutions, including data pipelines, ETL processes, and data integration mechanisms.                              Utilize a variety of open source, cloud-based, and industry-standard tools and technologies for data processing, storage, and retrieval ensuring optimal performance and reliability.                              Design and implement data models that support both transactional and analytical needs.     Optimize these data models for performance and scalability, considering both current and future requirements.                              Effectively communicate complex technical concepts to non-technical stakeholders.                              Identify and address performance bottlenecks in data pipelines, queries, and API interfaces.                                  Help design, implement, and enforce data governance policies to ensure the integrity, security, and privacy of data.                         Be an integral member of the core DevSecOps/Shared Services product team and work with SREs to help deploy and operate our services 24x7x365 globally            Must Have: Minimum Requirements           Doctorate degree OR Masters degree          OR             Bachelor s degree and 2-5 years of related experience          OR          Associate degree and 6-9 years of related experience          OR          High School Diploma and 10+ years of related experience        Nice to Have               A degree in computer science or related field         Proven experience as a Data Architect with a focus on hands-on data engineering.                         Proficiency in data engineering technologies such as Snowflake, Apache Kafka, SQL, Snowpipe, and data warehousing solutions.                          Familiarity with two or more database technologies (Snowflake, Spark, Hadoop, Mongo, Oracle, etc).              Proven proficiency with programming and/or scripting languages such as Python, Java, Ruby, and JavaScript, and build tools such as Maven, Ant, Gradle or Ivy          Experience with WebMethods.io and/or MuleSoft platform(s)          Hands-on experience with configuration management and infrastructure-as-code frameworks and tools, such as Terraform, Ansible, Chef, Puppet, Salt, or AWS CloudFormation Templates          Experience working in an agile development lifecycle environment.          Working knowledge of Amazon Web Services (AWS) or other cloud technology platforms          Understand security and encryption technologies. Knowledge of authentication protocols including OpenID, OIDC, OAuth, SAML, and LDAP.          Application and system level debug and triage experience          Experience managing distributed runtime applications using programmatic interfaces.          Experience working in a medical regulated environment considered a plus.          Experience with automated monitoring frameworks such as InfluxDB, Grafana, OpenTelemetry, Telegraf, Fluentd, Prometheus, etc also a plus.       ",3.00E+11,30-01-2024,29-04-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Webmethods, LDAP, Configuration management, Javascript, Oracle, Apache, Open source, Ruby, SQL, Python",-,9am-6pm,"Full Time, Permanent",Medtronic,Organization,Medtronic,https://img.naukimg.com/logo_images/groups/v1/722108.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Infra Developer,"Role & responsibilities     Hands on Python Programming Expert in Cloud architecture with good understanding of regions, zones, etc, Cloud Monitoring and Troubleshooting (CloudWatch, Cloudtrial, Azure Monitor) Storage experience with AWS S3, Azure Blob Storage Cloud Deployment and Automation (CI/CD, Configuration Management) Expert  in Cloud Infrastructure Management Both Native APIs and Infrastructure  as Code for (AWS, Azure) Infrastructure as Code (CloudFormation, ARM  Templates, Terraform). Should be comfortable in creating yaml  configurations to deploy custom cloud objects and deploy infrastructure  in both Azure and AWS Experience with k8 Microservices in cloud - AKS, EKS. Containerization and Orchestration (Docker, ECS, AKS, EKS)",90524009033,09-05-2024,07-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"S3, AWS, Python, Azure Blob, Cloudwatch, Cloudtrail",-,9am-6pm,"Full Time, Permanent",MSys Technologies,Organization,MSys Technologies,https://img.naukimg.com/logo_images/groups/v1/4610259.gif,"Pune, Bangalore Rural, Chennai","Pune, Bangalore Rural, Chennai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer," Global Pharma Tek is looking for Python Developer to join our dynamic team and embark on a rewarding career journey.        Coordinating with development teams to determine application requirements.      Writing scalable code using Python programming language.      Testing and debugging applications.      Developing back-end components.      Integrating user-facing elements using server-side logic.      Assessing and prioritizing client feature requests.      Integrating data storage solutions.      Reprogramming existing databases to improve functionality.      Developing digital tools to monitor online traffic.      Write effective, scalable code      Develop back-end components to improve responsiveness and overall performance      Integrate user-facing elements into applications      Test and debug programs      Improve functionality of existing systems      Implement security and data protection solutions      Assess and prioritize feature requests      Coordinate with internal teams to understand user requirements and provide technical solutions.    ",1.60E+11,16-04-2024,15-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,Python,-,9am-6pm,"Full Time, Permanent",Global Pharma Tek,Organization,Global Pharma Tek,https://img.naukimg.com/logo_images/groups/v1/1012164.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer - MSoW Offshore - Offshore,"DayToDay Responsbilities:  Develop software solutions by studying information needs; conferring with users; studying systems flow, data usage, and work processes; investigating problem areas; following the software development lifecycle. Design, develop, and implement software solutions. Analyze user needs and develop software solutions. Design and develop software systems using scientific analysis and mathematical models to predict and measure outcome and consequences of design. Investigate, analyze and make recommendations to management regarding technology improvements, upgrades and modifications. Develop and execute software test plans in order to identify software problems and their causes. Troubleshoot production problems related to software applications. Develop and maintain user manuals and guidelines. Train users in the proper use of software. Provide technical support to users.  Must Have: Must have skills:  At least 7+ years of experience of software development  Strong Expertise in Azure Cloud, (Data factory, Databricks), Python, Apache Airflow  For Senior roles, expertise in Spark  Exposure of Data frames, data transformation, creating Models (ER diagrams)  DB experience Oracle OR SQL DB queries  Healthcare domain knowledge or understanding Insurance Claims process  Prior on prem to Cloud Migration project done  DevOps (CI/CD pipeline)",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Azure Cloud, Apache Airflow, Data factory, Databricks, Oracle, SQL DB queries, Python",-,9am-6pm,"Full Time, Permanent",Ascendion Engineering Private Limited,Organization,Ascendion Engineering Private Limited,https://www.naukri.com/hotjobs/images/v3/ASE_oct22.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Python Developer ( ##UjAwMDg@NTYzMA*## ),"Experience - 7+ Years   Role - Senior Python Developer Location - 100% Remote  Timings  5:00AM to 02:00PM EST(2:00PM To 10:00PM IST) Duration - 6+ Months Job Description: Must have experience in Redis DB, LLM, and Data Science. Python Proficiency:  The candidate must have a strong command of Python, including advanced language features, libraries, and frameworks commonly used in web development, data analysis, and machine learning projects. They should be able to demonstrate their expertise through past projects and code samples.  Experience with Solar, Elastic OR Redis DB:  Given the importance of DB in caching, session management, and real-time data processing, the candidate should have hands-on experience working with one of these databases. They should understand how to design efficient data models, utilize the DB features effectively, and troubleshoot performance issues. Machine Learning (ML) Skills:  The candidate should possess a solid foundation in machine learning concepts, algorithms, and techniques. They should have experience building and deploying ML models using Python libraries such as TensorFlow, PyTorch, or scikit-learn. Additionally, familiarity with deep learning frameworks and natural language processing (NLP) would be advantageous, depending on the specific needs of the role. Data Science Expertise:  A strong understanding of data science principles and methodologies is essential. The candidate should be proficient in data manipulation, visualization, and analysis using libraries like pandas, NumPy, and Matplotlib. They should also have experience working with large datasets and implementing data-driven solutions to solve business problems. Adherence to Best Practices:  The candidate must demonstrate a commitment to writing clean, maintainable, and efficient code. This includes following PEP 8 guidelines for Python code style, using meaningful variable names, writing unit tests for critical components, and documenting code and processes effectively. E   oblem-solving Skills:  The ability to approach complex problems analytically, develop innovative solutions, and troubleshoot technical issues is crucial. The candidate should be able to demonstrate their problem-solving skills through real-world examples and technical discussions. Share your resume to:: Vishal Kumar  Vkumar26@fcsltd.com 9599556828 ",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Data Science, Redis, Python Development, LLM, NLP, Numpy",-,9am-6pm,"Full Time, Temporary/Contractual",FCS Software Solutions,Organization,FCS Software Solutions,https://img.naukimg.com/logo_images/groups/v1/284948.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Microsoft Power Platform Developer - PowerApps,"  Job Title     Qualifying Criteria  4 to 10 years of experience in Proficiency in Develop      Power Apps solutions (Enabling stakeholders to leverage Microsoft 365      offerings and specializing in Microsoft Power Platform) on the Microsoft      Power Platform, Setting up Infrastructure , Platform Transformation &      Modernization, Platform Governance Notice period / Joining duration  Immediately / Less      than 30 Days. Willing to work out of Bangalore  Hybrid model.   Job Description & Requirements   Must Haves Ability to deliver while working alongside a complex matrix of BAs, Architects, Operations, Security Consultants, and Product Team. Strong knowledge of Power Platform (Core), Microsoft Dynamics 365 CE, Power Apps, Power Automate Strong relevant experience on Power Platform Core (Dataverse/CDS, Model driven apps, Portals), Dynamics CRM / 365.  At least 2 Years of relevant experience on Power Apps & Power Automate/MS Flow. Experience/knowledge on implementing Power Virtual Agent, AI Builder, Robotic Process Automation/Desktop Flow. Strong development knowledge on Microsoft stack especially C#, ASP.NET, Azure and Office 365. Must perform coding & Development work. Strong knowledge of Power Platform (Core), Microsoft Dynamics 365 CE, Power Apps, Power Automate Strong platform delivery knowledge working with Architecture & operations team. Strong knowledge in platform integration, implementation & deployment experience.  Strong working knowledge on Azure (Azure Functions, Service Bus, Application insight, Power Shell).  Nice to Have 8 - 15 years of IT experience. Must have Power Platform/Dynamic 365 expert level experience in the IT industry. Strong working knowledge Azure DevOps build & release pipelines. Develop Power Apps solutions on the Microsoft Power Platform, including Common Data Service (CDS), Power Apps and Dynamics 365 CE Experience with Power Virtual Agent, AI Builder, Robotic Process Automation/Desktop Flow. Experience with building & deploying Power Apps component framework (PCF Control).  Analytically oriented to establish root causes of defects and incidents that may pop up during the initial environment set up and service onboarding of Power Platform and Dataverse (CDS) scaleup/scale out. Strong working knowledge of integrations to other data sources. Experience in architecting custom business solutions to client specifications. Effective working knowledge on the licensing model and the feature mapping across the power platform Nice to have: Experience with the CRM and Sales parts of MS D365. Bachelors/degree in information technology",1.91E+11,14-04-2024,13-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Dynamics 365, Power Platform, MS Flow, Powerapps, Power Automate, C#, Azure, Power Apps, Azure Logic Apps, Microsoft Dynamics, Dynamics, Dynamics CRM, Service Bus, Azure Functions, As.net, platform integration, Azure Automation, Azure DevOps",-,9am-6pm,"Full Time, Permanent",CGI,Organization,CGI,https://img.naukimg.com/logo_images/groups/v1/1402790.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
LT 47 Python with AI Developer,"5+ years of professional experience as a Python Developer with a significant focus on AI integration. Expertise in machine learning concepts and frameworks (TensorFlow, PyTorch, etc.). Proven proficiency in Python and experience with relevant libraries and frameworks (Django, Flask, etc.). Strong knowledge of RESTful APIs and experience integrating AI models into complex software applications. Experience with cloud platforms (AWS, Azure, Google Cloud) and containerization (Docker, Kubernetes). Excellent problem-solving and analytical skills. Strong leadership, communication, and teamwork skills. Location: Palakkad/Remote",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Django, AI, Python, Azure, PyTorch, Docker, AWS, Google Cloud, Kubernetes, TensorFlow",-,9am-6pm,"Full Time, Permanent",Leuwint Technologies,Organization,Leuwint Technologies,https://img.naukimg.com/logo_images/groups/v1/3704938.gif,Palakkad,Palakkad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Hiring Alteryx Developer,"Role & responsibilities    Minimum 5 years of experience as Alteryx Developer.   Ability to create data visualizations to report KPIs and metrics pull data from various data sources direct querying exploratory data analysis financial modelling and performing root cause analysis.  Resourceful and relentless independently capable of seeking information solving conceptual problems corralling resources and delivering results in challenging   situations.  MS Office proficiency Excel power   user Outlook Power Point SharePoint.  Strong knowledge of and experience with databases SQL SnowSQL etc  Extensive experience with automation technologies and data visualization   tools such as PowerBI Power   Automate Smartsheet workflow RPAs Alteryx or other BI tools.  Knowledge of data gathering cleaning   and transforming techniques preferred.   Preferred candidate profile   Hands on Power BI, Automate Smartsheet workflow/ RPA Alteryx tool Interested candiadtes can share thier resume at Neha.Chourasiya@ltimindtree.com",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Alteryx, Power Bi, Power automate, Tableau",-,9am-6pm,"Full Time, Permanent",Ltimindtree,Organization,Ltimindtree,https://img.naukimg.com/logo_images/groups/v1/7519247.gif,"Hyderabad, Pune","Hyderabad, Pune",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Developer (Lambda/Kinesis/Redshift),"   Qualification: Bachelors degree in Computer Science or related field or higher with a minimum 3-4 years of relevant experience.      Looking for experienced AWS Developer proficient in Lambda, Kinesis, and Redshift, adept at architecting and deploying scalable serverless applications.      Specialized in real-time data processing, data warehousing, and API development, with a strong focus on security, optimization, and collaborative problem-solving.      Responsibilities:        Design, develop, and deploy scalable and secure applications on the AWS platform.        Utilize AWS services such as Lambda, Kinesis, and Redshift to implement robust solutions.        Architect and implement serverless applications using AWS Lambda for efficient and cost-effective computing.        Implement real-time data processing solutions using AWS Kinesis for streaming and analytics.        Design, implement, and optimize data warehousing solutions using Amazon      Redshift for large-scale data storage and analytics.    Implement event-driven architectures using AWS services to enable asynchronous communication between microservices.        Build and optimize ETL processes to transform and load data into Redshift for reporting and analysis.        Ensure the security and compliance of AWS applications, implementing best practices for data protection and access control.        Identify and implement optimizations for performance, scalability, and cost efficiency in AWS environments.        Collaborate with cross-functional teams, including software developers, system administrators, and data engineers, to deliver integrated solutions.        Create and maintain technical documentation for AWS architectures, configurations, and processes.        Implement CI/CD pipelines for automated testing, deployment, and continuous integration of AWS applications.        Troubleshoot and resolve issues related to AWS services, ensuring timely resolution of production incidents.      Must-Have Skills:        Proficiency in a wide range of AWS services, with a focus on Lambda, Kinesis, and Redshift.        Strong skills in designing, implementing, and optimizing data warehousing solutions using Amazon Redshift.      Good-to-Have Skills:    Knowledge of SAP BO / BW      Skills:      Amazon CloudFront      Amazon Relational Database Svc      Amazon Simple Email Service      Amazon Simple Storage Service      Amazon Elastic Cloud Compute        ",2.71E+11,27-12-2023,26-03-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Business process, continuous integration, Business consulting, CGI, HP data protector, digital transformation, Analytics, Technical documentation, Recruitment",-,9am-6pm,"Full Time, Permanent",CGI,Organization,CGI,https://img.naukimg.com/logo_images/groups/v1/1402790.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python / Django / SQL /AWS Developer," - Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent experience)     - 6-10 years of experience in Python, Django, SQL, and AWS, including successful project implementations             Candidate Qualifications             - Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent experience)     - 6-10 years of experience in Python, Django, SQL, and AWS, including successful project implementations             Required Skills               Python     Django     SQL     AWS   ",2.61E+11,26-08-2023,24-11-2023,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Networking, Django, Business solutions, Information technology, AWS, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer,"  Job Description: We are seeking a talented Python Developer to join our team. The ideal candidate should have 2-5 years of experience in Python development and a strong understanding of software engineering principles. As a Python Developer, you will be responsible for developing and maintaining high-quality software solutions, collaborating with cross-functional teams, and contributing to the overall success of our projects. Mandatory Skill Sets: Python, Django, MY SQL and Mongodb   Bachelor's degree in Computer Science, Engineering, or a related field. 2-6 years of professional experience in Python development. Strong understanding of software engineering principles and design patterns. Proficient in Python, with a good knowledge of its ecosystems. Experience with Django, Flask, or other Python frameworks. Familiarity with front-end technologies (such as JavaScript, HTML5, and CSS3) is a plus. Experience with relational databases (e.g., PostgreSQL, MySQL) and non-relational databases (e.g., MongoDB) is preferred. Understanding of code versioning tools, such as Git. Excellent problem-solving and communication skills. Ability to work independently and in a team environment. If you are looking for a change then please share CV at abhishek.verma_7464@nihilent.com along with below details. Name: Experience: Python Experience: Current CTC: Expected CTC: Notice Period: Current Location: Current Company: Relocate to Pune: Y/N: Ok with 5 days work from office:",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Django, MongoDB, Python, SQL",-,9am-6pm,"Full Time, Permanent",Nihilent,Organization,Nihilent,https://img.naukimg.com/logo_images/groups/v1/675066.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS cloud Developer,"                 ?                 Plan, design, and execute end-to-end data migration projects from various data sources to Amazon Aurora PostgreSQL and Amazon DynamoDB.                                  Collaborate with cross-functional teams to understand data requirements, perform data analysis, and define migration strategies.                                  Develop and implement data transformation and manipulation procedures using Talend, AWS Glue, and AWS DMS to ensure data accuracy and integrity during the migration process.                                  Optimize data migration workflows for efficiency and reliability, and monitor performance to identify and address potential bottlenecks.                                  Collaborate with database administrators, data engineers, and developers to troubleshoot and resolve data-related issues.                                  Ensure adherence to best practices, security standards, and compliance requirements throughout the data migration process. Provide documentation, technical guidance, and training to team members and stakeholders on data migration procedures and best practices.                 ",61223502376,06-12-2023,05-03-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Training, Data analysis, Data migration, Compliance, Cloud, Database, Dms, Troubleshooting, AWS, Monitoring",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Microsoft Developer,           Comfortable with using Python to develop web applications using either Django or Flask frameworks                     Write performant and efficient code in Python for the core calculation                      Comfortable to design and develop REST API using Python                      Comfortable with using cloud features on either AWS or Azure                     Comfortable with database - either SQL Azure or NoSQL DB.                      Write automated test scripts using Python testing libraries                     Be comfortable with Github and Github actions for source code repository and build / deployment pipeline                     Familiar with Agile / Scrum development practices            ,60524501506,06-05-2024,04-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"github, NoSQL, Test scripts, Agile scrum, Django, microsoft azure, SQL Azure, AWS, Testing, Python",-,9am-6pm,"Full Time, Permanent",ITC Infotech,Organization,ITC Infotech,https://img.naukimg.com/logo_images/groups/v1/4602035.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Big Data Developer,"     A Multinational Technology consulting partner that designs and builds custom digital commerce platforms to power large and small enterprises is looking for suitable candidates         Mandatory skills :       - 5+ Years of core experience on Big Data and at least 2 years of AWS experience       - Background Experience in Java and JavaScript.   - 1+ year Experience with AWS Managed Services implementations like EMR, Lambda Functions, Step Functions, etc.   - Experience working in data streaming (Kinesis, Firehose, etc.) and data transformation using AWS Glue (ETL mechanisms)   - Experience working with Amazon S3, Redshift data stores.   - Experience writing complex SQL queries.   - Experience with configuring and maintaining Tableau (or any other data visualization) server instances   - Experience in coding secure solutions and basic tenets of Information Security   - Knowledge of micro service designs and implementation       Good to have skills :       - Familiarity with Atlassian suite of productivity tools like JIRA, Confluence documentation, Bitbucket   ",31023500024,03-10-2023,01-01-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Printing & Publishing,"SQL queries, Managed services, Technology consulting, Coding, Information security, Javascript, data visualization, JIRA, big data, AWS",-,9am-6pm,"Full Time, Permanent",Beechi Vidya Kendra Trust,Organization,Beechi Vidya Kendra Trust,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Azure ACE Engineer,"   As an Azure Customer Experience Engineer, you are the primary support and engineering contact accountable for the customer s Azure support experience driving resolution of complex critical problems and supporting key customer projects on Azure. You will also act as the voice of the customer within Azure to escalate problems and to drive prioritization of platform/support improvement needs for customers.         In this role, you will work together with field teams (Customer Success Account Managers, Cloud Solution Architects, Support Escalation Engineers) and Azure engineering teams with our mission to turn Azure customers into fans with a world-class engineering-led support experience.             This role is flexible in that you can work up to 100% from home.                       Experience: 6+ years of demonstrated IT experience supporting and troubleshooting enterprise level, mission-critical applications resolving highly complex issues/situations and driving technical resolution across cross-functional organizations. At least 2+ yrs. of cloud experience.         Excellent Communication: Must have the ability to empathize with customers and convey confidence. Able to explain highly technical issues to varied audiences. Able to prioritize and advocate customer s needs to the proper channels. Take ownership and work towards a resolution.         Customer Obsession: Passion for customers and focus on delivering the right customer experience.         Growth Mindset: Openness and ability to learn new skills and technologies in a fast-paced environment.         Technical Skills: Some understanding of cloud computing technologies.              Optionally, demonstrated hands on experience in one or more of the following:             Core IaaS: Compute, Storage, Networking, High Availability         Data Platform and Bigdata: SQL Server, Azure SQL DB, HDInsight/Hadoop, Machine Learning, Azure Stream Analytics,         Azure Data Factory / Data Bricks         Azure PaaS Services: Redis Cache, Service Bus, Event Hub, Cloud Service, IoT suite, Mobile Apps, etc         Identity and Authentication: SSO/Federation, AD/Azure AD, ADFS, etc         Preferred but not required: Cosmos DB, Azure Kubernetes Service         Experience in one or more automation languages (PowerShell, Python, C#, Open Source)                             Technical Oriented             Utilizes engineering tools, customer telemetry and direct input. Flags the patterns of defects/signals in the products or issues across customers. Inform customers and partners about the complex thematic active issues, progress made on them, and discuss next steps.         Synthesizes feedback from customers and partners to learn about the product usage and identify and resolve feature and knowledge gaps and key performance indicators (KPIs) in the current product. Leads team in sharing insights and best practices with customers and partners on these service improvements. Recommends changes to content improvement or troubleshooting guides and develops metrics to evaluate the changes             Customer Solution Lifecycle Management             Utilizes cross systems to conduct health checks to ensure customer environment (eg, product, service, feature) is optimized and configured for deployment. Provides guidance to customers on understanding and implementing new versions. Serves as a connecting point between the engineering team and customers throughout the solution s lifecycle. Utilizes any resources to respond and resolve the immediate issues throughout the solution lifecycle. Proactively provides guidance to customers on designing configurations and deploying solutions on Microsoft platforms.         Handles complex escalations on customer issues from the support or field teams. Conducts impact analysis to determine the priority of the escalations. Conducts deep root cause analysis of the issues and converts them into improvement opportunities. Serves as an escalation resource in areas of subject matter expertise. Represents the team on highly complex issues and answers a large variety of technical questions and concerns.             Relationship/Experience Management             Acts as a voice of customers and leverages customers feedback to provide input on business plans developed by the relevant product and business groups. Identifies customer usage patterns and driving resolutions on reoccurring customer issues with engineering and product and business groups. Leads their team in engaging with feature and product groups on redesign/customer requested changes for Microsoft products. Closes the loop of feedback with the customers on product features.         Partners with other teams (eg, program managers, software engineers, product, customer service support [CSS] teams) to prioritize and drive resolutions of complex, high-impact customer issues and integration of customer features into the products. Leads discussions with stakeholders on customer progression and provides expertise on resolutions plans for common types of customer issues. Proactively communicates the translation of signals into actionable insights/trends to product teams to improve service reliability. Leads coordination with stakeholders (eg, engineering/product teams) to develop mechanisms that improve customer health engagement and reduce the turnover time.     ",80524501570,08-05-2024,06-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Software Product,"Cloud computing, Automation, Networking, Management, microsoft, Troubleshooting, Open source, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Microsoft,Organization,Microsoft,https://img.naukimg.com/logo_images/groups/v1/614576.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Sr Plsql Developer,"writing SQL queries, Cursors, Sub programs like Procedures, Functions,. end-to-end product development / implementation right from Requirement analysis,Documentation,customization to Testing. Oracle PL / SQL Database Progra,Oracle,SQL and PL / SQL",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Miscellaneous,Plsql Development,-,9am-6pm,"Full Time, Permanent",Technoflair Solutions India,Organization,Technoflair Solutions India,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Hadoop Developer,"Job Description Big Data Developer (3 -10 Years) Primary Skills Good understanding of concurrent software systems and building them in a way that is scalable, maintainable, and robust Experience in designing application solutions in hadoop ecosystem Deep understanding of the concepts in Hive, HDFS, yarn, Spark, Spark sql, Scala and Pyspark HDFS file formats and their use cases (eg Parquet, ORC, Sequence etc) Good knowledge in data warehousing system Experienced in any scripting language (SHELL, PYTHON) HortonWorks distribution and understanding on SQL engines (Tez, MR) Java/RestServices/Maven experience is a value addition Control-M development. Monitoring Resource utilization using Grafana Tool Good skillset to create automation scripts in Jenkins & knowledge in working on automating builds, test frameworks, app configuration etc Experience in implementing scalable applications with fully automated deployment and control using Bitbucket, Jenkins, ADO etc Skillset: Mandatory: Big Data Hive, HDFS, Spark, Scala, Pyspark. Secondary Skills Good to have : Schedulers (Control-M), ETL Tool Dataiku , Unix/Shell scripting , Knowledge of Integration services of FileIT/MQ and others , CI/CD tools Jenkins, Jira, ADO Devops tools suite , Oracle. Good to have Trade Domain knowledge and also any experience on Testing tools.",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Big Data Hive, Unix, Pyspark, Scala, Hadoop, HDFS, Integration services, Jira, Jenkins, Control-M development, Shell scripting, Spark, ETL, Oracle",-,9am-6pm,"Full Time, Permanent",Capgemini,Organization,Capgemini,https://www.naukri.com/hotjobs/images/v3/captech_jun20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer - Pandas/Numpy,"The Python developer is an intermediate level position responsible for implementation of new or revised application systems. The ideal candidate is a strong, result-oriented programmer with a can-do attitude. Required Skills and Abilities : - 6 years of hands-on experience with Python and the ability to write robust code . - Good understanding of python data structures, data transformation and algorithms . - Experience with python libraries like Pandas, numpy, requests etc . - Experience using python as a scripting language, for utility as well as framework creation . - Working knowledge of SCM tools like Git, Bitbucket etc - Should have working knowledge of RDBMS, interacting with DB using SQL as well as programmatically . - Experience of unit testing frameworks like pytest, unit test etc. & performance testing . - Good Communication and debugging skills Good To Have :  - Any Cloud Experience  - Big Data - Pyspark/Spark-Scala Knowledge - Working knowledge of Kubernetes, dockerization  - Experience with Terraform",60524905256,06-05-2024,04-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Python, Git, RDBMS, Pandas, Bitbucket, Algorithm, Data Structure, Numpy, SQL",-,9am-6pm,"Full Time, Permanent",Edgesoft,Organization,Edgesoft,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Data Platform Engineer,"             Installing, configuring, monitoring, maintaining and improving the performance of databases and data stores.                 Develops and configures tools to enable automation of database administration tasks                  Monitors performance statistics and creating reports                  Identify and investigate complex problems and issues and to recommend corrective actions                  Perform routine configuration, installation and reconfiguration of database related products.                          Requirements                         Experience of the above, working as a DBA                     Direct contact with customers/end-users on the phone and by email                     IT Service Desk experience                             Desired Experience                     Working in Azure (VMs, networks, SQL DB/Managed Instance, etc.)                 Analytics/ETL solutions (Synapse, Databricks, Data Factory, etc.)                 Developing and managing Power BI reports         ",10524501152,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Analytics / KPO / Research,"Change management, Automation, Managed services, Data management, Database administration, Healthcare, Troubleshooting, Analytics, Monitoring, SQL",-,9am-6pm,"Full Time, Permanent",Coeo Labs,Organization,Coeo Labs,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer - AI/ML Data Engineer,"     Study and transform data science prototypes     Design machine learning systems     Research and implement appropriate ML algorithms and tools     Develop machine learning applications according to requirements     Select appropriate datasets and data representation methods     Run machine learning tests and experiments     Perform statistical analysis and fine-tuning using test results     Train and retrain systems when necessary     Extend existing ML libraries and frameworks     Keep abreast of developments in the field         Must have:         3 years of experience working as AI/ML or Data Engineer     Should have demonstrable experience of setting up the E2E AI/ML system process.     Familiarity with Python (Numpy, Pandas), NodeJS/JS     Experience working with ML algorithms such as Supervised/unsupervised learning algorithms     Experience working with AWS Sagemaker         Nice to have:             Experience with data Visualization tools such as Power BI, Tableau     Familiarity with R & other data analytics tools.     ",70823500485,07-08-2023,05-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Retail,"github, tableau, Statistical analysis, data science, Machine learning, power bi, Data analytics, data visualization, AWS, Python",-,9am-6pm,"Full Time, Permanent",Delivery Solutions,Organization,Delivery Solutions,-,"Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Principal Software Engineer - AI/NLP,"     We are looking for a strong backend software engineers with strong technical expertise and familiarity with AI/ML technologies      This person will be part of a fast growing, driven team playing a key role in Boomi s future initiatives      Successful candidates will be expected to thrive in challenging environments and have a track record of delivering on commitments      Applicants are expected to work within a fast-paced Agile environment where priorities and deliverables change based on tactical and strategic business needs                  Responsibilities:            As a key member of the Agile team, collaborate throughout the software development lifecycle.          Guide and participate in the design, development, unit testing, and deployment of Boomi products and services including enhancements and/or resolution of any reported issues.          Work independently with a minimal level of guidance from technical leadership.          Should deliver on commitments and meet deadlines on complex initiatives.          Collaborate on organization-wide initiatives with other Agile development teams.          Research, validate, and recommend technologies to deliver robust, secure, and scalable services and architectures.          Set up, develop, and maintain best practices for the team including thorough code reviews.          Ability to ramp up quickly and efficiently on new technologies          Provide architecture and detailed design documents enabling grooming and planning.          Investigate and resolve complex customer issues.          Mentor team members.          Keep up on the latest developments in the field by continuous learning and proactively championing promising new methods relevant to the problems at hand.          Collaborate closely with data scientists, data engineers, and front-end engineers in a multidisciplinary work environment.                        Requirements:            Masters or PhD degree in computer science preferably with specialization in NLP/AI.          Knowledgeable about all the developments happening in the world of GPT technologies and LLMs          12+ years of development experience with Java, Python.          Proficiency in SQL and Database technologies          Strong prior experience of working with AWS services          Experience developing micro services applications and deploying them using Docker and Kubernetes.          Strong problem-solving skills with an emphasis on product development.          Strong analytical, written, and verbal communication skills.          Experience with developing highly scalable, high throughput web applications and backend systems.          Experience using Linux/Unix environments.          Knowledge of Infrastructure provisioning tools like Terraform, Cloud Formation, Ansible          Be a technical leader and architect of the group and mentor junior software engineers        ",2.61E+11,26-10-2023,24-01-2024,EducationalOccupationalCredential,144,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Unix, Automation, Backend, Front end, Linux, Unit testing, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Boomi Software,Organization,Boomi Software,https://img.naukimg.com/logo_images/groups/v1/6333375.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Staff Software Engineer, Cloud Platform","     Creating and operating multiple high scale services that can be deployed seamlessly across any public cloud          Managing substantial Kubernetes workloads that span across all major cloud providers and regions          Owning the design and development of multiple features in the cloud control plane and platform          Non-functional requirements (performance, scalability, reliability, availability, etc.)          Championing Agile software development and quality standards            What we re looking for:            Bachelor s degree in Computer Science, and/or 8+ years relevant industry          Experience working on large-scale distributed systems          Proficiency with Java and/or Golang programming        Knowledge of cloud environments (AWS, Azure, GCP)      Strong experience with container workloads (Kubernetes,Terraform, ect.)      Experience building and operating SaaS services/products is a plus      Open-source contributions are a plus    ",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Cloud computing, Automation, GCP, SAAS, Agile, Open source, Apache, Distribution system, Analytics",-,9am-6pm,"Full Time, Permanent",Startree,Organization,Startree,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Senior Software Engineer, Cloud Platform","       The platform engineering role requires strong software engineering fundamentals, knowledge of building and operating distributed systems and services. In this role, you ll be contributing to the following areas:             Creating and operating multiple high scale services that can be deployed seamlessly across any public cloud         Managing substantial Kubernetes workloads that span across all major cloud providers and regions          Owning the design and development of multiple features in the cloud control plane and platform          Non-functional requirements (performance, scalability, reliability, availability, etc.)         Championing Agile software development and quality standards           What we re looking for:           Bachelor s degree in Computer Science, and/or 6+ years relevant industry         Experience working on large-scale distributed systems         Proficiency with Java and/or Golang programming       Knowledge of cloud environments (AWS, Azure, GCP)     Strong experience with container workloads (Kubernetes,Terraform, ect.)     Experience building and operating SaaS services/products is a plus     Open-source contributions are a plus     ",20424501064,02-04-2024,01-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Cloud computing, Automation, GCP, SAAS, Agile, Open source, Apache, Distribution system, Analytics",-,9am-6pm,"Full Time, Permanent",Startree,Organization,Startree,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer - Data Science,"Responsibilities : - Develop and maintain Python-based applications, with a focus on Flask for web development. - Collaborate with cross-functional teams to understand project requirements and translate them into technical solutions. - Design, implement, and maintain data pipelines for collecting, processing, and analyzing large datasets. - Perform exploratory data analysis to identify trends, patterns, and insights. - Build machine learning models and algorithms to solve business problems and optimize processes. - Deploy and monitor data science solutions in production environments. - Conduct code reviews, testing, and debugging to ensure the quality and reliability of software applications. - Stay updated with the latest trends and advancements in Python development, data science, and machine learning. Requirements: - Bachelors or Master's degree in Computer Science, Data Science, or a related field. - 2+ years of professional experience in Python development and data science. - Strong proficiency in Python programming language with Flask framework and familiarity with relational databases (e.g., MySQL). - Proficiency in handling and manipulating various types of data, including structured and unstructured data, using Python libraries such as Pandas, NumPy, and Beautiful Soup. - Apply machine-learning techniques to analyze and extract insights from large text datasets, including social media data, customer feedback, and user interactions, to inform business decisions and strategy. - Knowledge of machine learning techniques and libraries (e.g., scikit-learn, TensorFlow). - Familiarity with creating and managing projects involving language models such as OpenAI's GPT (Generative Pre-trained Transformer) series, including ChatGPT and other prompt engineering tasks. - Use models for LLMs and related tasks to enhance Chabot's, virtual assistants, and other conversational AI applications, improving natural language understanding, conversation flow, and response generation. - Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform. - Experience with version control systems (e.g., Git). - Excellent problem-solving skills and attention to detail. - Strong communication and collaboration abilities",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Python, Data Science, Azure, Git, Flask framework, MySQL, AWS, Google Cloud",-,9am-6pm,"Full Time, Permanent",Volody Product,Organization,Volody Product,-,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer,"A Python Developer is a software developer who specializes in using the Python programming language to build applications, software tools, and data analysis systems. The job description for a Python Developer typically includes the following responsibilities:     Writing and Testing Code: The Python Developer is responsible for writing clean, maintainable, and efficient Python code, as well as testing and debugging code to ensure that it meets quality standards.     Designing and Developing Applications: The Python Developer designs and develops applications, software tools, and data analysis systems using Python frameworks and libraries.     Developing and Maintaining APIs: The Python Developer creates and maintains RESTful APIs that enable seamless integration with other systems and applications.     Analyzing and Manipulating Data: The Python Developer uses Python libraries and tools to analyze and manipulate data, including data cleaning, transformation, and visualization.Requirements:     Strong proficiency in Python, including knowledge of Python frameworks such as Django, Flask, and Pyramid.     Experience in software development, including writing and testing code, designing and developing applications, and collaborating with cross-functional teams.     Knowledge of front-end technologies such as HTML, CSS, and JavaScript is a plus.     Strong analytical and problem-solving skills.     Experience working in Agile and/or Scrum methodologies.     Familiarity with database systems such as MySQL, PostgreSQL, and MongoDB.     Excellent communication and collaboration skills.   ",1.51E+11,15-09-2023,14-12-2023,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"developer, Python",-,9am-6pm,"Full Time, Permanent",Pinnacle Seven Technologies,Organization,Pinnacle Seven Technologies,-,Mumbai,Mumbai,-,-,-,Unpaid P.M ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Backend Developer - Python,"We are looking for a Backend Engineering lead whose highly talented individuals come from diverse backgrounds and are looking to solve real client problems at scale. We are looking for passionate techies with skills primarily around AWS and the latest tech stack who are aspiring for a fast-track career. Join us if you like to : - Build out a next-gen fintech product from ground 0 - Opportunity to influence the design of the product - Flexible and Hybrid work environment running out of Slack - Flat org structure - Stay up-to-date on industry trends and emerging technologies We'll need you to bring : - Bachelor's degree in Engineering or Master's degree in CS/ IT. - 3 to 4 years of experience  - Clean coding skills around Python - Knowledge of Redis.  - Experienced in SQL with Postgres and Good to have Influx DB. - Knowledge of NGINX or any other API gateway. - Strong AWS skills, techies with certifications from AWS are particularly encouraged to apply - AWS API Gateway, Route53, Lambda, EC2, RDS, SQS, CloudWatch, Cognito, QuickSight - Demonstratable experience around writing testable code, working with git, doing peer-level code review, daily standups, and generally championing software excellence",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Accounting / Auditing,"Python, AWS Lambda, RDS, PostgreSQL, Route53, Redis, SQL, QuickSight, SQS, EC2, CloudWatch, Cognito, Backend Architecture, AWS, Lambda, API Gateway",-,9am-6pm,"Full Time, Permanent",Ventura Securities,Organization,Ventura Securities,https://img.naukimg.com/logo_images/groups/v1/705036.gif,"Mumbai, Thane, Navi Mumbai","Mumbai, Thane, Navi Mumbai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
PLC & AWS Developer,"Key Technologies and skillsets the role will encompass : - Must have a minimum of 3-4 years of code development in Python, RESTAPI.  - Exposure and a solid understanding of Database concepts and one NOSQL database would be good to have.  - Should have experience in deploying applications in AWS.  - AWS Lamda / Serverless architecture.  - Good to have work in a product development team and Identity management domain.  - You will be working within the product development Team, collaborating with Test, Operations, and product owners to ensure successful build implementation and integration of all tiers and communicate technical parameters and constraints to the relevant teams.  - As a member of the Development team you will also have the opportunity to mentor and coach other members of the team as well as play a part in design/architecture.  Location : - Anywhere in India/Multiple Locations",60524905170,06-05-2024,04-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"AWS, AWS Lambda, RESTAPI, REST API, NoSQL, PLC, DB, Python",-,9am-6pm,"Full Time, Permanent",Autocal,Organization,Autocal,-,"Mumbai, Delhi / NCR, Bengaluru","Mumbai, Delhi / NCR, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Python Developer," As a Python Developer, you will be an integral member of the technology team. You will focus on the development of the code for the ContractPodAi application and configure it as per architectural requirements. You will closely work with the development and integration team to develop, integrate, refactor and configure the application. You need to have a strong passion to code and understand the application architecture and should be able to apply your mind to provide desired solutions and contribute towards the betterment of the product to take it to the next level.             Required Competencies:         6+ Years of experience using Python.     Experience with Linux and shell scripting     Proficiency in Django, Django Rest Framework, Docker, Docker-compose, relational databases (e.g., PostgreSQL), Flask, Redis, Celery, and PostgreSQL.     Good to know about data modelling MongoDB.     Must have a good understanding of creating complex data pipeline.     Experience in creating/designing RESTful services and APIs.     Understanding of software development best practices, including version control (e.g., Git), automated testing, and continuous integration/continuous deployment (CI/CD).               Job Responsibilities:         Collaborate with product owners, business managers and other developers to understand user requirements and develop high-quality software solutions.     Write efficient, maintainable, and scalable data transformation code using Spark, Python, and SQL.     Develop and maintain documentation on data pipeline architecture, processes, data flows, and data dictionary.     Assist with development and perform support test and deployment of new features.     Work on crucial features, as part of the team on a 2-week iterative Sprint.                   ",1.21E+11,12-10-2023,10-01-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Financial Services,"Cloud computing, Version control, GIT, Linux, Postgresql, Django, Shell scripting, application architecture, SQL, Python",-,9am-6pm,"Full Time, Permanent",ContractPod,Organization,ContractPod,https://img.naukimg.com/logo_images/groups/v1/3309460.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AI Staff Engineer,"   We are looking for an experienced, passionate, and self-driven individual who possesses both a broad technical strategy and the ability to tackle architectural and modernization challenges. As an Ideal candidate will help build enterprise Machine Learning platform. They will work with a team of enthusiastic and dynamic ML engineers and Data scientists in building a platform to help Synopsys R&D teams to experiment, train models and build Gen AI & ML products.        You will be responsible for:         Building AI Platform for Synopsys to orchestrate enterprise-wide Data pipelines, ML training, and inferencing servers.     Develop ""AI App Store"" eco system to enable R&D teams to host Gen AI applications in Cloud     Develop capabilities to ship Clou Native (Containerized) AI applications/AI systems to on-premises customers     Orchestrate GPU Scheduling from within Kubernetes eco-system (e.g. Nvidia GPU Operator, MIG, and so on)     Create reliable and cost-effective Hybrid cloud architecture using cutting edge technologies (E.g. Kubernetes Cluster Federation, Azure Arc and so on)       Required Qualifications       BS/MS/PhD in Computer Science/Software Engineering or an equivalent degree     5+ years of total experience building systems software, enterprise software applications, ML applications and microservices     Expertise and/or experience in following programming languages : Python, and Go     Design complex distributed systems (High-level and low-level systems design)     In-Depth Kubernetes knowledge: Be able to deploy Kubernetes on-prem, and working experience with managed Kubernetes services (AKS/EKS/GKE)     Strong systems knowledge in Linux Kernel, CGroups, namespaces, and Docker     Experience with at least one cloud provider (AWS/GCP/Azure)     Ability to solve complex problems using efficient algorithms     Experience with using RDBMS (PostgreSQL preferred) for storing and queuing large sets of data       Nice to have:       Prior experience with AI/ML workflows and tools (PyTorch, ML Flow, AirFlow, )     Experience prototyping, experimenting, and testing with large datasets, and analytic data flows in production     Strong fundamentals in Statistics, Machine Learning, and/or Deep Learning     ",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Architecture, RDBMS, GCP, Linux kernel, Postgresql, Machine learning, Scheduling, Distribution system, Python",-,9am-6pm,"Full Time, Permanent",Synopsys,Organization,Synopsys,https://img.naukimg.com/logo_images/groups/v1/448420.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
DevSecOps Platform Engineer,"Your Role and Responsibilities At IBM, you will help deliver a DevSecOps platform with capabilities that enable organizations to deploy and scale AI applications that are secure, compliant, and trustworthy. As a DevSecOps Engineer you will work closely with the engineering team to design, build and test CI/CD automation which ensures that IBM Software meets rigorous security and compliance standards, and can be audited against those standards. Required Technical and Professional Expertise 5+ years of experience with Programming Languages: Go, Ansible, Bash Scripting, NodeJS/JavaScript (2-3 years) Experience in Containers: Docker, Artifactory Proficient in CICD Orchestration: Tekton, Jenkins, Travis Cloud exposure is highly desirable Preferred Technical and Professional Expertise IBM Cloud experience (or equivalent) IBM Cloud Databases (NoSQL, Cloudant) IBM Kubernetes Service Red Hat OpenShift on IBM Kubernetes Service IBM Secrets Manager IBM Cloud Object Store IBM Continuous Delivery Service Activity Tracker, Sysdig and LogDNA",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"golang, ci/cd, docker, cloudant, bash scripting, artifactory, kubernetes, continuous integration, ansible, git, java, devops, linux, jenkins, mysql, shell scripting, san, python, github, javascript, nosql, continuous delivery, node.js, devsecops, bash, terraform, travis, aws",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Scientist,"As a Data Scientist at IBM, you will help transform our clients' data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it's investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live. Your Role and Responsibilities Proof of Concept (POC) Development: Develop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations. Customer Engagement and Support: Act as a technical point of contact for customers, addressing their questions, concerns, and feedback. Provide technical support during the solution deployment phase and offer guidance on AI-related best practices and use cases. Documentation and Knowledge Sharing: Document solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members. Industry Trends and Innovation: Stay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation. Required Technical and Professional Expertise Technical Skills:  At least 2 years of experience in developing AI/ML solutions in Python.  Should have strong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Should be able to use libraries such as SciKit Learn, Pandas, Matplotlib, etc. Should have a good knowledge of relational databases and SQL. Should be able to use NoSQL Databases in managing data for analysis. Should be able to follow the Agile methodology of development along with the knowledge of tools like Github. Soft Skills:  Should have excellent interpersonal and communication skills.  Should be able to engage with stakeholders and team for analysis and implementation.  Should be able to learn continuously and stay updated with advancements in the field of AI. Preferred Technical and Professional Expertise Experience in setting up web services using Flask, Django, FastAPI, etc. Experience in designing and delivering AI solutions, with a focus on foundation models, large language models. Exposure to open source AI libraries. Experience in full AI project lifecycle, from prototyping to deployment in production environments. Familiarity with cloud platforms (e.g. IBM Cloud, AWS, Azure, GCP) and related services.",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, github, artificial intelligence, sql, flask, scikit-learn, ibm cloud, microsoft azure, huggingface, relational databases, nosql, pandas, tensorflow, open source, django, gcp, matplotlib, product development, pytorch, keras, aws, agile methodology, ml",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer: Data Platforms,"Your Role and Responsibilities As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs. Your primary responsibilities include: Design, build, optimize and support new and existing data models and ETL processes based on our clients business requirements. Build, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization. Coordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too. Required Technical and Professional Expertise Developed the Pysprk code for AWS Glue jobs and for EMR.. Worked on scalable distributed data system using Hadoop ecosystem in AWS EMR, MapR distribution.. Developed Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine). Developed Hadoop streaming Jobs using python for integrating python API supported applications.. Developed Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations.. Re - write some Hive queries to Spark SQL to reduce the overall batch time Preferred Technical and Professional Expertise Understanding of Devops. Experience in building scalable end-to-end data ingestion and processing solutions Experience with object-oriented and/or functional programming languages, such as Python, Java and Scala",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pyspark, spark, hadoop, python, mapr, hive, cloudera, scala, amazon redshift, apache pig, sql, java, aws emr, devops, linux, data ingestion, mysql, big data, etl, hbase, data analysis, oozie, machine learning, data engineering, amazon ec2, mapreduce, kafka, sqoop, aws, unix",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer: Data Platforms,"Your Role and Responsibilities As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs. Your primary responsibilities include: Design, build, optimize and support new and existing data models and ETL processes based on our clients business requirements. Build, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization. Coordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too. Required Technical and Professional Expertise Developed the Pysprk code for AWS Glue jobs and for EMR.. Worked on scalable distributed data system using Hadoop ecosystem in AWS EMR, MapR distribution.. Developed Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine). Developed Hadoop streaming Jobs using python for integrating python API supported applications.. Developed Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations.. Re - write some Hive queries to Spark SQL to reduce the overall batch time Preferred Technical and Professional Expertise Understanding of Devops. Experience in building scalable end-to-end data ingestion and processing solutions Experience with object-oriented and/or functional programming languages, such as Python, Java and Scala",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"scala, java, spark, devops, hadoop, hive, cloudera, amazon redshift, pyspark, apache pig, sql, aws emr, linux, data ingestion, mysql, big data, etl, hbase, python, data analysis, mapr, oozie, machine learning, data engineering, amazon ec2, mapreduce, kafka, sqoop, aws, unix",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
DevSecOps Platform Engineer,"Your Role and Responsibilities At IBM, you will help deliver a DevSecOps platform with capabilities that enable organizations to deploy and scale AI applications that are secure, compliant, and trustworthy. As a DevSecOps Engineer you will work closely with the engineering team to design, build and test CI/CD automation which ensures that IBM Software meets rigorous security and compliance standards, and can be audited against those standards. Required Technical and Professional Expertise 5+ years of experience with Programming Languages: Go, Ansible, Bash Scripting, NodeJS/JavaScript (2-3 years) Experience in Containers: Docker, Artifactory Proficient in CICD Orchestration: Tekton, Jenkins, Travis Cloud exposure is highly desirable Preferred Technical and Professional Expertise IBM Cloud experience (or equivalent) IBM Cloud Databases (NoSQL, Cloudant) IBM Kubernetes Service Red Hat OpenShift on IBM Kubernetes Service IBM Secrets Manager IBM Cloud Object Store IBM Continuous Delivery Service Activity Tracker, Sysdig and LogDNA",1.21E+11,12-05-2024,10-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"golang, docker, javascript, nosql, node.js, artifactory, kubernetes, continuous integration, ci/cd, ansible, git, java, devops, linux, jenkins, mysql, shell scripting, san, python, github, cloudant, continuous delivery, devsecops, bash, terraform, travis, aws, bash scripting",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 full years of education and necessarily degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Proficiency in programming languages such as Python or Java. Experience with data integration and ETL processes. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 full years of education and necessarily degree in engineering",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, information technology, microsoft azure, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, hadoop, etl, aws, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  12  year(s) of experience is required Educational Qualification : 15 full years of education and necessarily degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Proficiency in programming languages such as Python or Java. Experience with data integration and ETL processes. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 full years of education and necessarily degree in engineering",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,144,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, information technology, microsoft azure, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, hadoop, etl, aws, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 full years of education and necessarily degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Proficiency in programming languages such as Python or Java. Experience with data integration and ETL processes. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 full years of education and necessarily degree in engineering",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, information technology, microsoft azure, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, hadoop, etl, aws, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer - Python," Background Any Graduate           Skills- Python, Data Engineering, Scala, ETL, Apache Kafka, Kubernetes, Docker, ADF, Talend, Pentaho, Glue, Informatica, SSIS, Data Warehousing, MYSQL, MongoDB, Postgres SQL, Oracle, GraphDB, DevOps, Casandra, Agile Methodology, Data Quality, Azure, Snowflake, GITLab, Jenkins, RestAPI           3+ Years of professional Python experience     3+ Years of professional Data Engineering experience     3+ years of experience with Scala     3+ years of experience in Cloud ETL/ELT for creating data pipelines     3+ years of experience with Apache Kafka, Kubernetes, Docker     Any professional experience with any ETL\ELT tool (Talend, Pentaho, Glue, ADF, GCD Glue, ADF, Informatica,SSIS)     Any professional Data Warehousing\Data Modeling experience in any of the following areas: Dimensional, DataMart, DataVault, DataLake, DataMesh, GraphDB.     Any professional experience with the following databases: MSSQL, Azure, MySql, PostgreSQL, Snowflake, MongoDb, MariaDB, Casandra, Oracle.     ?     Proficient experience in Snowflake,Kafkapython     Knowledge of Agile methodologies and DevOps practices.     Knowledge of Snowflake, SnowSQL.     Knowledge of Datawarehouse Design Methodologies (3NF, Kimball, Data Vault)     Knowledge of cloud orchestration tools (Airflow, ADF)     Knowledge of CI/CD     Knowledge of Data Products and Rest APIs     Any Data Quality Experience (GreatExpectations , ydata , pandas) a plus.     Experience with CI/CD tools such as Jenkins, GitLab, Azure Data Factory or TravisCI a plus.       ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Data modeling, MySQL, Agile, Data quality, Informatica, Oracle, Apache, SSIS, Pentaho, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Developer,"Azure With Monitoring Tools (Splunk will be preferred, if not then any other monitoring tool like Grafana or Kibana or Prometheus is fine too) JD: Responsibilities: Collaborate with cross-functional teams to gather requirements, design cloud-based solutions, and develop technical specifications for Azure projects. Design, develop, and deploy cloud-native applications and services on the Microsoft Azure platform, leveraging Azure PaaS (Platform as a Service) and serverless technologies. Implement CI/CD (Continuous Integration/Continuous Deployment) pipelines using Azure DevOps or similar tools to automate build, test, and deployment processes. Develop and integrate APIs (Application Programming Interfaces) using Azure API Management, Azure Functions, and other Azure services to enable seamless communication between cloud applications and external systems. Design and implement data storage solutions using Azure SQL Database, Azure Cosmos DB, Azure Blob Storage, and other Azure data services, ensuring data security, scalability, and performance. Optimize application performance and reliability by implementing caching, load balancing, and monitoring solutions using Azure Application Insights, Azure Monitor, and other monitoring tools. Implement security best practices and compliance standards in Azure solutions, including identity and access management, encryption, and network security. Troubleshoot and debug issues in cloud-based applications, identifying root causes and implementing effective solutions to ensure system stability and availability. Stay updated on emerging Azure technologies, industry trends, and best practices in cloud development, and recommend innovative solutions to enhance our Azure capabilities. Collaborate with project managers, architects, and stakeholders to deliver projects on time and within budget, providing regular updates on progress and addressing any issues or concerns. Locations : Noida,Gurugram,Pune,Mumbai,Indore,Ahmedabad,Bengaluru,Hyderabad,Goa",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Kibana, Azure, Prometheus, Splunk, Grafana, Azure Cosmos DB, data security, Azure Blob Storage, Azure data services, Azure SQL Database",-,9am-6pm,"Full Time, Permanent",Forward Eye Technologies,Organization,Forward Eye Technologies,-,"Noida, Pune, Gurugram","Noida, Pune, Gurugram",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Developer,"Azure With Monitoring Tools (Splunk will be preferred, if not then any other monitoring tool like Grafana or Kibana or Prometheus is fine too) . Responsibilities: Collaborate with cross-functional teams to gather and analyze requirements, design cloud-based solutions, and develop technical specifications. Design, develop, and deploy cloud-native applications and services on the Microsoft Azure platform, leveraging Azure PaaS (Platform as a Service) and serverless technologies. Implement CI/CD (Continuous Integration/Continuous Deployment) pipelines using Azure DevOps or similar tools to automate build, test, and deployment processes. Develop and integrate APIs (Application Programming Interfaces) using Azure API Management, Azure Functions, and other Azure services to enable seamless communication between cloud applications and external systems. Design and implement data storage solutions using Azure SQL Database, Azure Cosmos DB, Azure Blob Storage, and other Azure data services, ensuring data security, scalability, and performance. Optimize application performance and reliability by implementing caching, load balancing, and monitoring solutions using Azure Application Insights, Azure Monitor, and other monitoring tools. Implement security best practices and compliance standards in Azure solutions, including identity and access management, encryption, and network security. Troubleshoot and debug issues in cloud-based applications, identifying root causes and implementing effective solutions to ensure system stability and availability. Stay updated on emerging Azure technologies, industry trends, and best practices in cloud development, applying new knowledge and skills to improve development processes and solutions. Collaborate with project managers, architects, and stakeholders to deliver projects on time and within budget, providing regular updates on progress and addressing any issues or concerns. Location-Noida,Gurugram,Pune,Mumbai,Indore,Ahmedabad,Bengaluru,Hyderabad,Goa",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Azure, Kibana, DevOps, Azure Cosmos DB, Azure API Management, CI/CD, Prometheus, Azure Blob Storage, Splunk, Grafana, Azure SQL Database",-,9am-6pm,"Full Time, Permanent",Forward Eye Technologies,Organization,Forward Eye Technologies,-,"Noida, Pune, Gurugram","Noida, Pune, Gurugram",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer,"   We are looking for a Platform Engineer, who is passionate about Golang/Serverless and building high-quality and scalable products. We believe in ownership and are looking for people driven to continuously ship new, impactful features and capabilities for our users and partners. Seasoned ability to design, develop and test new functionality, whilst ensuring the platform is always robust and scalable.             Key Responsibilities:         Setup, maintain and evolve the external routing procedures for user s deployments with Envoy or other options.     Build an internal private network for dynamically meshing together services on the platform.     Build Golang GRPC services from scratch capable of supporting tens of thousands of users, and the million+ to come.     Define infrastructure that can be torn down, failed over, and reconstituted from scratch using the principle of immutable infrastructure using Terraform and Ansible.     Interface with our TypeScript and GraphQL edge to expose your microservice APIs for both internal and potentially external consumption.     Architecting and building highly available and scalable architectures and backends.     Working with product and design to creatively solve complex problems.               Required Skills:         A strong understanding of distributed systems. You enjoy building fault-tolerant, resilient, and scalable services.     A solid intuition about how long your solutions will last. All systems age. In startups, we can hope for 2-3 orders of magnitude, or 12-18 months.     A sense of grit to dive into a problem, implement a solution, scale that solution, and replace it when needed.     1+ years of experience of relevant experience.     Solid Golang and Serverless experience.     Experience with testing tools and practices.     DevOps on AWS, GCP, DigitalOcean.     ",2.41E+11,24-08-2023,22-11-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Internet,"Networking, Testing tools, GCP, devops, Equity, Infrastructure, Architecting, Distribution system, AWS, Principal",-,9am-6pm,"Full Time, Permanent",Forneu,Organization,Forneu,-,"Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Systems Development Senior Analyst/Engineer,"         The        BI Developer       reports to the Business Intelligence Manager and is responsible for    combining raw information from disparate IT source systems into data models, reports and dashboards to deliver business insights. The successful candidate will be able to complete the full lifecycle of development of data from the ETL process through to final deliverable of dashboards into the organization.     To succeed in this BI Developer position, you should have strong analytical skills and the ability to develop and maintain data and security models using modern techniques. If you are detail-oriented, with excellent organizational skills and experience in this field, we d like to hear from you.          Responsibilities for a BI Developer position include:               Essential Duties and Responsibilities               Participate in business requirement gathering and solution documentation     Build required infrastructure for optimal pipeline management, including extraction, transformation and loading of data from various data sources using Azure Data Factory, Databricks and SQL technologies     Assemble and analyze large, complex sets of data that meet non-functional and functional business requirements     Implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes     Work with users to assist them with data-related technical issues     Implement and test data and security models     Prepare data for prescriptive and predictive modeling     Develop interactive visual reports, dashboards, charts, and measures with KPI scorecards using Microsoft Power BI desktop     Analyze, design, deploy, troubleshoot, and support Power BI solutions       Participate in user acceptance testing       Explore and implement ways to enhance data quality and reliability     Collaborate with data scientists and architects as needed                     Education               Bachelor s degree (B.S./B.A.) in computer science,      information systems, informatics, statistics or another quantitative field     or equivalent from a college or university with IT focused specialization.         A Master s Degree or    Data engineering certification (e.g, IBM Certified Data Engineer) is a plus                         Skills/Experience               5+ years experience as a BI Developer or    related experience in a global company with significant experience in hands-on technology delivery roles.              Strong data analytics background with experience in developing use cases, deep understanding of managing data and generating insights thru visualization       Background in custom build experience using Power BI Report Builder, Power BI Desktop, Power BI Service, Tabular Editor, ALM Toolkit and DAX Studio designing Power BI data models; including writing complex DAX, SQL queries and implementing role level security     Ability to understand data modeling, data schemas (normalized, flat, star, snowflake, etc.), query optimization, query profiling and query performance monitoring tools and techniques     Knowledge of programming languages (e.g. Java, AngularJS, Python)       Experience with workflow management and pipeline tools - Azure Data Factory and DevOps; storage technologies - Azure Data Warehouse and Data Lake; stream-processing systems - Event Hub and Stream Analytics; transformation tools - Databricks; visualization tools - PowerBI; and metadata management systems.         Experience with big data tools like Spark is a plus         Familiarity with Machine Learning and Deep Learning concepts are a plus       Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata     Experience working with unstructured datasets     Hands-on experience with optimizing performance of SQL queries and applications     Great numerical and analytical skills       Ability to collaborate with technical resources to influence algorithms and other technology for improved customer experience       ",2.31E+11,23-11-2023,21-02-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Industrial Equipment / Machinery,"Computer science, Data modeling, Process control, Data structures, Predictive modeling, Data quality, microsoft, Business intelligence, User acceptance testing, Analytics",-,9am-6pm,"Full Time, Permanent",Hillenbrand,Organization,Hillenbrand,-,"Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure ACE Engineer II,"         We are Azure Customer Experience Engineering a global Azure Engineering Support organization (part of Azure Customer Experience group) that is customer-obsessed, and support engaged, with an engineering mindset         As an Azure Customer Experience Engineer, you are the primary support and engineering contact accountable for the customer's Azure support experience driving resolution of complex critical problems and supporting key customer projects on Azure     You will also act as the voice of the customer within Azure to escalate problems and to drive prioritization of platform/support improvement needs for customers       In this role, you will work together with field teams (Customer Success Account Managers, Cloud Solution Architects, Support Escalation Engineers) and Azure engineering teams with our mission to turn Azure customers into fans with a world-class engineering-led support experience       This role is flexible in that you can work up to 100% from home.                       Experience:    6+ years of demonstrated IT experience supporting and troubleshooting enterprise level, mission-critical applications resolving highly complex issues/situations and driving technical resolution across cross-functional organizations. At least 2+ yrs. of cloud experience.         Excellent Communication: Must have the ability to empathize with customers and convey confidence. Able to explain highly technical issues to varied audiences. Able to prioritize and advocate customer s needs to the proper channels. Take ownership and work towards a resolution.         Customer Obsession: Passion for customers and focus on delivering the right customer experience.         Growth Mindset: Openness and ability to learn new skills and technologies in a fast-paced environment.         Technical Skills: Some understanding of cloud computing technologies.             Optionally, demonstrated hands on experience in one or more of the following:             Core IaaS: Compute, Storage, Networking, High Availability         Data Platform and Bigdata: SQL Server, Azure SQL DB, HDInsight/Hadoop, Machine Learning, Azure Stream Analytics, Azure         Data Factory / Data Bricks         Azure PaaS Services: Redis Cache, Service Bus, Event Hub, Cloud Service, IoT suite, Mobile Apps, etc.         Identity and Authentication: SSO/Federation, AD/Azure AD, ADFS, etc.         Preferred but not required: Cosmos DB, Azure Kubernetes Service         Experience in one or more automation languages (PowerShell, Python, C#, Open Source)                 Technical Oriented             Utilizes engineering tools, customer telemetry and direct input. Flags the patterns of defects/signals in the products or issues across customers. Inform customers and partners about the complex thematic active issues, progress made on them, and discuss next steps.         Synthesizes feedback from customers and partners to learn about the product usage and identify and resolve feature and knowledge gaps and key performance indicators (KPIs) in the current product. Leads team in sharing insights and best practices with customers and partners on these service improvements. Recommends changes to content improvement or troubleshooting guides and develops metrics to evaluate the changes             Customer Solution Lifecycle Management             Utilizes cross systems to conduct health checks to ensure customer environment (e.g., product, service, feature) is optimized and configured for deployment. Provides guidance to customers on understanding and implementing new versions. Serves as a connecting point between the engineering team and customers throughout the solution s lifecycle. Utilizes any resources to respond and resolve the immediate issues throughout the solution lifecycle. Proactively provides guidance to customers on designing configurations and deploying solutions on Microsoft platforms.         Handles complex escalations on customer issues from the support or field teams. Conducts impact analysis to determine the priority of the escalations. Conducts deep root cause analysis of the issues and converts them into improvement opportunities. Serves as an escalation resource in areas of subject matter expertise. Represents the team on highly complex issues and answers a large variety of technical questions and concerns.             Relationship/Experience Management             Acts as a voice of customers and leverages customers feedback to provide input on business plans developed by the relevant product and business groups.          Identifies customer usage patterns and driving resolutions on reoccurring customer issues with engineering and product and business groups.          Leads their team in engaging with feature and product groups on redesign/customer requested changes for Microsoft products. Closes the loop of feedback with the customers on product features.         Partners with other teams (e.g., program managers, software engineers, product, customer service support [CSS] teams) to prioritize and drive resolutions of complex, high-impact customer issues and integration of customer features into the products.          Leads discussions with stakeholders on customer progression and provides expertise on resolutions plans for common types of customer issues.          Proactively communicates the translation of signals into actionable insights/trends to product teams to improve service reliability. Leads coordination with stakeholders (e.g., engineering/product teams) to develop mechanisms that improve customer health engagement and reduce the turnover time.     ",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Software Product,"Cloud computing, Automation, Operational excellence, Networking, Management, Customer service, Troubleshooting, microsoft, Analytics, SQL",-,9am-6pm,"Full Time, Permanent",Microsoft,Organization,Microsoft,https://img.naukimg.com/logo_images/groups/v1/614576.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Alation Platform Engineer,"   As an Alation Platform Engineer, you will be an expert professional who works with stakeholders and clients to enable them on the Alation Platform.      Role Purpose      Be an integral part of the Alation Platform team, which works with our Clients and provides consultancy to enable them for information management      Work with Product Owners , understand the business use case and Contribute to implementation Data lineage solutions      Expertise in Alation Platform troubleshooting , Alation cloud migration experience      Essential Knowledge Experience:      Required 8-10 years experience of data /information governance and management      Hands on experience on Alation Platform technology Must be hands on Alation On premise and cloud /SaaS solution installation      Documentation of documentation of APIs , operational manuals (Markdown, etc.).      Must be able to work on end to end activities from design, development and deployment    ",90424500956,09-04-2024,08-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Software Product,"Training, Cloud, Design development, Deployment, Information management, Management, Troubleshooting, Operations, Recruitment",-,9am-6pm,"Full Time, Permanent",Cosmic It,Organization,Cosmic It,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Kafka Data Professional,"   Experience with Azure/MS Data Platforms: Postgres Single and Flex, Azure SQL, MS SQL server, Kubernetes            Terraform (Preferred not Compulsorry), Bicep or other IaC.            Helm (helm deployments as well writing charts).            Azure DevOps pipelines.            Engineer, Data Engineer, or similar roles with a strong focus on Kafka and data engineering within the Azure environment.            Experience with Data pipelines for deploying database code - Postgres, Azure SQL, and SQL Server    ",81223500667,08-12-2023,07-03-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"MS SQL, devops, Flex, Database, Deployment, SQL",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,"Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Implementation Engineer," ?     Owns and delivers installation, configuration of AI products.     Participates in solution architecture discussions to estimate hardware sizing.     Analyses defects and conducts effective triaging.     Works closely with engineering, product, solution architects in resolving     defects.     ?     Proactively participates in product release cycle and sign off on client     releases.     Supports current ETL designs and code to resolves defects on existing     solutions.     ?     Ensures data quality.     Translates data access, transformation, and movement requirements into     functional requirements and mapping designs.     Develops logical and physical data flow models for ETL applications.     Documentation and Training     Communicates effectively with the client and manage expectations.           Must-Have     3+ years of experience with big data application stack including HDFS, YARN,     Spark, Hive and HBase.     2+ years of experience in AWS Cloud Setup.     2+ years of experience in Enterprise SW installation especially packages.     2- or 3-year Experience in ETL in Big Data Hadoop environment (Hive/HBase     knowledge).     1 or 2 years of experience in shell scripting or python.     Cloudera - CDH experience, CDP experience. ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Solution architecture, hive, cloudera, Shell scripting, Technical Lead, Data quality, big data, YARN, Python, HBase",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Implementation Engineer," ?     Owns and delivers installation, configuration of AI products.     Participates in solution architecture discussions to estimate hardware sizing.     Analyses defects and conducts effective triaging.     Works closely with engineering, product, solution architects in resolving     defects.     ?     Proactively participates in product release cycle and sign off on client     releases.     Supports current ETL designs and code to resolves defects on existing     solutions.     Ensures data quality.     ?     Translates data access, transformation, and movement requirements into     functional requirements and mapping designs.     Develops logical and physical data flow models for ETL applications.     Documentation and Training     Communicates effectively with the client and manage expectations.     ?     Must-Have     ?     3+ years of experience with big data application stack including HDFS, YARN,     Spark, Hive and HBase.     2+ years of experience in AWS Cloud Setup.     2+ years of experience in Enterprise SW installation especially packages.     2- or 3-year Experience in ETL in Big Data Hadoop environment (Hive/HBaseknowledge).     1 or 2 years of experience in shell scripting or python.     Cloudera - CDH experience, CDP experience.             ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Solution architecture, hive, Shell scripting, Technical Lead, Data quality, Business solutions, big data, YARN, Python, HBase",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure cloud Engineer,"       Good understanding of Azure AD, AD connect, Conditional access policies, Azure SSO services       Good understanding of Azure IAAS, PAAS services     Good understanding of M365, Mailbox, DL, DDL, mail flow     Working knowledge of Backups tools, Working knowledge of different flavors of Windows server OS with understanding of in place upgrade, patching, hardening etc     Working knowledge of vulnerabilities remediation     Working knowledge of Dell EMC storage and Dell servers devices     Aware of Incident, Problem and Change Management processes              ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"remediation, Change management, PAAS, Windows Server OS, Cloud, Budgeting, EMC storage",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Backend Engineer,"   ?                       You'll primarily work on 1P backend APIs and services, both public-facing and internal.                             Our Backend is based on microservices architecture and built using Python, NOSQL DB services and many other open source tools as well.                             Over time, we expect you to grow, train and scale your own backend team.                             You'll create experiences that shape an iconic product. We believe in hiring smart people that pride themselves in good values, their work ethics, and holding great responsibility.               You'd fit in if:                          Youve built SaaS or consumer products before.   You understand your users, like bringing great products to life and think of making APIs as a craft.                                 You've built, scaled, and maintained large-scale systems before on AWS.   This is a specific ask, and this is a big plus, but not absolutely required.                                 You are a fast learner.   Our tech stack will grow with time, you need to be able to learn on the go and build solutions quickly.                                 You can work in teams  . You can hold your ground and don't need constant oversight, but can collaborate and work with more people whenever necessary.                                 You enjoy building from scratch.   Being an early hire, creating things from the ground up should excite you.                                 You can think about business constraints.   We want our engineers to be mindful of their choices, understand users, and think about the impact of their work on the bottom line.                   ",2.51E+11,25-09-2023,24-12-2023,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Fitness & Wellness,"Training, Backend, NoSQL, Architecture, SAAS, Open source, AWS, Python, microservices",-,9am-6pm,"Full Time, Permanent",1pharmacy Network,Organization,1pharmacy Network,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Devops/Hadoop Administrator,"   Must have-       Shell Scripting - 1 yrs, Hadoop (Apache, CDH), yarn, hive deployment - 3 years      Security understanding of Hadoop      AWS/GCP Cloud computing - 1 yr      Application server deployment - 1 yr              Good to have-       Spark, yarn, hive, hbase deployment      LDAP setup      Docker understanding    ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"hive, Cloud computing, LDAP, GCP, Shell scripting, Hadoop, Deployment, Apache, YARN, HBase",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Sr. SW Engineer  Cloud (Backend),                 Should have at least 75% match with the following skills:        Core Java Node.js Python      AWS API & Micro Services      SQLNoSQL database      Data Streaming - Kafka ActiveMQ Kinesis        AWS is a must.                           ,2.01E+11,20-09-2023,19-12-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"activemq, Backend, Core Java, Cloud, Database, Javascript, AWS, Python",-,9am-6pm,"Full Time, Permanent",Expedite Technology Solutions,Organization,Expedite Technology Solutions,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Systems Development Senior Analyst/Engineer,"   The        BI Developer       reports to the Business Intelligence Manager and is responsible for    combining raw information from disparate IT source systems into data models, reports and dashboards to deliver business insights. The successful candidate will be able to complete the full lifecycle of development of data from the ETL process through to final deliverable of dashboards into the organization.     To succeed in this BI Developer position, you should have strong analytical skills and the ability to develop and maintain data and security models using modern techniques. If you are detail-oriented, with excellent organizational skills and experience in this field, we d like to hear from you.          Responsibilities for a BI Developer position include:                 Essential Duties and Responsibilities               Participate in business requirement gathering and solution documentation     Build required infrastructure for optimal pipeline management, including extraction, transformation and loading of data from various data sources using Azure Data Factory, Databricks and SQL technologies     Assemble and analyze large, complex sets of data that meet non-functional and functional business requirements     Implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes     Work with users to assist them with data-related technical issues     Implement and test data and security models     Prepare data for prescriptive and predictive modeling     Develop interactive visual reports, dashboards, charts, and measures with KPI scorecards using Microsoft Power BI desktop     Analyze, design, deploy, troubleshoot, and support Power BI solutions       Participate in user acceptance testing       Explore and implement ways to enhance data quality and reliability     Collaborate with data scientists and architects as needed                     Education               Bachelor s degree (B.S./B.A.) in computer science,      information systems, informatics, statistics or another quantitative field     or equivalent from a college or university with IT focused specialization.         A Master s Degree or    Data engineering certification (e.g, IBM Certified Data Engineer) is a plus                         Skills/Experience               5+ years experience as a BI Developer or    related experience in a global company with significant experience in hands-on technology delivery roles.              Strong data analytics background with experience in developing use cases, deep understanding of managing data and generating insights thru visualization       Background in custom build experience using Power BI Report Builder, Power BI Desktop, Power BI Service, Tabular Editor, ALM Toolkit and DAX Studio designing Power BI data models; including writing complex DAX, SQL queries and implementing role level security     Ability to understand data modeling, data schemas (normalized, flat, star, snowflake, etc.), query optimization, query profiling and query performance monitoring tools and techniques     Knowledge of programming languages (e.g. Java, AngularJS, Python)       Experience with workflow management and pipeline tools - Azure Data Factory and DevOps; storage technologies - Azure Data Warehouse and Data Lake; stream-processing systems - Event Hub and Stream Analytics; transformation tools - Databricks; visualization tools - PowerBI; and metadata management systems.         Experience with big data tools like Spark is a plus         Familiarity with Machine Learning and Deep Learning concepts are a plus       Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata     Experience working with unstructured datasets     Hands-on experience with optimizing performance of SQL queries and applications     Great numerical and analytical skills       Ability to collaborate with technical resources to influence algorithms and other technology for improved customer experience     ",1.00E+11,10-04-2024,09-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Petrochemical / Plastics / Rubber,"Computer science, Data modeling, Process control, Data structures, Predictive modeling, Data quality, microsoft, Business intelligence, User acceptance testing, Analytics",-,9am-6pm,"Full Time, Permanent",Mold Masters,Organization,Mold Masters,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Cloud Engineer,"               Contribute to Public Cloud Strategy                             Transform requirements for new capabilities or changes from architecture, operations or customers to include them in the design of Public Cloud architecture                             Ensure operational supportability of design, development and engineering by working with operational support team in design and development such that the solution is operationally supportable technically and is properly documented operationally                             Capture Lessons Learned through each iteration of development process and incorporate ongoing changes to processes and/or technology to improve effectiveness                             Engineer and drive implementation of automation services into the Public Cloud                             Create the detailed infrastructure design to operate the application software/services on Azure public cloud platform                             Discuss with application manager/architects to understand requirements and document the detail requirements                             Contribute in detail solution design for scalable, highly available, and fault tolerant hosting platform on Azure                             Work on selecting the appropriate Azure service based on compute, data, networking, high-availability and security requirements                             Conduct solution design review meetings with application team and other stakeholders                             Guide implementation and control the flow of data to and from Azure                             Designing and implementing organization wide security and governance strategies.                     Designing and Developing High Availability and Business Continuity Plan based upon RPO and RTO requirements.                           Co-ordinate with IT systems/application support, networks, and liaise for server builds and upgrades, domain configuration, and performance and tuning                         Conduct Azure Cost Optimizations initiatives                          Experience of working on configuration management platforms.                         Strong scripting skills e g PowerShell, Python, etc.                                          BASIC QUALIFICATIONS     :                           Bachelor's degree in Computers or equivalent with overall 5 - 8 years working in IT (preferably in infrastructure management/support role)                             1 to 3 years of relevant experience in hosting solution design of enterprise scale business application on Azure platform                             Azure Administrator Associate Certification highly desired                             Excellent English communication skills (both written and verbal)                             A very strong customer focus                             Ability to juggle many tasks and projects in a fast-moving environment                             Be a self-starter who is excited about learning new and emerging technologies                                                           REQUIRED SKILLS     :                           Strong Scripting skills (PowerShell or Azure CLI)                             Hands on experience with ARM template Usage                             Hands on experience with Terraform Usage                             Experienced on Azure Automation, PowerShell Runbook                             Good Knowledge on Azure Security and Platform governance                             Experience with DevOps Orchestration / Configuration / Continuous Integration Management technologies                             Knowledge on Azure Networking is desirable                             Technical support expertise is also desirable                             Experience provisioning, operating, and maintaining systems running on Azure                             Ability to identify and gather requirements to define a solution to be built and operated on Azure                             Capabilities to provide Azure operations and deployment guidance and best practices throughout the lifecycle of a project                             Knowledge of application deployment and data migration on Azure for different regions                             Experience with using a broad range of Azure services, mainly from the list below:                             Web Apps, Web jobs, Storage, Azure Key Vault, App Insights, Azure SQL DB, Cosmos DB, Functions, Azure Bot Service, Express Route                             Cognitive Services - LUIS, QnA Maker, Search, Speech Services, Translator services                             Azure VM, Azure VNet, Azure DevOps and CI/CD Pipeline                             Azure Active Directory, Azure AD B2C                             Azure Analytics Services - Azure Analysis Services, SQL Data Warehouse, Data Factory, Databricks                             Develop and maintain an Azure based cloud solution, with an emphasis on best practice cloud security.                              Infrastructure and Application monitoring across production and non-production platforms                             Knowledge on hybrid public cloud design concepts                             Very good understanding of Cloud IaaS, PaaS and SaaS services                             Exposure to Amazon Web Services will be considered as a plus                             MS - Azure or Amazon AWS Certification or other Cloud related certifications highly desired                             Good understanding of High Availability and Disaster Recovery concepts for infrastructure                              Familiarity with monitoring tools                              Problem Solving: Ability to analyze and resolve complex infrastructure resource and application deployment issues.                            Experience with Microsoft Windows Servers administration             ",71223500852,07-12-2023,06-03-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Solution design, Networking, Design review, Infrastructure management, Cloud, Active directory, Cosmos, Technical support, SQL, Python",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Python (Programming Language) Good to have skills : Amazon Web Services (AWS) Minimum  2  year(s) of experience is required Educational Qualification : 15 years of full time education Summary :As a Data Platform Engineer, you will be responsible for developing and maintaining data platforms using Python programming language. Your typical day will involve working with Amazon Web Services (AWS) and ensuring data quality and integrity.  Roles & Responsibilities: Design, develop, and maintain data platforms using Python programming language. Ensure data quality and integrity by implementing data munging techniques such as data cleaning, transformation, and normalization. Collaborate with cross-functional teams to integrate data platforms with other systems and applications. Implement and maintain data security and privacy measures to protect sensitive data. Stay updated with the latest advancements in data platform development and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Proficiency in Python programming language. Good To Have Skills:Experience with Amazon Web Services (AWS). Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity. Experience in implementing and maintaining data platforms. Strong understanding of data security and privacy measures. Ability to collaborate with cross-functional teams and integrate data platforms with other systems and applications. Additional Information: The candidate should have a minimum of 2 years of experience in Python programming language. The JOB FAMILY and PROJECT ROLE information are not for candidate's experience. This position is based at our Bengaluru office. Qualification 15 years of full time education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, data security, data modeling, aws, data munging, data analysis, data analytics, natural language processing, data mining, power bi, machine learning, sql, data quality, r, tableau, java, data science, clustering, data visualization, logistic regression",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 full years of education and necessarily degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Proficiency in programming languages such as Python or Java. Experience with data integration and ETL processes. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 full years of education and necessarily degree in engineering",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, information technology, microsoft azure, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, hadoop, etl, aws, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Netapp Network Attached Storage (NAS) Administration Good to have skills : Storage Area Networks (SAN) Architecture and Design Minimum  2  year(s) of experience is required Educational Qualification : Bachelors Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Assist with the implementation and maintenance of Netapp Network Attached Storage (NAS) Administration. Provide support for Storage Area Networks (SAN) Architecture and Design. Troubleshoot and resolve issues related to the data platform components. Professional & Technical Skills: Must To Have Skills:Experience in Netapp Network Attached Storage (NAS) Administration. Good To Have Skills:Experience in Storage Area Networks (SAN) Architecture and Design. Experience in troubleshooting and resolving issues related to the data platform components. Experience in collaborating with Integration Architects and Data Architects. Strong understanding of data platform components and their integration with systems and data models. Additional Information: The candidate should have a minimum of 2 years of experience in Netapp Network Attached Storage (NAS) Administration. The ideal candidate will possess a strong educational background in Software Engineering, Computer Science, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification Bachelors",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"san, troubleshooting, software engineering, network attached storage, netapp, emc clariion, nas, python, networking, data architecture, brocade, storage area network, iscsi, netapp storage, data modeling, linux, nfs, emc storage, fc, cifs, storage administration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Scientist,"As a Data Scientist at IBM, you will help transform our clients' data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it's investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live. Your Role and Responsibilities Proof of Concept (POC) Development: Develop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations. Customer Engagement and Support: Act as a technical point of contact for customers, addressing their questions, concerns, and feedback. Provide technical support during the solution deployment phase and offer guidance on AI-related best practices and use cases. Documentation and Knowledge Sharing: Document solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members. Industry Trends and Innovation: Stay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation. Required Technical and Professional Expertise Technical Skills:  At least 2 years of experience in developing AI/ML solutions in Python.  Should have strong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Should be able to use libraries such as SciKit Learn, Pandas, Matplotlib, etc. Should have a good knowledge of relational databases and SQL. Should be able to use NoSQL Databases in managing data for analysis. Should be able to follow the Agile methodology of development along with the knowledge of tools like Github. Soft Skills:  Should have excellent interpersonal and communication skills.  Should be able to engage with stakeholders and team for analysis and implementation.  Should be able to learn continuously and stay updated with advancements in the field of AI. Preferred Technical and Professional Expertise Experience in setting up web services using Flask, Django, FastAPI, etc. Experience in designing and delivering AI solutions, with a focus on foundation models, large language models. Exposure to open source AI libraries. Experience in full AI project lifecycle, from prototyping to deployment in production environments. Familiarity with cloud platforms (e.g. IBM Cloud, AWS, Azure, GCP) and related services.",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, artificial intelligence, pandas, tensorflow, matplotlib, github, scikit-learn, ibm cloud, microsoft azure, huggingface, relational databases, nosql, sql, open source, django, gcp, product development, pytorch, keras, flask, aws, agile methodology, ml",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Experience with troubleshooting and resolving data platform issues. Solid grasp of programming languages such as Python or Java. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",90524911757,09-05-2024,07-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, microsoft azure, data engineering, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, troubleshooting, hadoop, aws, big data",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve technical problems. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Must To Have Skills:Experience with Azure Data Factory and Azure Databricks. Good To Have Skills:Experience with other Azure data services such as Azure Synapse Analytics and Azure Stream Analytics. Strong understanding of data modeling and database design principles. Experience with data integration and ETL processes. Familiarity with data security and privacy regulations such as GDPR and CCPA. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. 15 years of Education and necessary degree in engineering This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",90524911715,09-05-2024,07-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, data services, microsoft azure, azure data factory, data modeling, python, azure data lake, azure synapse, power bi, machine learning, gdpr, database design, sql azure, spark, azure stream analytics, design principles, etl, data integration, ccpa",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Proficiency in Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Experience with troubleshooting and resolving data platform issues. Solid grasp of programming languages such as Python or Java. Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",90524911599,09-05-2024,07-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, microsoft azure, data engineering, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, troubleshooting, hadoop, aws, big data",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Ceph Data Migration Engineer,"The IBM Storage Software team is looking for a Ceph L3 Engineer to join us in Bangalore, India. In this role, you'll be required to have a deep background in Linux storage system administration and development. In this role, you will deliver an outstanding sustaining experience to our Ceph and its layered product support teams and customers. You will be an escalation point for critical customer issues and a mentor across the globe for L2 Support Engineers and partners. As a Ceph L3 Engineer, you will need to have experience with large and complex systems and be able to think creatively, learn new things, and adapt to a constantly changing environment. Your Role and Responsibilities You will be part of the Storage Development Business of the Infrastructure organization with the following key responsibilities: Responsibilities: Develop and maintain data migration strategy, processes, tools, and techniques to ensure that the data is migrated accurately, securely, and efficiently.  Design, implement, and maintain migration scripts. Troubleshoot and resolve migration automation or manual failures promptly. Develop and execute comprehensive migration plans to migrate data to Ceph from different storage systems. Analyze automation, and manual procedures, identify improvement areas, and collaborate with teams for enhancements. Required Technical and Professional Expertise 5 10 years of experience in storage and automation Proficiency in data migration development tools. Strong scripting (Python, Bash, etc.) and programming skills (C/C++). Experience with storage protocols like iSCSI, and NVMe/TCP. Experience with VMware virtualization, and storage.  In-depth knowledge of Ceph storage architecture, components, and deployment. Hands-on experience with configuring and tuning Ceph clusters. Understanding of RADOS, CephFS, and RBD (Rados Block Device). Ability to design and implement end-to-end automation solutions. Proven ability to troubleshoot and debug complex system issues. Preferred Technical and Professional Expertise Knowledge of Opensource development and working experience in Opensource projects. Certifications related to Ceph storage and performance testing are a plus. Familiarity with cloud platforms (AWS, Azure, GCP) and their storage services. Experience with container orchestration tools such as Kubernetes. Knowledge of monitoring tools (Prometheus, Grafana) and logging frameworks. Ability to work effectively in a collaborative, cross-functional team environment. Knowledge of AI/ML, exposure to Gen AI",90524909058,09-05-2024,07-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"c++, ceph storage, microsoft azure, system administration, linux, kubernetes, python, performance testing, vmware, networking, data migration, artificial intelligence, docker, iscsi, nvme, rbd, gcp, grafana, troubleshooting, bash, mysql, openstack, prometheus, aws",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,"Pune, Bengaluru","Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines and ETL processes using Azure Data Factory and other relevant tools. Design and implement data security and compliance measures to ensure data privacy and integrity. Optimize data storage and retrieval processes for performance and scalability. Professional & Technical Skills: Must To Have Skills:Proficiency in Microsoft Azure Data Services. Good To Have Skills:Experience with other cloud platforms such as AWS or Google Cloud Platform. Strong understanding of data modeling and database design principles. Experience with data integration and ETL tools such as Azure Data Factory, Informatica, or Talend. Familiarity with data security and compliance regulations such as GDPR and CCPA. Experience with data storage and retrieval technologies such as SQL Server, Azure SQL Database, or Azure Cosmos DB. Additional Information: The candidate should have a minimum of 7.5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",90524904789,09-05-2024,07-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, information technology, microsoft azure, data modeling, design principles, talend, cloud platforms, azure data factory, sql server, gdpr, sql, database design, sql azure, gcp, data ingestion, azure cosmosdb, etl, aws, informatica, data integration, ccpa",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Proficiency in Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Experience with troubleshooting and resolving data platform issues. Solid grasp of programming languages such as Python or Java. Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",90524904757,09-05-2024,07-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, microsoft azure, data engineering, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, troubleshooting, hadoop, aws, big data",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  2  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve technical problems. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Must To Have Skills:Experience with Azure Data Factory and Azure Databricks. Good To Have Skills:Experience with other Azure data services such as Azure Synapse Analytics and Azure Stream Analytics. Strong understanding of data modeling and database design principles. Experience with data integration and ETL processes. Familiarity with data security and privacy regulations such as GDPR and CCPA. Additional Information: The candidate should have a minimum of 2 years of experience in Microsoft Azure Data Services. 15 years of Education and necessary degree in engineering. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",90524904683,09-05-2024,07-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, data services, microsoft azure, azure data factory, data modeling, python, azure data lake, azure synapse, power bi, machine learning, gdpr, database design, sql azure, spark, azure stream analytics, design principles, etl, data integration, ccpa",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Experience with troubleshooting and resolving data platform issues. Solid grasp of programming languages such as Python or Java. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",90524904667,09-05-2024,07-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, microsoft azure, data engineering, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, troubleshooting, hadoop, aws, big data",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Must To Have Skills:Strong understanding of data integration and data modeling concepts. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Good To Have Skills:Knowledge of data security and privacy regulations. Good To Have Skills:Experience with troubleshooting and resolving data platform issues. Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",80524912419,08-05-2024,06-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, microsoft azure, data engineering, data modeling, data integration, azure databricks, hive, python, data warehousing, azure data factory, machine learning, sql server, sql, amazon ec2, java, spark, kafka, troubleshooting, hadoop, sqoop, aws, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve technical problems. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Must To Have Skills:Experience with Azure Data Factory and Azure Databricks. Good To Have Skills:Experience with other Azure data services such as Azure Synapse Analytics and Azure Stream Analytics. Strong understanding of data modeling and database design principles. Experience with data integration and ETL processes. Familiarity with data security and privacy regulations such as GDPR and CCPA. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. 15 years of Education and necessary degree in engineering This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",80524912232,08-05-2024,06-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, data services, microsoft azure, azure data factory, data modeling, python, azure data lake, azure synapse, power bi, machine learning, gdpr, database design, sql azure, spark, azure stream analytics, design principles, etl, data integration, ccpa",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Microsoft Azure Data Services.  Roles & Responsibilities: Design and implement data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines and ETL processes. Ensure data quality and integrity by implementing data governance and security best practices. Optimize data platform performance and scalability by implementing efficient data storage and retrieval strategies. Professional & Technical Skills: Must To Have Skills:Proficiency in Microsoft Azure Data Services. Good To Have Skills:Experience with other cloud platforms such as AWS or Google Cloud Platform. Strong understanding of data governance and security best practices. Experience with data pipelines and ETL processes. Solid grasp of data storage and retrieval strategies. Experience with data platform performance optimization and scalability. Additional Information: The candidate should have a minimum of 7.5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",80524911733,08-05-2024,06-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data ingestion, data services, microsoft azure, data engineering, data governance, hive, data warehousing, docker, sql, iot, java, git, data modeling, gcp, spark, devops, linux, jenkins, mysql, hadoop, etl, big data, python, cloud platforms, data quality, agile, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Analysis & Interpretation Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : As per Company Standards Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and analyzing and interpreting complex data sets for actionable insights.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Analyze and interpret complex data sets using statistical methodologies and data munging techniques for actionable insights. Develop and deploy data pipelines and ETL processes to support data integration and transformation. Design and implement data security and privacy measures to ensure data integrity and compliance. Professional & Technical Skills: Must To Have Skills:Strong experience in Data Analysis & Interpretation. Good To Have Skills:Experience with data pipelines and ETL processes, data security and privacy measures. Solid understanding of statistical methodologies and data munging techniques. Experience with data integration and transformation. Experience with data security and privacy measures. Strong programming skills in languages such as Python, Java, or Scala. Additional Information: The candidate should have a minimum of 7.5 years of experience in Data Analysis & Interpretation. The ideal candidate will possess a strong educational background in computer science, data science, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification As per Company Standards",80524911701,08-05-2024,06-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data analysis, statistical techniques, scala, java, data munging, python, data analytics, natural language processing, data mining, machine learning, sql, r, tableau, data modeling, data science, data visualization, etl, data integration, statistics",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines and ETL processes using Azure Data Factory and other relevant tools. Design and implement data security and compliance measures to ensure data privacy and integrity. Optimize data storage and retrieval processes for performance and scalability. Professional & Technical Skills: Must To Have Skills:Proficiency in Microsoft Azure Data Services. Good To Have Skills:Experience with other cloud platforms such as AWS or Google Cloud Platform. Strong understanding of data modeling and database design principles. Experience with data integration and ETL tools such as Azure Data Factory, Informatica, or Talend. Familiarity with data security and compliance regulations such as GDPR and CCPA. Experience with data storage and retrieval technologies such as SQL Server, Azure SQL Database, or Azure Cosmos DB. Additional Information: The candidate should have a minimum of 7.5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",80524911550,08-05-2024,06-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, information technology, microsoft azure, data modeling, design principles, talend, cloud platforms, azure data factory, sql server, gdpr, sql, database design, sql azure, gcp, data ingestion, azure cosmosdb, aws, etl, informatica, data integration, ccpa",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Experience with troubleshooting and resolving data platform issues. Solid grasp of programming languages such as Python or Java. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",80524911466,08-05-2024,06-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, microsoft azure, data engineering, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, troubleshooting, hadoop, aws, big data",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : SAS Data Integration Good to have skills : NA Minimum  2  year(s) of experience is required Educational Qualification : Any Degree Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using SAS Data Integration.  Roles & Responsibilities: Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Assist with the blueprint and design of the data platform components. Develop and maintain data integration processes using SAS Data Integration. Troubleshoot and resolve data integration issues. Ensure data quality and integrity throughout the data integration process.  Professional & Technical Skills: Must To Have Skills:Proficiency in SAS Data Integration. Good To Have Skills:Experience with other data integration tools. Strong understanding of data integration processes and techniques. Experience with troubleshooting and resolving data integration issues. Solid grasp of data quality and integrity best practices.  Additional Information: The candidate should have a minimum of 2 years of experience in SAS Data Integration. The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification Any Degree",80524911373,08-05-2024,06-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sas, data quality, troubleshooting, software engineering, data integration, python, data analysis, data management, data analytics, oracle, data warehousing, machine learning, business intelligence, sql server, sql, tableau, data modeling, etl, informatica",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years of Education and necessary degree in engineering Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Azure Data Factory, Azure Databricks, and other Azure data services. Implement data security and privacy measures to ensure compliance with industry standards and regulations. Troubleshoot and resolve data platform issues, working closely with cross-functional teams to identify and resolve root causes.  Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Experience with Azure Data Factory, Azure Databricks, and other Azure data services. Strong understanding of data modeling and database design principles. Experience with data security and privacy measures, including compliance with industry standards and regulations. Experience with troubleshooting and resolving data platform issues. Solid grasp of programming languages such as Python or Java.  Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of Education and necessary degree in engineering",80524911316,08-05-2024,06-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, microsoft azure, data engineering, data modeling, design principles, azure databricks, hive, python, azure data factory, machine learning, sql server, sql, database design, java, spark, design patterns, troubleshooting, hadoop, aws, big data",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python / AI Developer,"5+ years of professional experience as a Python Developer with a significant focus on AI integration. Expertise in machine learning concepts and frameworks (TensorFlow, PyTorch, etc.). Proven proficiency in Python and experience with relevant libraries and frameworks (Django, Flask, etc.). Strong knowledge of RESTful APIs and experience integrating AI models into complex software applications. Experience with cloud platforms (AWS, Azure, Google Cloud) and containerization (Docker, Kubernetes). Excellent problem-solving and analytical skills. Strong leadership, communication, and teamwork skills.",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Python, RESTful API, Azure, PyTorch, Django, AI Development, AWS, Google Cloud, TensorFlow, Flask",-,9am-6pm,"Full Time, Permanent",Leuwint Technologies,Organization,Leuwint Technologies,https://img.naukimg.com/logo_images/groups/v1/3704938.gif,Palakkad,Palakkad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Data/SQL, Python, Pyspark Developer","   Clifyx Technology is looking for Data/SQL, Python, Pyspark Developer to join our dynamic team and embark on a rewarding career journey     Data Processing and Analysis:Develop, implement, and maintain data processing pipelines using SQL, Python, and PySpark     Perform data extraction, transformation, and loading (ETL) tasks from heterogeneous sources     Analyze and interpret complex datasets to extract insights and support decision-making processes     Database Management:Design, optimize, and maintain relational databases using SQL (e g, PostgreSQL, MySQL, SQL Server)     Write efficient SQL queries for data retrieval, manipulation, and aggregation     Ensure data integrity, security, and availability in database systems     Python Development:Develop data-driven applications, scripts, and tools using Python programming language     Utilize Python libraries and frameworks for data manipulation, analysis, and visualization (e g, Pandas, NumPy, Matplotlib, Seaborn)     PySpark Development:Design and implement data processing workflows using PySpark, a Python API for Apache Spark     Leverage Spark's distributed computing capabilities for large-scale data processing and analytics     Optimize PySpark jobs for performance and scalability     Big Data Technologies:Work with big data technologies and platforms such as Hadoop, Spark, and Kafka     Develop expertise in handling large volumes of data and distributed computing paradigms     Testing and Debugging:Write unit tests and perform code reviews to ensure code quality, reliability, and maintainability     Debug and troubleshoot issues in data pipelines, applications, and analytics solutions     Documentation and Collaboration:Document technical specifications, data schemas, and codebase for reference and knowledge sharing     Collaborate with cross-functional teams including data engineers, data scientists, and business analysts   ",1.40E+11,14-02-2024,14-05-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"schema, python, sql development, sql queries, pyspark, python development, relational databases, sql server, distributed computing, scalability, sql, scripting, apache, postgresql, spark, kafka, debugging, mysql, code review, hadoop, api, etl, programming",-,9am-6pm,"Full Time, Permanent",Clifyx Technology,Organization,Clifyx Technology,-,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python developer,"Requirements: ??Experience ??4 to 6 years ??Job location ??Ahmedabad Responsibilities: - Develop back-end components to improve responsiveness and overall performance - Write effective, scalable code, Integrate user-facing elements into applications - Design and develop bespoke database components. - Develop software interfaces and specifications that clearly define interoperability with software delivered by other delivery teams. - Build and keep an up-to-date knowledge base of software systems including technical and end-user documentation. - Design and create unit test cases and make your code work seamlessly in a continuous integration environment - Develop iteratively and test early on the release cycle; report on task progress regularly; promptly raise risks and arrange for mitigation Requirements Skill set:   Python development   Django   Pandas   MongoDB   RabbitMQ   Kafka Benefits   Health Benefits: Insurance, Gym membership   5 days working (Monday-Friday)",1.10E+11,11-04-2024,10-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"continuous integration, css, numpy, jquery, artificial intelligence, docker, sql, git, postgresql, data science, linux, mysql, html, data structures, mongodb, rest, python, github, natural language processing, python development, machine learning, rabbitmq, javascript, pandas, django, kafka, flask, aws",-,9am-6pm,"Full Time, Permanent",Nextgen Clearing Ltd,Organization,Nextgen Clearing Ltd,-,Ahmedabad,Ahmedabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Junior Python Developer,"   Python Programming: Junior Python Developers are proficient in Python and use it to write code, develop applications, and work on various software projects     They may be responsible for implementing specific features, fixing bugs, and writing scripts using Python     Development Environment: They are familiar with Python development tools and environments, such as Integrated Development Environments (IDEs) like PyCharm or Visual Studio Code     They use these tools to write, test, and debug code     Basic Software Development Concepts: Junior Python Developers have a basic understanding of software development concepts, including data structures, algorithms, and object-oriented programming (OOP) principles     Collaboration: They work as part of a development team and collaborate with other developers, as well as software architects, designers, and quality assurance engineers     They often participate in code reviews and contribute to discussions about design and implementation     Code Testing: Junior Python Developers write unit tests to verify the correctness of their code and ensure that it functions as expected     They may also assist in testing efforts and help identify and fix defects     Problem Solving: These developers work on problem-solving and troubleshooting, both independently and with guidance     They may need to investigate and resolve issues that arise during development     Version Control: They use version control systems like Git to manage and track changes to code     This is important for collaboration and code management     Learning and Growth: Junior Python Developers are continuously learning and improving their Python skills and knowledge of best practices     They seek guidance from more experienced team members and work on expanding their skill set     Documentation: They document their code and contribute to project documentation, ensuring that the codebase is well-documented for other team members     Project Support: Junior Python Developers assist in various project-related tasks, which may include requirements analysis, user story implementation, and helping to meet project milestones and deadlines     Adhering to Coding Standards: They follow coding standards and best practices within the organization to ensure code quality and maintainability     Communication: Effective communication within the development team is important for discussing requirements, reporting progress, and seeking assistance when needed       ",91123500194,09-11-2023,07-02-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"HR Executive, SAS, Finance, Manager Technology, Infrastructure, Animation, Business Development Executive, Python, Junior Data Analyst, Testing",-,9am-6pm,"Full Time, Permanent",Mabzone Technologies,Organization,Mabzone Technologies,-,Mohali,Mohali,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Sr Golang Developer,"Below is the overview of the required skills/JD: Minimum of 4 years of experience in development of microservices based on Golang (strong Java developers with some experience on golang can also be considered) Hands-on participation in design and development of the highly scalable, low latency data platform using edge computing/MEC and cloud. Design and development of a multi-tenant and big data ingestion and processing platform using MQTT, transportation data standards, security, and low latency data processing. Develop cloud services using AWS technologies such as AWS IoT, AWS EKS, Kinesis, Kafka, Lambda, Dynamodb, API Gateway, Cognito, EMR/Spark, and Aurora, SageMaker. Partner with other teams and development leaders to ensure overall architecture and design is aligned with edge and cloud development best practices. Work closely with globally spread development teams. Develop platform core services that makes vertical application development faster and consistent with the overall design of the entire ecosystem. Work with product managers and stakeholders to set strategic direction for the platform. Develop API and support 3rd party development ecosystem for partners to develop applications on top of the platform.",80524906713,08-05-2024,06-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Golang, cloud, Java, AWS EKS, Kinesis, Kafka, API, AWS, Lambda, Microservices, Amazon DynamoDB",-,9am-6pm,"Full Time, Permanent",Vervenest Technologies,Organization,Vervenest Technologies,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Python Developer," o Python/Pyspark      o Strong in SQL      o Data warehousing Data LAke      o Understanding of Data model      o Demonstrated knowledge in the Big Data ecosystem - Hive, Hadoop etc            Preferred skills:      o GCP services understanding      o Cloud Warehouses like BigQuery (preferred), Amazon Redshift, Snowflake etc      o Distributed file systems like GCS, S3, HDFS etc      o PySpark      o Airflow / Cloud Composer      o CI/CD and DevOps              Job responsibilites            Mandatory skills:      o Python/Pyspark      o Strong in SQL      o Data warehousing Data LAke      o Understanding of Data model      o Demonstrated knowledge in the Big Data ecosystem - Hive, Hadoop etc            Preferred skills:      o GCP services understanding      o Cloud Warehouses like BigQuery (preferred), Amazon Redshift, Snowflake etc      o Distributed file systems like GCS, S3, HDFS etc      o PySpark      o Airflow / Cloud Composer      o CI/CD and DevOps  ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"GCP, Data modeling, amazon redshift, devops, Cloud, hdfs, big data, Data warehousing, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Python developer,"       Scope of work Development, deployment and testing of backend Services/ components in Python, AWS      Monitoring of alerts, errors and logs of the system      Analyzing the outage information of system/services and identifying root cause, identifying the solution      Perform unit testing of the components developed      Effective tracking and closure of assign tickets/defects      Identifying performance issues if any in the developed components/ systems      Interacting with the overseas clients in scrum ceremonies, requirement discussion, other calls        Responsibility        Be part of a global distributed team that designs, implement, test and deploy SEO based components/tools      Be an excellent problem solver and willing to roll up sleeves to      tackle any issue thrown on the way      Creating documentation, runbooks, monitoring and alerting documents, unit tests        Understand and embrace our core values:        Transparency, Accountable, Generous, Fun, Empathetic and Exceptional              C) Skills Required      Essential Skills Proficiency in the following technologies:              Python      AWS tech stack- Concept of S3, EC2, SNS, VPC, Lambda, ECS, No      sql database (DynamoDB)      Docker      Experience in full life cycle of software development (SDLC)      Excellent communication and interpersonal skill        Desired Skills        Experience working remotely with a distributed global team      Knowledge of Jira, Confluence, Slack      Knowledge of Terraform        D) Other Information        Educational Qualifications Bachelor's degree/MCA      Experience 3 - 6 Years    ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Backend, Scrum, Unit testing, SEO, JIRA, Business solutions, AWS, SDLC, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer,       Experience writing APIs using Django Rest Framework                Experience in Git and Bitbucket                Experience with PostgresSQL and PostGIS                Knowledge of AWS cloud services will be good.                    Knowledge of IoT based systems will be good.              ,2.10E+11,21-03-2024,19-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Software Product,"css, bootstrap, bitbucket, jquery, iot, docker, sql, git, java, postgresql, aws cloud, linux, json, html, mysql, web development, api, mongodb, postgis, rest, geoserver, cloud services, python, github, c, python development, machine learning, javascript, django, system, aws",-,9am-6pm,"Full Time, Permanent",Hexatic Tech,Organization,Hexatic Tech,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer - Lead," Python Developer responsibilities include developing and maintaining AI    pipelines, including data preprocessing, feature extraction, model training,    and evaluation.      Responsibilities:      * Designing, developing, and implementing generative AI models and algorithms    utilizing state-of-the-art techniques such as GPT, VAE, and GANs.      * Conducting research to stay up-to-date with the latest advancements in    generative AI, machine learning, and deep learning techniques and identify    opportunities to integrate them into our products and services.      * 2-3 years of Experience in creating rest api using popular python web    frameworks like Django, flask or fastapi.      * Knowledge of databases like postgres, elastic, mongo etc.      * Knowledge of working with external integrations like redis, kafka, s3, ec2    etc.      * Some experience in ML integrations will be a plus.      Requirements:      * Work experience as a Python Developer      * Team spirit      * Good problem-solving skills      * Proficient in Python and have experience with machine learning libraries    and frameworks such as TensorFlow, PyTorch, or Keras.      * strong knowledge of data structures, algorithms, and software engineering    principles      * Nice to have experience with natural language processing (NLP) techniques    and tools, such as SpaCy, NLTK, or Hugging Face  ",1.40E+11,14-02-2024,14-05-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Training, deep learning, Web technologies, Django, Machine learning, Development Lead, Data structures, Natural language processing, Research, Python",-,9am-6pm,"Full Time, Permanent",Tekdi Technologies Pvt. Ltd.,Organization,Tekdi Technologies Pvt. Ltd.,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Development Engineer,"Project Role : Software Development Engineer Project Role Description : Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.  Must have skills : Google BigQuery Good to have skills : NA Educational Qualification : minimum 15 years of full time education Project Role :Software Development Engineer Project Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work. Must have Skills :Google BigQueryGood to Have Skills : Job Requirements :Key Responsibilities :Analyze and model client market and key performance data Use analytical tools and techniques to develop business insights and improve decisionmaking 1:Data Proc PubSub Data flow Kalka Streaming Looker SQL No FLEX2:Proven track record of delivering data integration data warehousing soln3:Strong SQL And Handson Pro in BigQuery SQL languageExp in Shell Scripting Python No FLEX4:Exp with data integration and migration projects Oracle SQL Technical Experience :Google BigQuery1:Expert in Python NO FLEX Strong handson knowledge in SQL NO FLEX Python programming using Pandas NumPy deep understanding of various data structure dictionary array list tree etc experiences in pytest code coverage skills2:Exp with building solutions using cloud native services:bucket storage Big Query cloud function pub sub composer and Kubernetes NO FLEX3:Pro with tools to automate AZDO CI CD pipelines like ControlM GitHub JIRA confluence CI CD Pipeline Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills presentation skills ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:minimum 15 years of full time education Additional Information : Qualification minimum 15 years of full time education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sql, flex, bigquery, python, google, continuous integration, kubernetes, confluence, data warehousing, numpy, control-m, java, git, shell scripting, html, mysql, jira, github, software development, oracle, javascript, sql server, pandas, oracle sql, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Heroku Kafka Developer,"1) Design, develop, and maintain scalable and reliable data streaming solutions using Heroku Kafka and/or Kafka PostgreSQL , Snowflake 2) Collaborate with cross-functional teams to gather requirements, define architecture, and implement solutions that meet business needs. 3) Build and optimize data pipelines for ingestion, processing, and delivery of real-time data 4) Configure and manage Kafka clusters on Heroku or PostgreSQL for high availability, fault tolerance, and performance. 5) Monitor system performance, troubleshoot issues, and implement solutions to ensure optimal operation of Kafka infrastructure. 6) Work closely with DevOps and infrastructure teams to automate deployment, scaling, and monitoring of Kafka clusters. 7) Develop and maintain documentation, best practices, and guidelines for Kafka development and operations. 8) Stay updated with the latest trends and advancements in Kafka ecosystem and recommend innovative solutions to enhance system efficiency. 9) Good to have Data lake house and Data lake knowledge. Location- Remote,  Delhi NCR, Bangalore, Chennai, Pune, Kolkata, Ahmedabad, Mumbai, Hyderabad",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Heroku Kafka, Kafka PostgreSQL, Snowflake, documentation, Data lake, Heroku Kafka Developer",-,9am-6pm,"Full Time, Permanent",D-TechWorks Pvt Ltd,Organization,D-TechWorks Pvt Ltd,-,"Mumbai, Bengaluru, Delhi / NCR","Mumbai, Bengaluru, Delhi / NCR",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Building Tool DBT Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : B Tech or any other relevant degree Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Data Building Tool DBT. Responsibility: Translate functional specifications and change requests into technical specifications Translate business requirement document, functional specification, and technical specification to related coding Develop efficient code with unit testing and code documentation Ensuring accuracy and integrity of data and applications through analysis, coding, documenting, testing, and problem solving Communicate with all the project stakeholders on the project status Manage, monitor, and ensure the security and privacy of data to satisfy business needs Contribute to the automation of modules, wherever required To be proficient in written, verbal and presentation communication (English) Co-ordinating with the UAT teamRequirement: Design, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks, ensure the effective transformation and load data from diverse sources into data warehouse or data lake. Migrate legacy transformation code into modular DBT data models. Clear understanding of DBT projects, packages, macros, Jinja, cataloging and workflows Integrating DBT with other tools like Databricks, Synapse, ADLS2, Purview Build reliable and maintainable ETL Workflows on Databricks. Implement and manage data models in DBT, guarantee accurate data transformation and alignment with business needs. Write and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance. Establish best DBT processes to improve performance, scalability, and reliability. Expertise in SQL and a strong understanding of Data Warehouse concepts and Modern Data Architectures. Experience in Data validation and performing unit testing Sound aptitude, outstanding logical reasoning, and analytical skills Willingness to learn and take initiatives Ability to adapt to fast-paced Agile environment Additional Information: The candidate should have a minimum of 3 years of experience in Data Building Tool DBT. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification B Tech or any other relevant degree",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"presentation skills, data architecture, sql, data warehousing concepts, presentation communication, jinja, data validation, unit testing, data warehousing, machine learning, data engineering, data bricks, data modeling, agile, data transformation, etl, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Building Tool DBT Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : B Tech or any other relevant degree Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Data Building Tool DBT. Responsibility: Translate functional specifications and change requests into technical specifications Translate business requirement document, functional specification, and technical specification to related coding Develop efficient code with unit testing and code documentation Ensuring accuracy and integrity of data and applications through analysis, coding, documenting, testing, and problem solving Communicate with all the project stakeholders on the project status Manage, monitor, and ensure the security and privacy of data to satisfy business needs Contribute to the automation of modules, wherever required To be proficient in written, verbal and presentation communication (English) Co-ordinating with the UAT teamRequirement: Design, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks, ensure the effective transformation and load data from diverse sources into data warehouse or data lake. Migrate legacy transformation code into modular DBT data models. Clear understanding of DBT projects, packages, macros, Jinja, cataloging and workflows Integrating DBT with other tools like Databricks, Synapse, ADLS2, Purview Build reliable and maintainable ETL Workflows on Databricks. Implement and manage data models in DBT, guarantee accurate data transformation and alignment with business needs. Write and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance. Establish best DBT processes to improve performance, scalability, and reliability. Expertise in SQL and a strong understanding of Data Warehouse concepts and Modern Data Architectures. Experience in Data validation and performing unit testing Sound aptitude, outstanding logical reasoning, and analytical skills Willingness to learn and take initiatives Ability to adapt to fast-paced Agile environment Additional Information: The candidate should have a minimum of 7.5 years of experience in Data Engineering. The ideal candidate will possess a strong educational background in Computer Science, Information Technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. Qualification B Tech or any other relevant degree",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"presentation skills, data architecture, data engineering, sql, data warehousing concepts, jinja, python, unit testing, data warehousing, machine learning, data bricks, data modeling, html, agile, data transformation, etl, presentation communication, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Modern Data Platform Good to have skills : PySpark, Microsoft Azure Data Services Minimum  7.5  year(s) of experience is required Educational Qualification : A:15 Years of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Modern Data Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Modern Data Platform. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Microsoft Azure Data Services and PySpark. Implement data security and privacy measures to ensure data integrity and compliance with regulatory requirements. Troubleshoot and resolve issues related to data platform components and data pipelines. Professional & Technical Skills: Must To Have Skills:Proficiency in Microsoft Azure Modern Data Platform. Good To Have Skills:Experience with Microsoft Azure Data Services and PySpark. Strong understanding of data integration and data modeling concepts. Experience with data security and privacy measures. Experience with troubleshooting and resolving issues related to data platform components and data pipelines. Additional Information: The candidate should have a minimum of 7.5 years of experience in Microsoft Azure Modern Data Platform. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification A:15 Years of full time education",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pyspark, data modeling, microsoft azure, troubleshooting, data integration, hive, amazon redshift, data warehousing, data pipeline, sql, java, spark, mysql, hadoop, big data, etl, data services, python, machine learning, sql server, nosql, amazon ec2, kafka, sqoop, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Building Tool DBT Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : B Tech or any other relevant degree Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Data Building Tool DBT.  Roles & Responsibilities: Translate functional specifications and change requests into technical specifications Translate business requirement document, functional specification, and technical specification to related coding Develop efficient code with unit testing and code documentation Ensuring accuracy and integrity of data and applications through analysis, coding, documenting, testing, and problem solving Communicate with all the project stakeholders on the project status Manage, monitor, and ensure the security and privacy of data to satisfy business needs Contribute to the automation of modules, wherever required To be proficient in written, verbal and presentation communication (English) Co-ordinating with the UAT team Professional & Technical Skills: Design, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks, ensure the effective transformation and load data from diverse sources into data warehouse or data lake. Migrate legacy transformation code into modular DBT data models. Clear understanding of DBT projects, packages, macros, Jinja, cataloging and workflows Integrating DBT with other tools like Databricks, Synapse, ADLS2, Purview Build reliable and maintainable ETL Workflows on Databricks. Implement and manage data models in DBT, guarantee accurate data transformation and alignment with business needs. Write and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance. Establish best DBT processes to improve performance, scalability, and reliability. Expertise in SQL and a strong understanding of Data Warehouse concepts and Modern Data Architectures. Experience in Data validation and performing unit testing Sound aptitude, outstanding logical reasoning, and analytical skills Willingness to learn and take initiatives Ability to adapt to fast-paced Agile environment Additional Information: The candidate should have a minimum of 5 years of experience in Data Engineering. The ideal candidate will possess a strong educational background in Computer Science, Information Technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. Resource can be from any location across India. Qualification B Tech or any other relevant degree",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"presentation skills, data architecture, data engineering, sql, data warehousing concepts, jinja, python, unit testing, data warehousing, machine learning, data bricks, data modeling, html, agile, data transformation, etl, presentation communication, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Building Tool DBT Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : B Tech or any other relevant degree Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Data Building Tool DBT.  Additional Information: The candidate should have a minimum of 3 years of experience in Data Building Tool DBT. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Responsibility: Translate functional specifications and change requests into technical specifications Translate business requirement document, functional specification, and technical specification to related coding Develop efficient code with unit testing and code documentation Ensuring accuracy and integrity of data and applications through analysis, coding, documenting, testing, and problem solving Communicate with all the project stakeholders on the project status Manage, monitor, and ensure the security and privacy of data to satisfy business needs Contribute to the automation of modules, wherever required To be proficient in written, verbal and presentation communication (English) Co-ordinating with the UAT teamRequirement: Design, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks, ensure the effective transformation and load data from diverse sources into data warehouse or data lake. Migrate legacy transformation code into modular DBT data models. Clear understanding of DBT projects, packages, macros, Jinja, cataloging and workflows Integrating DBT with other tools like Databricks, Synapse, ADLS2, Purview Build reliable and maintainable ETL Workflows on Databricks. Implement and manage data models in DBT, guarantee accurate data transformation and alignment with business needs. Write and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance. Establish best DBT processes to improve performance, scalability, and reliability. Expertise in SQL and a strong understanding of Data Warehouse concepts and Modern Data Architectures. Experience in Data validation and performing unit testing Sound aptitude, outstanding logical reasoning, and analytical skills Willingness to learn and take initiatives Ability to adapt to fast-paced Agile environment Qualification B Tech or any other relevant degree",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"presentation skills, data architecture, data engineering, sql, data warehousing concepts, jinja, data validation, unit testing, data warehousing, machine learning, data bricks, data modeling, agile, data transformation, etl, presentation communication, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Cognitive Search Engineer,"Role & responsibilities     Proficient at developing AI/ML solutions   using Python and Large   language models like Azure OpenAI or Google VertexAI Azure   cognitive services Prompt   engineering using Python. Able   to use AI-related Python libraries. Hands-on   experience with langchain is preferred. CI/CD   using Azure DevOps Working   knowledge of model fine-tuning is preferred. Working   knowledge of key Azure cloud concepts and services like: Resource   groups and RBAC Managed   identities and SPNs Azure   storage and data lake gen2 Azure   SQL database, synapse analytics and Azure Databricks Self-starter with strong   analytical skills who can independently work with business teams to   understand requirements and provide solution guidance. Preferred candidate profile     5+ years of experience in building machine learning solutions using Python (preferably on Azure) Minimum Req: Python, Azure OpenAI/Google vertexAI, Azure cognitive services, Databricks, Langchain, Prompt Engineering ",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Azure, python, open Ai, cognitive services, Prompt Engineering, Langchain, Databricks",-,9am-6pm,"Full Time, Permanent",Orcapod Consulting Services,Organization,Orcapod Consulting Services,https://img.naukri.com/logo_images/v3/602389.gif,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,7-12 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Modeler,"Project Role : Data Modeler Project Role Description : Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.  Must have skills : AWS Architecture Good to have skills : Data Modeling Techniques and Methodologies Minimum  5  year(s) of experience is required Educational Qualification : B Tech Summary :As an AWS Architect in the Data Engineering team, you will be responsible for designing and implementing scalable and secure cloud-based solutions using AWS. Your typical day will involve working with various AWS services, designing and implementing data models, and collaborating with cross-functional teams to deliver high-quality solutions.  Roles & Responsibilities:Responsibilities: Designing, implementing, and maintaining scalable data solutions on AWS cloud platform, i.e. data pipelines for ingesting, processing, and transforming data from various sources. Collaborating with stakeholders to understand data requirements and translate them into technical solutions. Architecting data lakes, data warehouses, and data pipelines to support various analytics and reporting needs. Architecting the secure designs / solutions according to the Client's requirement. Implementing data security and governance best practices to ensure compliance with regulations and industry standards. Evaluating and recommending appropriate AWS services and technologies for specific data use cases. Optimizing data infrastructure for performance, reliability, and cost-effectiveness.Skills and Qualifications: In depth knowledge of AWS Services:EC2, VPC, S3, Glue, Athena, Lambda, Lake Formation, etc. Data Warehousing:Designing and implementing data warehouses using best practices. Strong understanding of data engineering principles and practices (ETL/ELT, data modelling, data quality). Problem-solving:Analytical skills to troubleshoot and resolve complex data issues. String in SQL for querying databases and data warehouses. Knowledge of data security and compliance standards. Professional & Technical Skills: Must To Have Skills:Strong experience in AWS architecture and services such as EC2, S3, RDS, and Redshift. Good To Have Skills:Experience with data modeling techniques and methodologies. Solid understanding of data modeling concepts and techniques. Experience with database technologies such as SQL and NoSQL. Experience with ETL tools such as AWS Glue and Apache Spark. Strong understanding of security and compliance requirements in AWS. Experience with DevOps tools such as AWS CloudFormation and Terraform. Additional Information: The candidate should have a minimum of 5 years of experience in AWS architecture and data modeling. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering high-quality solutions. This position is based at our Bengaluru office. Qualification B Tech",40524905907,04-05-2024,02-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"aws iam, information technology, data engineering, data modeling, aws, glue, amazon redshift, data warehousing, aws cloudformation, amazon rds, elt, sql, nosql, aws glue, amazon ec2, lambda expressions, spark, etl tool, devops, athena, terraform, etl, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Informatica Administration Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Any Equivalent Degree Summary :As an Informatica Administrator, you will be responsible for managing and maintaining the Informatica environment. Your typical day will involve monitoring and troubleshooting the Informatica environment, performing upgrades, and ensuring the environment is running optimally.  Roles & Responsibilities: Manage and maintain the Informatica environment, including monitoring and troubleshooting issues. Perform upgrades and patches to the Informatica environment. Ensure the Informatica environment is running optimally, including performance tuning and capacity planning. Collaborate with cross-functional teams to ensure the Informatica environment is integrated with other systems and applications. Professional & Technical Skills: Must To Have Skills:Experience in Informatica Administration. Good To Have Skills:Experience with cloud platforms such as AWS or Azure. Strong understanding of ETL concepts and best practices. Experience with database technologies such as Oracle or SQL Server. Experience with Linux/Unix operating systems. Solid grasp of networking concepts and protocols. Additional Information: The candidate should have a minimum of 3 years of experience in Informatica Administration. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of managing and maintaining Informatica environments. This position is based at our Bengaluru office. Qualification Any Equivalent Degree",30524911748,03-05-2024,01-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"networking concepts, microsoft azure, sql server, informatica administration, etl process, kubernetes, python, oracle, vmware, data warehousing, cloud platform, docker, ansible, sql, git, java, devops, linux, jenkins, cloud infrastructure, etl, aws, cloud computing, unix",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Modeler,"Project Role : Data Modeler Project Role Description : Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.  Must have skills : AWS Architecture Good to have skills : Data Modeling Techniques and Methodologies Minimum  5  year(s) of experience is required Educational Qualification : B Tech Summary :As an AWS Architect in the Data Engineering team, you will be responsible for designing and implementing scalable and secure cloud-based solutions using AWS. Your typical day will involve working with various AWS services, designing and implementing data models, and collaborating with cross-functional teams to deliver high-quality solutions.  Roles & Responsibilities:Responsibilities: Designing, implementing, and maintaining scalable data solutions on AWS cloud platform, i.e. data pipelines for ingesting, processing, and transforming data from various sources. Collaborating with stakeholders to understand data requirements and translate them into technical solutions. Architecting data lakes, data warehouses, and data pipelines to support various analytics and reporting needs. Architecting the secure designs / solutions according to the Client's requirement. Implementing data security and governance best practices to ensure compliance with regulations and industry standards. Evaluating and recommending appropriate AWS services and technologies for specific data use cases. Optimizing data infrastructure for performance, reliability, and cost-effectiveness.Skills and Qualifications: In depth knowledge of AWS Services:EC2, VPC, S3, Glue, Athena, Lambda, Lake Formation, etc. Data Warehousing:Designing and implementing data warehouses using best practices. Strong understanding of data engineering principles and practices (ETL/ELT, data modelling, data quality). Problem-solving:Analytical skills to troubleshoot and resolve complex data issues. String in SQL for querying databases and data warehouses. Knowledge of data security and compliance standards. Professional & Technical Skills: Must To Have Skills:Strong experience in AWS architecture and services such as EC2, S3, RDS, and Redshift. Good To Have Skills:Experience with data modeling techniques and methodologies. Solid understanding of data modeling concepts and techniques. Experience with database technologies such as SQL and NoSQL. Experience with ETL tools such as AWS Glue and Apache Spark. Strong understanding of security and compliance requirements in AWS. Experience with DevOps tools such as AWS CloudFormation and Terraform. Additional Information: The candidate should have a minimum of 5 years of experience in AWS architecture and data modeling. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering high-quality solutions. This position is based at our Bengaluru office. Qualification B Tech",30524910792,03-05-2024,01-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"aws iam, information technology, data engineering, data modeling, aws, glue, amazon redshift, data warehousing, elt, aws cloudformation, amazon rds, sql, nosql, aws glue, amazon ec2, lambda expressions, spark, etl tool, devops, athena, terraform, etl, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance. Must have skills : Microsoft Azure Windows Virtual Desktop Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : BE or Equivalent Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance using Microsoft Azure Windows Virtual Desktop.  Roles & Responsibilities: Deploy infrastructure and platform environments using Microsoft Azure Windows Virtual Desktop. Create a proof of architecture to test architecture viability, security, and performance. Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure. Collaborate with cross-functional teams to ensure successful deployment of cloud solutions. Professional & Technical Skills: Proficiency in Microsoft Azure Windows Virtual Desktop. Experience in designing, building, testing, and deploying cloud application solutions. Strong understanding of cloud infrastructure and platform environments. Experience in creating a proof of architecture to test architecture viability, security, and performance. Experience in collaborating with cross-functional teams to ensure successful deployment of cloud solutions. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Windows Virtual Desktop. The ideal candidate will possess a strong educational background in software engineering, computer science, or a related field, along with a proven track record of delivering impactful cloud solutions. This position is based at our Bengaluru office. Qualification BE or Equivalent",20524910662,02-05-2024,31-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure iaas, microsoft azure, desktop, cloud infrastructure, software engineering, c#, kubernetes, python, vmware, networking, cloud platform, azure devops, docker, ansible, git, java, devops, linux, paas, jenkins, iaas, aws, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
VIS & BI Strategy Practitioner,"Project Role : VIS & BI Strategy Practitioner Project Role Description : Develop a VIS/BI vision, business case, and modernization strategy; discover and assess VIS/BI opportunities; identify industry- and function-centered VIS/BI use cases; guide clients on industry and function relevant measures and metrics; develop a VIS/BI roadmap and operating model; help drive adoption, decision making and behavior change; measure VIS/BI value.  Must have skills : Machine Learning Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : BE Summary :You will be responsible for utilizing data science methodology to inform the design research process. Your typical day will involve employing statistical and quantitative data analysis to enhance solution development.  Roles & Responsibilities: Having broad technology background, with focus on developing software applications in Python for production ready applications Writing container-based code and testing code, debugging programs and integrating applications with third-party web services, deployment Work with cross functional team to scale the architecture supporting AI models Support data needs for model training/ensemble the models Architecture, security reviews, NFRs Experience in Agile, DevOps, driving best practices Lead the development and deployment of advanced machine learning models using Python and associated libraries like Pandas, NumPy, and Scikit-learn. Conduct detailed analysis of complex data sets, employing statistical methodologies and data munging techniques for actionable insights, alongside a robust understanding of statistical analysis and machine learning algorithms. Collaborate with cross-functional teams, applying expertise in diverse machine learning algorithms, including experience in implementing various algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms. Communicate technical findings effectively to stakeholders, utilizing data visualization tools like Tableau or Power BI for clarity. Stay updated with the latest advancements in machine learning and data science, integrating innovative approaches for sustained competitive advantage, including experience with TensorFlow, Natural Language Processing (NLP), and Big Data technologies. Professional & Technical Skills: Using APIs in python Flask, Falcon or Django, Kafka ML and python libraries as scikit-learn, numpy and pandas.  Machine learning, Deep learning, Image Processing Familiarity with Apache SOLR, MongoDB Familiarity with Celery, Rabbit MQ for background task queues Data pre-processing techniques GIT, Docker, Jenkins, Swagger, Cloud AWS/Azure Must To Have Skills:Proficiency in Machine Learning. Good To Have Skills:Experience with TensorFlow, Natural Language Processing (NLP), and Big Data technologies. Strong understanding of statistical analysis and machine learning algorithms. Experience with data visualization tools such as Tableau or Power BI. Experience in implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms. Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.Additional Information: The candidate should have a minimum of 7.5 years of experience in Machine Learning. The ideal candidate will possess a strong educational background in statistics, mathematics, computer science, or a related field, along with a proven track record of delivering impactful data-driven solutions. Qualification BE",20524909210,02-05-2024,31-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"numpy, machine learning, machine learning algorithms, statistics, data munging, python, natural language processing, scikit-learn, big data technologies, power bi, rabbitmq, pandas, deep learning, tableau, tensorflow, git, aws cloud, devops, agile, api, flask, mongodb",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AI Platform Engineer,"Project Role : AI Platform Engineer Project Role Description : Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing.  Must have skills : Natural Language Processing (NLP) Good to have skills : NA Minimum  12  year(s) of experience is required Educational Qualification : BE Summary :As an AI Platform Engineer, you will be responsible for developing applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, and natural language processing. Your typical day will involve working with NLP, developing and deploying AI models, and collaborating with cross-functional teams.  Roles & Responsibilities:-Build predictive models, develop advanced algorithms that extract and classify information from large datasets quantify model performance  Evaluate emerging technologies that may contribute to our analytical platform Identify and exploit new patterns in data using various techniques Worked with ML data mining toolkits like NLP, Semantic Web, R, Core NLP, NLTK etc. Information retrieval libraries like Lucene, SOLR fast paced, test driven, collaborative and iterative programming environment. Professional & Technical Skills: Work with developers to integrate front end with backend, knowledge in Insurance, Banking and Capital markets domains Strong coding ability in Python, R programming. Should know about crunching billions of datapoints for statistical modeling, data mining for insights and recommendation solutions. Additional Information: The candidate should have a minimum of 5 years of experience in NLP. The ideal candidate will possess a strong educational background in computer science, mathematics, or a related field, along with a proven track record of delivering impactful AI-driven solutions. Qualification BE",20524908894,02-05-2024,31-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data mining, nltk, python, natural language processing, r, algorithms, scikit-learn, numpy, artificial intelligence, docker, sql, deep learning, tensorflow, spacy, java, git, data science, linux, keras, mysql, data structures, hadoop, machine learning, pandas, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Building Tool DBT Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : B Tech or any other relevant degree Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Data Building Tool DBT. Responsibility: Translate functional specifications and change requests into technical specifications Translate business requirement document, functional specification, and technical specification to related coding Develop efficient code with unit testing and code documentation Ensuring accuracy and integrity of data and applications through analysis, coding, documenting, testing, and problem solving Communicate with all the project stakeholders on the project status Manage, monitor, and ensure the security and privacy of data to satisfy business needs Contribute to the automation of modules, wherever required To be proficient in written, verbal and presentation communication (English) Co-ordinating with the UAT teamRequirement: Design, develop, and maintain scalable data models and transformations using DBT in conjunction with Databricks, ensure the effective transformation and load data from diverse sources into data warehouse or data lake. Migrate legacy transformation code into modular DBT data models. Clear understanding of DBT projects, packages, macros, Jinja, cataloging and workflows Integrating DBT with other tools like Databricks, Synapse, ADLS2, Purview Build reliable and maintainable ETL Workflows on Databricks. Implement and manage data models in DBT, guarantee accurate data transformation and alignment with business needs. Write and optimize SQL queries within DBT to enhance data transformation processes and improve overall performance. Establish best DBT processes to improve performance, scalability, and reliability. Expertise in SQL and a strong understanding of Data Warehouse concepts and Modern Data Architectures. Experience in Data validation and performing unit testing Sound aptitude, outstanding logical reasoning, and analytical skills Willingness to learn and take initiatives Ability to adapt to fast-paced Agile environment Additional Information: The candidate should have a minimum of 3 years of experience in Data Building Tool DBT. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification B Tech or any other relevant degree",10524911226,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"presentation skills, data architecture, sql, data warehousing concepts, presentation communication, jinja, data validation, unit testing, data warehousing, machine learning, data engineering, data bricks, data modeling, agile, data transformation, etl, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Salesforce Omnistudio Platform Good to have skills : Salesforce Lightning Web Components Minimum  5  year(s) of experience is required Educational Qualification : 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Salesforce Omnistudio Platform.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Salesforce Omnistudio Platform. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data platform components using Salesforce Omnistudio Platform. Troubleshoot and resolve issues related to data platform components using Salesforce Omnistudio Platform. Ensure data platform components are scalable, reliable, and secure. Professional & Technical Skills: Must To Have Skills:Experience in Salesforce Omnistudio Platform. Good To Have Skills:Experience in Salesforce Lightning Web Components. Strong understanding of data platform components and their integration with systems and data models. Experience in developing and maintaining data platform components. Ability to troubleshoot and resolve issues related to data platform components. Knowledge of scalability, reliability, and security of data platform components. Additional Information: The candidate should have a minimum of 5 years of experience in Salesforce Omnistudio Platform. The ideal candidate will possess a strong educational background in software engineering or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of education",10524910819,01-05-2024,30-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"salesforce lightning, apex, salesforce, data modeling, software engineering, visualforce, rest, soql, css, web services, sfdc, triggers, machine learning, javascript, salesforce crm, sales force development, data loader, java, installation, html",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Apache Kafka Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 yrs of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Apache Kafka.  Roles & Responsibilities: Collaborate with Integration Architects and Data Architects to design and implement data platform components using Apache Kafka. Assist with the development and deployment of data pipelines using Apache Kafka. Ensure data quality and integrity by implementing data governance policies and procedures. Troubleshoot and resolve issues related to data platform components and data pipelines. Professional & Technical Skills: Must To Have Skills:Experience with Apache Kafka. Good To Have Skills:Experience with other data platform components such as Apache Spark, Hadoop, and Cassandra. Strong understanding of data governance policies and procedures. Experience with data pipeline development and deployment. Experience with troubleshooting and resolving issues related to data platform components and data pipelines. Additional Information: The candidate should have a minimum of 3 years of experience in Apache Kafka. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 yrs of full time education",10524910519,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data engineering, kafka, data governance, troubleshooting, hadoop, hive, python, data warehousing, microsoft azure, data pipeline, machine learning, sql, nosql, data quality, amazon ec2, java, data modeling, cassandra, spark, mysql, sqoop, aws, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Apache Kafka Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 yrs of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Apache Kafka.  Roles & Responsibilities: Collaborate with Integration Architects and Data Architects to design and implement data platform components using Apache Kafka. Assist with the development and deployment of data pipelines using Apache Kafka. Ensure data quality and integrity by implementing data governance policies and procedures. Troubleshoot and resolve issues related to data platform components and data pipelines. Professional & Technical Skills: Must To Have Skills:Experience with Apache Kafka. Good To Have Skills:Experience with other data platform components such as Apache Spark, Hadoop, and Cassandra. Strong understanding of data governance policies and procedures. Experience with data pipeline development and deployment. Experience with troubleshooting and resolving issues related to data platform components and data pipelines. Additional Information: The candidate should have a minimum of 3 years of experience in Apache Kafka. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 yrs of full time education",10524910144,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data engineering, kafka, data governance, troubleshooting, hadoop, hive, python, data warehousing, microsoft azure, data pipeline, machine learning, sql, nosql, data quality, amazon ec2, java, data modeling, cassandra, spark, mysql, sqoop, aws, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Qlik Sense Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Graduation Manage day to day operations of Qlik Platforms.Deliver customer service within specified service levels and typically around more complex problems including diagnosis and resolution of issues within a Qlik Platform.Perform periodic maintenance and upgrades of Qlik Software components.Collaborating with the Qlik global support team to solve customer technical issuesResolving customers technical issues through diligent research, reproduction, and troubleshootingKnowledge of ITSM Tools and processes.Actively contributing with creating technical articles, using and maintaining QLIK knowledge baseInstall, configure and upgrade Qlik Sense application.Manage migration of QLIK objects between Development, QA and Production environmentsAnalyze Windows Event logs and Qlik Sense Server log files to troubleshoot and solve issues.Identify potential solutions to resolve and prevent service interruptions.Provide Qlik Sense administration services via Management Console :managing licenses, setting up permission, designing folder structure and scheduling tasks for publishing purposes.Administer and maintain access to Qlik Sense Management Console.Proactively monitor the Qlik Sense environment to ensure performance and scalability.Perform day to day administrative activities on the Qlik applications and servers.Monitor daily task execution on Qlik Sense.Work with product teams in trouble shooting, new feature discussion and operational support.Prepare and update disaster recovery steps for the applications with the help of infrastructure teams.Perform fire drill operations on the Applications.Perform regular compliance adherence and monitoring activities like user access management & roles, maintain vulnerability scan results and fixes, following enterprise standards on data security.Follow SLA adherence and fulfill service request and resolve incidents.Define best practices and governance for Qlik Sense deploymentsExperience with Sense Monitoring Dashboard.Experience in performance tuning of Qlik Sense server.Working knowledge of clustering/multi node installation of Qlik Sense in AWS VM environment.Working knowledge of AWS with load balancer, DNS and firewallMentor developers and provide technical documentation, developer guides and assist in trouble shooting.Participate in the design reviews to ensure conformance to design/certification standardsMaintain knowledge on the latest Qlik Sense and other BI technology and best practicesEffectively communicate with client teams and internal teamExperience in NPrinting and Alerting is added advantageFormal Qlik Sense certification is a plusExperience with shell scripting is a plus",10524909421,01-05-2024,30-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"performance tuning, bi, dns, qlikview, shell scripting, python, load balancing, oracle, data analysis, qlikview development, power bi, data warehousing, machine learning, business intelligence, research, sql server, sql, qlik nprinting, tableau, data modeling, vm, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Architect- AWS,"     Sr AWS Data Engineer JD:RoleThe ability to easily find, access, and analyze data across an organization is key for every modern business to be able to efficiently make decisions, optimize processes, and to create new business models       As a Sr Data Engineer in the Healthcare Digital Data - Data Governance Architecture team you will work hands-on to deliver and maintain the pipelines required by the Healthcare business functions to derive value from their data     For this, you will bring data from a varied landscape of source systems into our cloud-based analytics stack and implement necessary cleaning and pre-processing steps in close collaboration with our business customers     Furthermore, you will work closely together with our Data Governance and Quality Compliance teams to ensure that all data assets are governed according to the FAIR principles     To keep the engineering team scalable, you and your peers will create reusable components, libraries, and infrastructure that will be used to accelerate the pace with which future use-cases can be delivered     What we offerYou will be part of a team dedicated to delivering state-of-the-art solutions for enabling data analytics use cases across the Healthcare sector of a leading, global Science Technology company     As such, you will have the unique opportunity to gain insight into our diverse business functions allowing you to expand your skills in various technical, scientific, and business domains     Working in a project-based way covering a multitude of data domains and technological stacks, you will be able to significantly develop your skills and experience as a Data Engineer       Who you are        MSc/PhD in Computer Science or related field and 8+ years of work experience in a relevant capacityExperience in working with cloud environments such as, Hadoop, AWS, GCP, and Azure     Experience with enforcing security controls and best practices to protect sensitive data within AWS data pipelines, including encryption, access controls, and auditing mechanisms     Agile mindset, a spirit of initiative, and desire to work hands-on together with your team     Interest in solving challenging technical problems and developing the future data architecture that will enable the implementation of innovative data analytics use-cases     Experience in leading small to medium-sized teams     Experience in creating architectures for ETL processes for batch as well as streaming ingestion     Knowledge of designing and validating software stacks for GxP relevant contexts as well as working with PII data     Familiarity with the data domains covering the Pharma value-chain (eg research, clinical, regulatory, manufacturing, supply chain, and commercial)     Strong, hands-on experience in working with Python, Pyspark R codebases, proficiency in additional programming languages (eg C/C++, Rust, Typescript, Java, ) is expected     Knowledge of database technologies for OLTP and OLAP workloads and a firm grasp of SQL     Experience working with Apache Spark and the Hadoop ecosystemWorking with heterogenous compute environments and multi-platform setups     Experience in working with cloud environments such as, Hadoop, AWS, GCP, and AzureBasic knowledge of Statistics and Machine Learning algorithms is favorable       This is the respective role description:       The ability to easily find, access, and analyze data across an organization is key for every modern business to be able to efficiently make decisions, optimize processes, and to create new business models     The Data Architect plays a key role in unlocking this potential by defining and implementing a harmonized data architecture for Healthcare     Together with a team of Data Engineers, the Data Architect is responsible for the delivery and maintenance of reliable data pipelines for the ingestion of data from a heterogenous ecosystem as well as the creation of specialized, cloud-based data processing and analytics stacks     He/she will work hand in hand with our Data Governance team to ensure that all data assets are covered by our governance process     During the process the Data Architect will collaborate closely with our business, corporate functions, as well as out IT organization, in order to gather requirements and shape a global framework of engineering best practices   ",10524500113,01-05-2024,30-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Supply chain, C++, Pharma, Machine learning, Agile, Healthcare, OLAP, Python, Data architecture",-,9am-6pm,"Full Time, Permanent",Merck,Organization,Merck,https://img.naukimg.com/logo_images/groups/v1/269320.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Principal Software Engineer (Java/Python & Data Engineering),"                               As a Principal Software Engineer, you will develop best practices and making architectural choices to rapidly improve critical data processing & analytics pipelines     You will collaborate with and wonderful software engineers     You will lead solutions to sophisticated and modern engineering problems     As part of the team, you will grow bring data closer to our users     You will make critical choices, and improve the platforms reliability, resiliency, and scalability     You will report to Engineering Manager     You will work from Pune location                                         Responsibilities            Contribute to the teams vision and articulate strategies to have fundamental impact at our massive scale.     You will need a product-focused mindset. It is essential for you to understand requirements and architect systems to scale and extend to accommodate those needs     Diagnose complex problems in distributed systems, develop and document technical solutions and sequence work to make fast, iterative deliveries and improvements     Build and maintain high-performance, fault-tolerant and scalable distributed systems that can handle our massive scale     Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency)     Automate cloud infrastructure, services, and observability     Develop CI/CD pipelines and testing automation     Establish and uphold best engineering practices through thorough code and design reviews and improved processes     Groom junior engineers through mentoring and delegation     Promote a culture of trust, respect and inclusion within your team     Design & defining the interaction between the different components     Fast prototyping of features and get feedback.     Develop new features in Java/Python, AWS Stack               Minimum Qualifications           Bachelor s degree in Computer Science, Engineering or related field, or equivalent training, fellowship or work experience     8-12 years of relevant industry experience in large back-end distributed systems and cloud computing.     Solid overall programming skills, able to write modular, maintainable code in Java/Python & SQL     Expert understanding of SQL, dimensional modelling, and at least one relational database including solid contribution to ERD     Good Understanding of front end frameworks like ReactJS/AngularJS     Solid proficiency with automation frameworks/tools like Git, Jenkins, Ansible, and Cloudformation (or Terraform)     Solid Proficiency with containers and    infrastructure-as-code    fundamentals     Solid Proficiency with Amazon Web Services     Good Understanding of Data Engineering and related frameworks     Familiarity with MVC, SOA, Restful Web services               Preferred Qualifications           Experience with data ingestion and SQL databases     Experience with Hadoop / Spark Source Code     Experience with Hive and/or Snowflake     Experience in Hadoop 2.0 and its ecosystem     Experience with Airflow     ",20224501054,02-02-2024,02-05-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Analytics / KPO / Research,"Cloud computing, Automation, Front end, SOA, MVC, Distribution system, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Autodesk,Organization,Autodesk,https://img.naukimg.com/logo_images/groups/v1/690506.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer,   Python with OOPs (Mandatory for all profiles)     PySpark (Mandatory for senior Profiles)     SQL/PLSQL (Mandatory for all profiles)     Knowledge on Azure (Good To have) (work on GCP or AWS should also be fine)     Snowflake (Good To have)     Unix Shell Script (Good to have)     Kubernetes/Docker (Good to Have)   ,60524500346,06-05-2024,04-08-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Insurance,"Unix, GCP, OOPS, Shell scripting, PLSQL, AWS, SQL, Python",-,9am-6pm,"Full Time, Permanent",Chubb,Organization,Chubb,-,"Warangal, Hyderabad, Nizamabad","Warangal, Hyderabad, Nizamabad",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Integration Engineer,"Project Role : Integration Engineer Project Role Description : Provide consultative Business and System Integration services to help clients implement effective solutions. Understand and translate customer needs into business and technology solutions. Drive discussions and consult on transformation, the customer journey, functional/application designs and ensure technology and business solutions represent business requirements.  Must have skills : Microsoft Azure PaaS Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Graduate Summary :As an Integration Engineer, you will be responsible for providing consultative Business and System Integration services to help clients implement effective solutions. Your typical day will involve understanding and translating customer needs into business and technology solutions, driving discussions and consult on transformation, the customer journey, functional/application designs and ensuring technology and business solutions represent business requirements.  Roles & Responsibilities: Design and implement Microsoft Azure PaaS solutions for clients. Provide technical expertise and guidance to clients on Microsoft Azure PaaS solutions. Collaborate with cross-functional teams to ensure successful implementation of Microsoft Azure PaaS solutions. Develop and maintain technical documentation related to Microsoft Azure PaaS solutions. Stay updated with the latest advancements in Microsoft Azure PaaS technologies and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Strong experience in Microsoft Azure PaaS. Good To Have Skills:Experience in other cloud platforms such as AWS or Google Cloud Platform. Experience in designing and implementing cloud-based solutions. Experience in developing and maintaining technical documentation. Strong understanding of cloud security and compliance. Experience in working with cross-functional teams. Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure PaaS. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful solutions. This position is based at our Mumbai office. Qualification Graduate",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure iaas, cloud security, system integration, gcp, aws, c#, asp.net mvc, entity framework, microsoft azure, sql server, azure devops, jquery, active directory, linux, powershell, paas, asp.net, windows server, web api, iaas, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google Cloud Full Stack Developer,"     As a Google cloud full stack developer, you will be responsible for software development, cloud computing & database management to develop & deploy solutions.     Responsibilities        Responsible fordesign, development, implementation, operation improvement, debug cloudenvironments in GCP & Cloud Management Platform             Performsengineering design evaluations for new environment builds,             Architect,implement and improve possible automations for cloud environments using nativeor 3rd party tools like Terraform, Salt, Chef etc,             Recommendsalterations to development and design to improve the quality of products and/orprocedures,             Implementationof industry standard security practices during implementation and maintain itthroughout the lifecycle.             Has knowledge ofcommonly used concepts, practices, and procedures through automation tools.Relies on instructions and pre-established guidelines to perform the functionsof the job,         Have a goodunderstanding of Google best practice/recommendation and should be able toalign the same with the customer requirements to deliver best in class solutionfor the customer.                 Skills and Qualifications          Graduate in engineering or computer science.             2-8 years of cloud-based development experience (GCPPreferable)               GCPCertification - Associate Cloud Engineer/Professional Cloud Architect/Professional Cloud Developer               Knowledgein                Cloudplatforms: GCP & Firebase                 Languages:Python & JavaScript                 Database:     SQL & Big Query             In depth knowledge of key GCP services - GCE, GKE, GAE,GCS, Cloud SQL, VPC, Resource Manager, Stack Driver, Cloud CDN, Cloud IAM,GSuite(good to have) Infrastructure as Code etc     ..             In depthknowledge of GCP SKD/API tools like gcloud, gutils, kubeclt etc.,               CI and CD using Jenkins, Ansible, Python, Shell scripting,Change management, GCP Cloud product & Services knowledge and prod supportactivities.                 Experience with web or mobile app development (JavaScript,iOS or Android)             Must have worked on Workflow monitoring in distributedcomputing environment for implementing Big Data solution experience in GoogleCloud Platform modules like Dataproc, Dataflow, Composer, BigTable, GCS             In-depth knowledge of design, implementation, engineering,automation and devops implementation, service operation and service improvementinitiatives             Experience in emerging reporting tools and technologieslike Tableau,Bigtable, Datastudio & Analytics dashboards etc.             Experience in collaborating with teams across geographiesand functions.         ",60524501160,06-05-2024,04-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Advertising & Marketing,"Cloud computing, Automation, Change management, Shell scripting, Javascript, Workflow, Analytics, SQL, Android, Python",-,9am-6pm,"Full Time, Permanent",Pivotroots Digital,Organization,Pivotroots Digital,-,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
PYSPARK DSA Professional,"Bachelors or masters degree in computer science, Engineering, or related field 5-8 years of experience in data engineering and machine learning Extensive experience with Python programming, including Pandas, NumPy, and various libraries for data manipulation and analysis Hands-on experience with Spark architecture, including data frame operations, lazy evaluation, and UDFs Strong understanding of DevOps principles and experience with tools like Docker, Kubernetes, and Jenkins Familiarity with machine learning libraries such as Spark ML-lib or Azure ML is a plus Previous experience in product-oriented organizations or Tier 1 companies is preferred Primary Skills Utilize advanced SQL techniques such as joins with in-line views and self-joins for data manipulation and extraction Proficient in Python programming, including but not limited to variables, functions, loops, conditions, and various data structures Implement object-oriented programming concepts including polymorphism, abstract classes, and interfaces for code modularity and reusability Develop and maintain data pipelines using Spark architecture, understanding nodes, clusters, lazy evaluation, and DAG in Spark Perform data frame operations and user-defined functions (UDFs) for efficient data processing in Spark Integrate APIs for data retrieval and interaction with external systems Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions Apply best practices in software development and data engineering to ensure scalability, reliability, and performance of the ML pipelines Secondary Skills Good Communication Skills",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data engineering, object-oriented programming concepts, software development, Python programming, Spark, machine learning, SQL",-,9am-6pm,"Full Time, Permanent",Capgemini,Organization,Capgemini,https://www.naukri.com/hotjobs/images/v3/captech_jun20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Machine Learning Engineer,"Responsibilities Responsible for successful delivery of MLOps solutions and services in client consulting environments; Define key business problems to be solved; formulate high level solution approaches and identify data to solve those problems, develop, analyze/draw conclusions and present to client. Assist clients with operationalization metrics to track performance of ML Models Agile trained to manage team effort and track through JIRA High Impact Communication- Assesses the target audience need, prepares and practices a logical flow, answers audience questions appropriately and sticks to timeline. Technical and Professional Requirements: Technical knowledge- has expertise in cloud technologies, specifically MS Azure, and services with hands on coding to Python Programming - Expert and Experienced - 4 -5 years DevOps Working knowledge with implementation experience - 1 or 2 projects a minimum Hands-On MS Azure Cloud knowledge Understand and take requirements on Operationalization of ML Models from Data Scientist Help team with ML Pipelines from creation to execution List Azure services required for deployment, Azure Data bricks and Azure DevOps Setup Assist team to coding standards (flake8 etc) Guide team to debug on issues with pipeline failures Engage with Business / Stakeholders with status update on progress of development and issue fix Automation, Technology and Process Improvement for the deployed projects Setup Standards related to Coding, Pipelines and Documentation Adhere to KPI / SLA for Pipeline Run, Execution Research on new topics, services and enhancements in Cloud Technologies  Preferred Skills: Technology->Machine Learning->Python  Additional Responsibilities: Masters degree in Computer Science Engineering, with Relevant experience in the field of MLOps / Cloud Domain experience in Capital Markets, Banking, Risk and Compliance etc. Exposure to US/ overseas markets is preferred Azure Certified DP100, AZ/AI900 Domain / Technical / Tools Knowledge: Object oriented programming, coding standards, architecture & design patterns, Config management, Package Management, Logging, documentation Experience in Test Driven Development and experience in using Pytest frameworks, git version control, Rest APIs Azure ML best practices in environment management, run time configurations (Azure ML & Databricks clusters), alerts. Experience designing and implementing ML Systems & pipelines, MLOps practices Exposure to event driven orchestration, Online Model deployment Contribute towards establishing best practices in MLOps Systems development Proficiency with data analysis tools (e.g., SQL, R & Python) High level understanding of database concepts/reporting & Data Science concepts Hands on experience in working with client IT/Business teams in gathering business requirement and converting into requirement for development team Experience in managing client relationship and developing business cases for opportunities Azure AZ-900 Certification with Azure Architecture understanding is a plus  Educational Requirements Bachelor of Engineering  Service Line Data & Analytics Unit *  Location of posting is subject to business requirements",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Machine Learning, client relationship management, python, r, data analysis, data analytics, data science, team management, business development, sql",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://www.naukri.com/hotjobs/images/v3/infosys_nov13.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Tableau Good to have skills : MySQL Minimum  2  year(s) of experience is required Educational Qualification : 15 years of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and utilizing Tableau for data visualization.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Utilize Tableau for data visualization. Assist with the implementation and maintenance of MySQL databases. Assist with the design and implementation of AWS architecture. Assist with the development of Python scripts for data processing and analysis. Professional & Technical Skills: Must To Have Skills:Proficiency in Tableau. Good To Have Skills:Experience with MySQL, AWS Architecture, and Python (Programming Language). Solid understanding of data platform components and their integration. Experience with data visualization tools such as Tableau. Experience with MySQL database implementation and maintenance. Experience with AWS architecture design and implementation. Experience with Python scripting for data processing and analysis. Additional Information: The candidate should have a minimum of 2 years of experience in Tableau. The ideal candidate will possess a strong educational background in Data Science, Computer Science, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Pune office. Qualification 15 years of full time education",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, tableau, data modeling, mysql, aws, sosl, visualforce, soql, sfdc, machine learning, triggers, dashboards, javascript, computer assembling, apex, salesforce, salesforce crm, data loader, java, installation, html, apex classes, hardware troubleshooting",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,"Pune, Bengaluru","Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Databricks Unified Data Analytics Platform Good to have skills : Python (Programming Language), PySpark Minimum  3  year(s) of experience is required Educational Qualification : 15 years Education is must Summary :As a Data Platform Engineer, you will be responsible for assisting with the data platform blueprint and design, encompassing the relevant data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and working with Databricks Unified Data Analytics Platform and PySpark.  Roles & Responsibilities: Assist with the design and implementation of the data platform blueprint, encompassing the relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Implement and maintain data pipelines using Databricks Unified Data Analytics Platform and PySpark. Develop and maintain data models and data dictionaries. Ensure data quality and integrity through data validation and testing. Professional & Technical Skills: Must To Have Skills:Experience with Databricks Unified Data Analytics Platform. Must To Have Skills:Strong understanding of data modeling and data pipelines. Good To Have Skills:Proficiency in Python (Programming Language) and PySpark. Experience with data validation and testing. Experience with data dictionaries and metadata management. Additional Information: The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform. The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Pune office. Qualification 15 years Education is must",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, data analytics, pyspark, data modeling, software engineering, hive, metadata, data analysis, data management, metadata management, data dictionary, data warehousing, machine learning, sql, data quality, spark, data governance, hadoop, aws, etl, big data, unix",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Microsoft Azure Modern Data Platform, Databricks Unified Data Analytics Platform, Python (Programming Language) Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform, ensuring cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services, collaborating with Integration Architects and Data Architects, and utilizing your expertise in data platform components.  Roles & Responsibilities: Assist with the blueprint and design of the data platform, encompassing the relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Implement and maintain data platform components using Microsoft Azure Data Services. Develop and maintain data pipelines, ensuring data quality and integrity. Troubleshoot and resolve data platform issues, working with cross-functional teams as needed. Professional & Technical Skills: Must To Have Skills:Expertise in Microsoft Azure Data Services. Good To Have Skills:Experience with Microsoft Azure Modern Data Platform, Databricks Unified Data Analytics Platform, and Python (Programming Language). Strong understanding of data platform components and architecture. Experience with data pipeline development and maintenance. Ability to troubleshoot and resolve data platform issues. Solid grasp of data quality and integrity best practices. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of education",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, python, microsoft azure, data engineering, data quality, hive, data analytics, data warehousing, data architecture, machine learning, sql server, sql, nosql, amazon ec2, java, data modeling, spark, kafka, mysql, hadoop, sqoop, aws, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Python (Programming Language), Microsoft Azure Modern Data Platform, Databricks Unified Data Analytics Platform Minimum  5  year(s) of experience is required Educational Qualification : Minimum 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform, ensuring cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services, collaborating with Integration Architects and Data Architects, and utilizing your expertise in Python and Databricks Unified Data Analytics Platform.  Roles & Responsibilities: Assist with the blueprint and design of the data platform, encompassing the relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Implement and maintain data pipelines using Microsoft Azure Data Services. Develop and maintain data processing and transformation jobs using Databricks Unified Data Analytics Platform. Troubleshoot and optimize data pipelines and data processing jobs for performance and scalability. Professional & Technical Skills: Must To Have Skills:Expertise in Microsoft Azure Data Services. Good To Have Skills:Proficiency in Python, Microsoft Azure Modern Data Platform, and Databricks Unified Data Analytics Platform. Strong understanding of data modeling and database design principles. Experience with data integration and ETL processes. Familiarity with data warehousing and data lake concepts. Knowledge of data security and compliance best practices. Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of education",1.11E+11,11-05-2024,09-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, data services, information technology, microsoft azure, data modeling, hive, data analytics, data warehousing, data architecture, machine learning, sql, database design, data bricks, java, spark, design principles, hadoop, etl, aws, data integration, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Architecture Principles Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : Any Degree Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and implementing data architecture principles.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Implement data architecture principles. Develop and maintain data architecture artifacts, including data models, data flow diagrams, and data dictionaries. Professional & Technical Skills: Must To Have Skills:Strong understanding of data architecture principles. Good To Have Skills:Experience with data modeling tools such as ERwin or ER/Studio. Experience with data integration technologies such as ETL or ELT. Experience with data governance and data quality frameworks. Experience with cloud-based data platforms such as AWS or Azure. Additional Information: The candidate should have a minimum of 7.5 years of experience in Data Architecture Principles. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office.",2.70E+11,27-04-2024,26-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"information technology, data architecture, data architecture principles, etl, data integration, oracle, workday, microsoft azure, data warehousing, erwin, machine learning, er, sql server, sql, plsql, data quality, data modeling, data governance, aws, informatica",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Engineer I - Platform Engineer,"           As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers digital lives       Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems     American Express offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source     And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development     Find your place in technology of TeamAmex           American Express has embarked on an exciting Cloud, Big Data and Mobile transformation driven by an energetic new team of high performers     This group is nimble and creative with the power to shape our technology and product roadmap     If you have the talent and desire to deliver innovative payment products and services at a rapid pace, serving our customers seamlessly across physical, digital, mobile, and social media, join our transformation team     You will contribute to a rock star start-up engineering team which will deliver the next generation auto-scaled, Highly Available, hybrid cloud enterprise application platform (PaaS) for the mission critical American Express Delivery Transformation initiative           The platform will deliver a framework to support multiple programming languages ie Java, Node.js, Python, Ruby/Rails, Scala and Fuse for enterprise and consumer mobile & web application developers     You will create innovative tools and capabilities to enable continuous delivery - building, provisioning, and administering multi-tenant, scale-out applications throughout the life cycle, from development, test and into production     You will be challenged with identifying innovative ideas and proofs of concept to deliver against the existing and future needs of our customers     we will wear many hats, work fast and smart, and adapt and iterate quickly                  Responsibilities Include:                     This is an individual contributor role to manage and build Public, Private and Hybrid cloud infrastructure at in highly hydrogenous environment.                     Familiarity with API & integration patterns to securely communicate with backend services and clients.                  Uses technical acumen to provide alternatives, ask probing questions and support the team in technical issue resolution                  Coordinates communications, escalates, and facilitates resolution of risks, issues, and changes tied to the Infrastructure and Operations                      Qualifications:                      Overall 6+ years of industry experience with 4-5 Years of relevant experience                  development experience in a modern development stack (Java or Golang preferred) is essential.                  3+ years of experience working with public cloud services (preferably AWS or GCP) and a proven track record of building complex infrastructure.                  3+ years of Strong hands-on deployment and troubleshooting expertise experience working with private cloud (OpenShift, Kubernetes, Docker)                  Demonstrated professional expertise in a large Fortune 100 company running an IT Production Operation at scale including Infrastructure (Servers, Network, Storage, Security) and Applications (Distributed, Middleware, Databases across all tiers)                  Ability to work with Infrastructures and Platforms including IaaS, PaaS, Cloud technologies and tools for Continuous Delivery (CD)                  In depth understanding of Linux functionalities/ features as well as good experience of Linux system engineering.                  Good understanding of web technologies (http, headers, authentication, ssl, routers, load balancing, etc.)                      At least medium level expertise in one of following:                              Python/ shell scripting                     Golang and Rest API                         Desirable experience                                 Splunk                              ELK                              Node.js                              Java, JBoss, WAS                              Ansible                              Puppet                              Cloud management and administration                              Continuous delivery experience/DevOps/Agile                      Other skills                                 Proven diagnostic, troubleshooting, and service restoration skills.                              Excellent interpersonal skills, customer service skills, and English communication skills with difficult customers in outage situations.                              Metrics management                              Outstanding written and verbal communication skills                                  Benefits include:                 Competitive base salaries              Bonus incentives              Support for financial-well-being and retirement.              Comprehensive medical, dental, vision, life insurance, and disability benefits (depending on location)              Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need.              Generous paid parental leave policies (depending on your location)              Free access to global on-site wellness centers staffed with nurses and doctors (depending on location)              Free and confidential counseling support through our Healthy Minds program              Career development and training opportunities       ",2.70E+11,27-02-2024,27-05-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Financial Services,"Career development, Backend, Linux, Social media, Shell scripting, Agile, Issue resolution, HTTP, Troubleshooting, Open source",-,9am-6pm,"Full Time, Permanent",AMERICAN EXPRESS,Organization,AMERICAN EXPRESS,https://img.naukimg.com/logo_images/groups/v1/82242.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Salesforce Omnistudio Platform Good to have skills : Salesforce Lightning Web Components Minimum  3  year(s) of experience is required Educational Qualification : 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working with Salesforce Omnistudio Platform and Salesforce Lightning Web Components.  Roles & Responsibilities: Assist with the design and implementation of the data platform blueprint, encompassing relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Utilize Salesforce Omnistudio Platform to develop and maintain data integration solutions. Develop and maintain Salesforce Lightning Web Components to enhance user experience. Troubleshoot and resolve issues related to data integration and platform components. Professional & Technical Skills: Must To Have Skills:Experience with Salesforce Omnistudio Platform. Good To Have Skills:Experience with Salesforce Lightning Web Components. Strong understanding of data integration and data modeling concepts. Experience with troubleshooting and resolving issues related to data integration and platform components. Experience with Agile development methodologies. Additional Information: The candidate should have a minimum of 3 years of experience in Salesforce Omnistudio Platform. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification 15 years of education",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"salesforce lightning, salesforce, data modeling, troubleshooting, data integration, visualforce, css, sfdc, data warehousing, triggers, business intelligence, javascript, apex, sql, salesforce crm, sales force development, data loader, lwc, html, agile, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Modeling Techniques and Methodologies-Data Platform Engineer,"Project Role :Data Platform Engineer  Project Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills :Data Modeling Techniques and Methodologies  Good to have skills :Data Engineering,Cloud Data Migration Minimum 9 year(s) of experience is required  Educational Qualification :BE or BTech must Key Reponsibilities :1Drive discussions with clients deal teams to understand business requirements, how Industry Data Model fits in implementation and solutioning 2Develop the solution blueprint and scoping, estimation, staffing for delivery project and solutioning 3Drive Discovery activities and design workshops with client and lead strategic road mapping and operating model design discussions 4Good to have Data Vault,Cloud DB design,Graph data modeling, Ontology, Data Engineering,Data Lake design Technical Experience : 1 7plus year overall exp,3plus Data Modeling,Cloud DB Model,3NF,Dimensional,Conversion of RDBMS data model to Graph Data ModelInstrumental in DB design through all stages of Data Model 2 Exp on at least one Cloud DB Design work must be familiar with Data Architecture Principles Professional Attributes :1Strong requirement analysis and technical solutioning skill in Data and Analytics 2Excellent writing, communication and presentation skills 3Eagerness to learn and develop self on an ongoing basis 4Excellent client facing and interpersonal skillsExp in estimation,PoVs,Solution Approach creation Exp on data transformation,analytic projects,DWH Qualification NA",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,108,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"presentation skills, data engineering, data architecture principles, data modeling, cloud data migration, snowflake, python, oracle, data warehousing, microsoft azure, machine learning, sql, plsql, data transformation, aws, etl, informatica, db, data lake, unix",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Data Architecture Principles Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : Any Degree Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and implementing data architecture principles.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Implement data architecture principles. Develop and maintain data architecture artifacts, including data models, data flow diagrams, and data dictionaries. Professional & Technical Skills: Must To Have Skills:Strong understanding of data architecture principles. Good To Have Skills:Experience with data modeling tools such as ERwin or ER/Studio. Experience with data integration technologies such as ETL or ELT. Experience with data governance and data quality frameworks. Experience with cloud-based data platforms such as AWS or Azure. Additional Information: The candidate should have a minimum of 7.5 years of experience in Data Architecture Principles. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office. Qualification Any Degree",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"information technology, data architecture, elt, er, data architecture principles, python, oracle, microsoft azure, data warehousing, erwin, machine learning, sql server, sql, plsql, data quality, data modeling, data governance, aws, etl, data integration, informatica",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Red Hat OpenShift Good to have skills : Red Hat 3Scale API Management Minimum  5  year(s) of experience is required Educational Qualification : RedHat OCP Admin Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance using Red Hat OpenShift and Red Hat 3Scale API Management.  Roles & Responsibilities: Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure using Red Hat OpenShift. Deploy infrastructure and platform environments, create a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure successful delivery of cloud application solutions. Provide technical support and troubleshooting for cloud application solutions. Professional & Technical Skills: Must To Have Skills:Experience in Red Hat OpenShift. Good To Have Skills:Experience in Red Hat 3Scale API Management. Experience in designing, building, testing, and deploying cloud application solutions. Experience in deploying infrastructure and platform environments. Strong understanding of cloud architecture, security, and performance. Experience in providing technical support and troubleshooting for cloud application solutions. Additional Information: The candidate should have a minimum of 5 years of experience in Red Hat OpenShift. The ideal candidate will possess a strong educational background in software engineering or a related field, along with a proven track record of delivering impactful cloud application solutions. This position is based at our Bengaluru office. Qualification RedHat OCP Admin",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"redhat openshift, redhat linux, api management, cloud architecture, software engineering, kubernetes, python, openshift, microsoft azure, cloud platform, docker, ansible, red, java, git, linux, microsoft windows, jenkins, troubleshooting, cloud infrastructure, hat, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Data Modeling Techniques and Methodologies Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : A Engineering graduate preferably Computer Science graduate 15 years of full time education Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance.  Roles & Responsibilities: Overall 5+ years of experience working in DWH, Data Analytics projects Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure. Deploy infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure successful delivery of cloud-based solutions. Stay updated with the latest advancements in cloud technologies and integrate innovative approaches for sustained competitive advantage. MUST have done fresh data modeling for 2 or more projects Have strong understanding of Datawarehouse concept Well versed with Data modeling tool like Erwin Experience of Rationalization of data model to move to Self service mode Good to have Cloud (Azure) exposure or experience Professional & Technical Skills: Must To Have Skills:Strong experience in Data Modeling Techniques and Methodologies. Good To Have Skills:Experience in cloud technologies such as AWS, Azure, or Google Cloud Platform. Experience in deploying infrastructure and platform environments. Experience in creating a proof of architecture to test architecture viability, security, and performance. Experience in collaborating with cross-functional teams to ensure successful delivery of cloud-based solutions. Additional Information: The candidate should have a minimum of 5 years of experience in Data Modeling Techniques and Methodologies. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful cloud-based solutions. This position is based at our Bengaluru office. Qualification A Engineering graduate preferably Computer Science graduate 15 years of full time education",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data warehousing, data modeling, gcp, data analytics, aws, kubernetes, networking, erwin, docker, ansible, git, java, devops, linux, paas, jenkins, shell scripting, mysql, cloud computing, python, maven, vmware, microsoft azure, cloud platform, saas, cloud infrastructure",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
VIS & BI Strategy Practitioner,"Project Role : VIS & BI Strategy Practitioner Project Role Description : Utilizes data science methodology to inform the design research process. Employs statistical and quantitative data analysis to enhance solution development.  Must have skills : Data Science Good to have skills : NA Minimum  10+  year(s) of experience is required Educational Qualification : std Accenture requirements Project Role :Data Designer Project Role Description :Utilizes data science methodology to inform the design research process. Employs statistical and quantitative data analysis to enhance solution development.  Must have Skills :Data Science, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :A:Having broad technology background, with focus on developing software applications in Python for production ready applications B:Writing container-based code and testing code, debugging programs and integrating applications with third-party web services, deployment Work with cross functional team to scale the architecture supporting C:AI models Support data needs for model training/ensemble the models Architecture, security reviews, NFRs Experience in Agile, DevOps, driving best practices Technical Experience :A:Using APIs in python Flask, Falcon or Django, Kafka ML and python libraries as scikit-learn, numpy and pandas B:Machine learning, Deep learning, Image Processing Familiarity with Apache SOLR, MongoDB C:Familiarity with Celery, Rabbit MQ for background task queues Data pre-processing techniques GIT, Docker, Jenkins, Swagger, Cloud AWS/Azure Professional Attributes :Good Communication skills, Good in Analytics and collaboration Educational Qualification:std Accenture requirementsAdditional Info :",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, data science, devops, mongodb, machine learning, scikit-learn, falcon, numpy, swagger, research, docker, deep learning, git, celery, apache, java, aws cloud, solr, jenkins, mysql, ml, image processing, python, microsoft azure, rabbitmq, pandas, django, kafka, agile, flask",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Full Stack Engineer,"Project Role : Full Stack Engineer Project Role Description : Responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. Use development skills to deliver innovative solutions that help our clients improve the services they provide. Leverage new technologies that can be applied to solve challenging business problems with a cloud first and agile mindset.  Must have skills : Machine Learning Good to have skills : Cloud Infrastructure, Gen AI Minimum  5  year(s) of experience is required Educational Qualification : 15 Years of full time education  Machine Learning Engineer Job DescriptionA Machine Learning Engineer is a professional who is skilled and specializes in designing and developing machine learning systems, possess expertise in statistics, programming, and data science. The role involves creating efficient self-learning applications and contributing to advancements in artificial intelligence. Qualifications: Strong programming skills (Python, R, or similar languages). Knowledge of data science concepts, including feature engineering, model evaluation, and validation. Expertise in statistics and understanding of machine learning algorithms. Familiarity with deep learning frameworks (e.g., TensorFlow, PyTorch). Experience with cloud platforms (AWS, Google Cloud, Azure) for model deployment. Ability to work collaboratively in cross-functional teams.Responsibilities:?Study and Transform Data Science Prototypes:Work with data to create models, perform statistical analysis, and train and retrain systems to optimize performance.Explore and preprocess data, ensuring it's suitable for machine learning tasks.Design Machine Learning Systems:Design and construct sophisticated machine learning models.Choose appropriate algorithms and architectures based on the problem domain and data characteristics.?Research and Implement ML Algorithms and Tools:Staying updated with the latest developments in the field, they explore new algorithms and tools.Implement machine learning algorithms, adapting them to specific use cases.Develop Machine Learning Applications:Build applications that leverage machine learning models.Collaborate with software engineers to integrate ML components into larger systems.Deploy and Maintain Machine Learning ModelsEnsure scalability and reliability, they deploy models in cloud platforms such as AWS.Regular performance evaluations, identifying bottlenecks, and implementing optimization strategies.Passionate about pushing the boundaries of AI technology and creating innovative self-learning applications, consider joining our team as a Machine Learning Engineer! Qualification 15 Years of full time education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data science, pytorch, python, machine learning, statistics, algorithms, c++, css, docker, ansible, gen, tensorflow, java, git, gcp, devops, linux, jenkins, data structures, html, mysql, mongodb, microsoft azure, javascript, r, node.js, full stack, cloud infrastructure, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Full Stack Engineer,"Project Role : Full Stack Engineer Project Role Description : Responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. Use development skills to deliver innovative solutions that help our clients improve the services they provide. Leverage new technologies that can be applied to solve challenging business problems with a cloud first and agile mindset.  Must have skills : Machine Learning Good to have skills : Cloud Infrastructure, Gen AI Minimum  3  year(s) of experience is required Educational Qualification : 15 Years of full time education Machine Learning Engineer Job DescriptionA Machine Learning Engineer is a professional who is skilled and specializes in designing and developing machine learning systems, possess expertise in statistics, programming, and data science. The role involves creating efficient self-learning applications and contributing to advancements in artificial intelligence. Qualifications: Strong programming skills (Python, R, or similar languages). Knowledge of data science concepts, including feature engineering, model evaluation, and validation. Expertise in statistics and understanding of machine learning algorithms. Familiarity with deep learning frameworks (e.g., TensorFlow, PyTorch). Experience with cloud platforms (AWS, Google Cloud, Azure) for model deployment. Ability to work collaboratively in cross-functional teams.Responsibilities:?Study and Transform Data Science Prototypes:Work with data to create models, perform statistical analysis, and train and retrain systems to optimize performance.Explore and preprocess data, ensuring it's suitable for machine learning tasks.Design Machine Learning Systems:Design and construct sophisticated machine learning models.Choose appropriate algorithms and architectures based on the problem domain and data characteristics.?Research and Implement ML Algorithms and Tools:Staying updated with the latest developments in the field, they explore new algorithms and tools.Implement machine learning algorithms, adapting them to specific use cases.Develop Machine Learning Applications:Build applications that leverage machine learning models.Collaborate with software engineers to integrate ML components into larger systems.Deploy and Maintain Machine Learning ModelsEnsure scalability and reliability, they deploy models in cloud platforms such as AWS.Regular performance evaluations, identifying bottlenecks, and implementing optimization strategies.Passionate about pushing the boundaries of AI technology and creating innovative self-learning applications, consider joining our team as a Machine Learning Engineer! Qualification 15 Years of full time education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, machine learning, r, cloud infrastructure, statistics, algorithms, c++, css, docker, ansible, gen, tensorflow, java, git, data science, gcp, devops, linux, pytorch, jenkins, data structures, html, mysql, mongodb, microsoft azure, javascript, node.js, full stack, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Full Stack Engineer,"Project Role : Full Stack Engineer Project Role Description : Responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. Use development skills to deliver innovative solutions that help our clients improve the services they provide. Leverage new technologies that can be applied to solve challenging business problems with a cloud first and agile mindset.  Must have skills : Machine Learning Good to have skills : Gen AI Minimum  5  year(s) of experience is required Educational Qualification : 15 Years of full time education  Machine Learning Engineer Job DescriptionA Machine Learning Engineer is a professional who is skilled and specializes in designing and developing machine learning systems, possess expertise in statistics, programming, and data science. The role involves creating efficient self-learning applications and contributing to advancements in artificial intelligence. Qualifications: Strong programming skills (Python, R, or similar languages). Knowledge of data science concepts, including feature engineering, model evaluation, and validation. Expertise in statistics and understanding of machine learning algorithms. Familiarity with deep learning frameworks (e.g., TensorFlow, PyTorch). Experience with cloud platforms (AWS, Google Cloud, Azure) for model deployment. Ability to work collaboratively in cross-functional teams.Responsibilities:?Study and Transform Data Science Prototypes:Work with data to create models, perform statistical analysis, and train and retrain systems to optimize performance.Explore and preprocess data, ensuring it's suitable for machine learning tasks.Design Machine Learning Systems:Design and construct sophisticated machine learning models.Choose appropriate algorithms and architectures based on the problem domain and data characteristics.?Research and Implement ML Algorithms and Tools:Staying updated with the latest developments in the field, they explore new algorithms and tools.Implement machine learning algorithms, adapting them to specific use cases.Develop Machine Learning Applications:Build applications that leverage machine learning models.Collaborate with software engineers to integrate ML components into larger systems.Deploy and Maintain Machine Learning ModelsEnsure scalability and reliability, they deploy models in cloud platforms such as AWS.Regular performance evaluations, identifying bottlenecks, and implementing optimization strategies.Passionate about pushing the boundaries of AI technology and creating innovative self-learning applications, consider joining our team as a Machine Learning Engineer! Qualification 15 Years of full time education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"tensorflow, data science, python, machine learning, statistics, algorithms, c++, css, docker, spring, gen, java, git, postgresql, gcp, linux, pytorch, data structures, html, mysql, mongodb, microsoft azure, javascript, sql server, angular, r, node.js, django, full stack, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Data Modeling Techniques and Methodologies Good to have skills : Extract Transform & Load Minimum  5  year(s) of experience is required Educational Qualification : A Engineering graduate preferably Computer Science graduate 15 years of full time education Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance.  Roles & Responsibilities: Overall 5+ years of experience working in DWH, Data Analytics projects Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure. Deploy infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure successful delivery of cloud-based solutions. Stay updated with the latest advancements in cloud technologies and integrate innovative approaches for sustained competitive advantage. MUST have done fresh data modeling for 2 or more projects Have strong understanding of Datawarehouse concept Well versed with Data modeling tool like Erwin Experience of Rationalization of data model to move to Self service mode Good to have Cloud (Azure) exposure or experience Professional & Technical Skills: Must To Have Skills:Strong experience in Data Modeling Techniques and Methodologies. Good To Have Skills:Experience in cloud technologies such as AWS, Azure, or Google Cloud Platform. Experience in deploying infrastructure and platform environments. Experience in creating a proof of architecture to test architecture viability, security, and performance. Experience in collaborating with cross-functional teams to ensure successful delivery of cloud-based solutions. Additional Information: The candidate should have a minimum of 5 years of experience in Data Modeling Techniques and Methodologies. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful cloud-based solutions. This position is based at our Bengaluru office. Qualification A Engineering graduate preferably Computer Science graduate 15 years of full time education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data warehousing, data modeling, gcp, data analytics, aws, kubernetes, networking, erwin, docker, ansible, git, java, devops, linux, paas, jenkins, shell scripting, mysql, cloud computing, python, maven, vmware, microsoft azure, cloud platform, saas, cloud infrastructure",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Data Modeling Techniques and Methodologies Good to have skills : .NET Programming Minimum  5  year(s) of experience is required Educational Qualification : A Engineering graduate preferably Computer Science graduate 15 years of full time education Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance.  Roles & Responsibilities: Overall 5+ years of experience working in DWH, Data Analytics projects Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure. Deploy infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure successful delivery of cloud-based solutions. Stay updated with the latest advancements in cloud technologies and integrate innovative approaches for sustained competitive advantage. MUST have done fresh data modeling for 2 or more projects Have strong understanding of Datawarehouse concept Well versed with Data modeling tool like Erwin Experience of Rationalization of data model to move to Self service mode Good to have Cloud (Azure) exposure or experience Professional & Technical Skills: Must To Have Skills:Strong experience in Data Modeling Techniques and Methodologies. Good To Have Skills:Experience in cloud technologies such as AWS, Azure, or Google Cloud Platform. Experience in deploying infrastructure and platform environments. Experience in creating a proof of architecture to test architecture viability, security, and performance. Experience in collaborating with cross-functional teams to ensure successful delivery of cloud-based solutions. Additional Information: The candidate should have a minimum of 5 years of experience in Data Modeling Techniques and Methodologies. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful cloud-based solutions. This position is based at our Bengaluru office. Qualification A Engineering graduate preferably Computer Science graduate 15 years of full time education",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data analytics, data warehousing, data modeling, net programming, aws, c#, kubernetes, asp.net ajax, python, software development, microsoft azure, erwin, cloud platform, sql server, jquery, gcp, linux, asp.net, jenkins, cloud infrastructure, .net, html, mvc, asp",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Accenture Delivery Architectures (ADA)-AI Platform Engineer,"Project Role :AI Platform Engineer  Project Role Description :Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing. Must have skills :Accenture Delivery Architectures (ADA)  Good to have skills :No Technology Specialization Minimum 10+ year(s) of experience is required  Educational Qualification : Key Reponsibilities :1 Lead architecture, design, cross business area projects and development teams for AI applications2 Identify and explore the framework, NLP tools, VA platform, architecture, or tech-stack can be used to create a solution and the connection between the components Technical Experience :1 Knowledge of applications using MEAN stack, Angular JS, Nodejs, Python2 Understanding of UI technologies like HTML, JavaScript, jQuery3 Hands-on experiences on VA tools like Microsoft Bot Framework, Google Dialog flow, RASA, Amazon Lex at least 24 Knowledge of:Big Data, LAMP Linux, Apache, MySQL, PHP, DevOps, PaaS a:Custom Frameworks, Spring related technologies, UML, OO Design, Spring Boot, Microservice and JEE Architecture b:Cloud AWS/Azure/Google c:Artificial intelligence / Machine Professional Attributes :Good team handling good verbal and written communication. Qualification NA",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"mean stack, ui technologies, artificial intelligence, paas, mysql, rasa, amazon lex, jquery, microservices, spring, apache, aws cloud, uml, devops, linux, html, big data, microsoft bot framework, microsoft azure, google, lamp, javascript, spring boot, angular, node.js, php",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : PySpark Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Graduate Project Role :Data Platform Engineer Project Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have Skills :PySpark, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :Work on client projects to deliver AWS, PySpark, Databricks based Data engineering Analytics solutions Build and operate very large data warehouses or data lakes ETL optimization, designing, coding, tuning big data processes using Apache Spark Build data pipelines applications to stream and process datasets at low latencies Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data Technical Experience :Minimum of 1 years of experience in Databricks engineering solutions on AWS Cloud platforms using PySpark Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture delivery Minimum 2 year of Experience in one or more programming languages Python, Java, Scala Experience using airflow for the data pipelines in min 1 project 1 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform Professional Attributes :-Ready to work in B Shift 12 PM 10 PM 2-A Client facing skills:solid experience working in client facing environments, to be able to build trusted relationships with client stakeholders 3-Good critical thinking and problem-solving abilities 4-Health care knowledge Good Communication Skills Educational Qualification:GraduateAdditional Info :Level and Across Accenture Location Facilities",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, pyspark, aws cloud microservices, etl, big data, kubernetes, python, scala, data warehousing, data pipeline, warehouse, data engineering, docker, data bricks, java, git, spark, aws cloud, jenkins, shell scripting, terraform, hadoop, ci cd pipeline, data lake",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AI Platform Engineer,"Project Role : AI Platform Engineer Project Role Description : Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing.  Must have skills : Machine Learning Good to have skills : NA Minimum  12  year(s) of experience is required Educational Qualification : BE Summary :Stakeholder management Creation, tracking and reporting of project status by working closely with customer Use the different statistical/ modelling techniques to increase and optimize the business outcomes.  Roles & Responsibilities: Strong understanding of Python data types and Object-oriented programming. Understanding of the Machine Learning algorithms. (Supervised, Semi-supervised and unsupervised) Understanding different integrations touch points in ML delivery. Should anchor Pilot/MVP/Production Machine learning use case delivery for deployments on cloud as well as open source. Familiar with computer vision and NLP. Mine and analyze the data using state of the arts methods. Develop and implement custom data models and algorithms to apply to datasets. Professional & Technical Skills: Collaborative mindset with strong verbal and written communication skills. Experience to work in Agile and DevOps processes. Open to learn newer technologies. Additional Information: The ideal candidate will possess a strong educational background in computer science, mathematics, or a related field, along with a proven track record of delivering impactful AI-driven solutions.",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,144,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, machine learning, python data, devops, agile, continuous integration, c++, windows xp, jsp, operating systems, jdbc, bootstrap, javascript, sql server, jquery, sql, open source, java, microsoft windows, windows server, .net, mysql, azure data explorer, perl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Full Stack Engineer,"Design and develop scalable full-stack applications utilizing AWS services including AWS Lambda, SQS, Python, Node etc. Role is primarily 50% to 80% back-end and 50% to 20% front-end development Optimize application performance and scalability Required Candidate profile 5+ years with AWS services(Lambda, SNS, SQS, EventBridge, etc)  Experience in Python and React Exp in back-end technologies Node.js, Lambda, ECS, API Gateway Hands on AI /ML experience",2.11E+11,12-05-2024,10-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, Machine Learning, Microservices, Snowflake Sql, Aws Lambda, C#, Pyspark, Aws Cloudformation, Api Gateway, Power Bi, Amazon Kinesis, EMR, Aws Glue, SSIS, Dynamo Db, Rest Api Development, asp.net, Azure Data Lake, Sns, React.Js, Athena, AWS",-,9am-6pm,"Full Time, Permanent",Vichara Technologies,Organization,Vichara Technologies,https://img.naukimg.com/logo_images/groups/v1/244948.gif,"Kochi, Hyderabad, Delhi / NCR","Kochi, Hyderabad, Delhi / NCR",-,-,-,25-30 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Science-VIS & BI Strategy Practitioner,"Project Role :VIS & BI Strategy Practitioner  Project Role Description :Develop a VIS/BI vision, business case, and modernization strategy; discover and assess VIS/BI opportunities; identify industry- and function-centered VIS/BI use cases; guide clients on industry and function relevant measures and metrics; develop a VIS/BI roadmap and operating model; help drive adoption, decision making and behavior change; measure VIS/BI value. Must have skills :Data Science  Good to have skills :NA Minimum 10+ year(s) of experience is required  Educational Qualification :std Accenture requirements Key Reponsibilities :Having broad technology background, with focus on developing software applications in Python for production ready applications Writing container-based code and testing code, debugging programs and integrating applications with third-party web services, deployment Work with cross functional team to scale the architecture supporting AI models Support data needs for model training/ensemble the models Architecture, security reviews, NFRs Experience in Agile, DevOps, driving best practices Technical Experience : Python Dev, 8 years experience Min 6 years experience Using APIs in python Flask, Falcon or Django, Kafka ML and python libraries as scikit-learn, numpy and pandas Machine learning, Deep learning, Image Processing Familiarity with Apache SOLR, MongoDB Familiarity with Celery, Rabbit MQ for background task queues Data pre-processing techniques GIT, Docker, Jenkins, Swagger, Cloud AWS/Azure Professional Attributes :Good Communication skills, Good in Analytics and collaboration",1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"falcon, python flask, data science, api, django, scikit-learn, numpy, swagger, docker, sql, deep learning, celery, git, apache, aws cloud, devops, solr, jenkins, mongodb, ml, image processing, python, microsoft azure, machine learning, rabbitmq, pandas, kafka, agile, flask",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Modern Data Platform Good to have skills : PySpark Minimum  7.5  year(s) of experience is required Educational Qualification : A:15 Years of full time education Project Role :Data Platform Engineer Project Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have Skills :Microsoft Azure Modern Data Platform, SSI: NON SSI:Good to Have Skills :SSI:PySpark NON SSI :Job Requirements :Key Responsibilities :1 Show a strong development skill in Pyspark and Databrick to build complex data pipelines 2 Should be able to deliver the development task assigned independently 3 Should be able to participate in daily status calls and have good communication skills to manage day to day work Technical Experience :1 Should have more than 5 years of experience in IT 2 Should have more than 2 years of experience in technologies like Pyspark and Databricks 3 Should be able to build end to end pipelines using Pyspark with good knowledge on Delta Lake 4 Should have good knowledge on Azure services like Azure Data Factory, Azure storage solutions like ADLS, Delta Lake, Azure AD Professional Attributes :A:Good Communication skills and other soft Skills Educational Qualification:A:15 Years of full time educationAdditional Info : Qualification A:15 Years of full time education",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, pyspark, microsoft azure, data bricks, azure active directory, hive, python, azure data lake, data warehousing, azure data factory, data pipeline, machine learning, sql server, sql, java, data modeling, spark, kafka, mysql, hadoop, sqoop, aws, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
VIS & BI Strategy Practitioner,"Project Role : VIS & BI Strategy Practitioner Project Role Description : Develop a VIS/BI vision, business case, and modernization strategy; discover and assess VIS/BI opportunities; identify industry- and function-centered VIS/BI use cases; guide clients on industry and function relevant measures and metrics; develop a VIS/BI roadmap and operating model; help drive adoption, decision making and behavior change; measure VIS/BI value.  Must have skills : Machine Learning Good to have skills : CSS3 Minimum  7.5  year(s) of experience is required Educational Qualification : BE Summary :You will be responsible for utilizing data science methodology to inform the design research process. Your typical day will involve employing statistical and quantitative data analysis to enhance solution development.  Roles & Responsibilities: Having broad technology background, with focus on developing software applications in Python for production ready applications Writing container-based code and testing code, debugging programs and integrating applications with third-party web services, deployment Work with cross functional team to scale the architecture supporting AI models Support data needs for model training/ensemble the models Architecture, security reviews, NFRs Experience in Agile, DevOps, driving best practices Lead the development and deployment of advanced machine learning models using Python and associated libraries like Pandas, NumPy, and Scikit-learn. Conduct detailed analysis of complex data sets, employing statistical methodologies and data munging techniques for actionable insights, alongside a robust understanding of statistical analysis and machine learning algorithms. Collaborate with cross-functional teams, applying expertise in diverse machine learning algorithms, including experience in implementing various algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms. Communicate technical findings effectively to stakeholders, utilizing data visualization tools like Tableau or Power BI for clarity. Stay updated with the latest advancements in machine learning and data science, integrating innovative approaches for sustained competitive advantage, including experience with TensorFlow, Natural Language Processing (NLP), and Big Data technologies. Professional & Technical Skills: Using APIs in python Flask, Falcon or Django, Kafka ML and python libraries as scikit-learn, numpy and pandas.  Machine learning, Deep learning, Image Processing Familiarity with Apache SOLR, MongoDB Familiarity with Celery, Rabbit MQ for background task queues Data pre-processing techniques GIT, Docker, Jenkins, Swagger, Cloud AWS/Azure Must To Have Skills:Proficiency in Machine Learning. Good To Have Skills:Experience with TensorFlow, Natural Language Processing (NLP), and Big Data technologies. Strong understanding of statistical analysis and machine learning algorithms. Experience with data visualization tools such as Tableau or Power BI. Experience in implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms. Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.Additional Information: The candidate should have a minimum of 7.5 years of experience in Machine Learning. The ideal candidate will possess a strong educational background in statistics, mathematics, computer science, or a related field, along with a proven track record of delivering impactful data-driven solutions. Qualification BE",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"numpy, machine learning, machine learning algorithms, statistics, data munging, python, css, natural language processing, scikit-learn, big data technologies, power bi, rabbitmq, pandas, deep learning, tableau, tensorflow, git, aws cloud, devops, agile, api, mongodb",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AI Platform Engineer,"Project Role : AI Platform Engineer Project Role Description : Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing.  Must have skills : Natural Language Processing (NLP) Good to have skills : NA Minimum  12  year(s) of experience is required Educational Qualification : BE Summary :As an AI Platform Engineer, you will be responsible for developing applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, and natural language processing. Your typical day will involve working with NLP, developing and deploying AI models, and collaborating with cross-functional teams.  Roles & Responsibilities:-Build predictive models, develop advanced algorithms that extract and classify information from large datasets quantify model performance  Evaluate emerging technologies that may contribute to our analytical platform Identify and exploit new patterns in data using various techniques Worked with ML data mining toolkits like NLP, Semantic Web, R, Core NLP, NLTK etc. Information retrieval libraries like Lucene, SOLR fast paced, test driven, collaborative and iterative programming environment. Professional & Technical Skills: Work with developers to integrate front end with backend, knowledge in Insurance, Banking and Capital markets domains Strong coding ability in Python, R programming. Should know about crunching billions of datapoints for statistical modeling, data mining for insights and recommendation solutions. Additional Information: The ideal candidate will possess a strong educational background in computer science, mathematics, or a related field, along with a proven track record of delivering impactful AI-driven solutions.",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,144,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"nltk, coding, python, natural language processing, r, algorithms, scikit-learn, numpy, artificial intelligence, docker, sql, deep learning, tensorflow, spacy, java, git, data science, linux, keras, mysql, data structures, hadoop, machine learning, pandas, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
VIS & BI Strategy Practitioner,"Project Role : VIS & BI Strategy Practitioner Project Role Description : Develop a VIS/BI vision, business case, and modernization strategy; discover and assess VIS/BI opportunities; identify industry- and function-centered VIS/BI use cases; guide clients on industry and function relevant measures and metrics; develop a VIS/BI roadmap and operating model; help drive adoption, decision making and behavior change; measure VIS/BI value.  Must have skills : Machine Learning Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : BE Summary :You will be responsible for utilizing data science methodology to inform the design research process. Your typical day will involve employing statistical and quantitative data analysis to enhance solution development.  Roles & Responsibilities: Having broad technology background, with focus on developing software applications in Python for production ready applications Writing container-based code and testing code, debugging programs and integrating applications with third-party web services, deployment Work with cross functional team to scale the architecture supporting AI models Support data needs for model training/ensemble the models Architecture, security reviews, NFRs Experience in Agile, DevOps, driving best practices Lead the development and deployment of advanced machine learning models using Python and associated libraries like Pandas, NumPy, and Scikit-learn. Conduct detailed analysis of complex data sets, employing statistical methodologies and data munging techniques for actionable insights, alongside a robust understanding of statistical analysis and machine learning algorithms. Collaborate with cross-functional teams, applying expertise in diverse machine learning algorithms, including experience in implementing various algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms. Communicate technical findings effectively to stakeholders, utilizing data visualization tools like Tableau or Power BI for clarity. Stay updated with the latest advancements in machine learning and data science, integrating innovative approaches for sustained competitive advantage, including experience with TensorFlow, Natural Language Processing (NLP), and Big Data technologies. Professional & Technical Skills: Using APIs in python Flask, Falcon or Django, Kafka ML and python libraries as scikit-learn, numpy and pandas.  Machine learning, Deep learning, Image Processing Familiarity with Apache SOLR, MongoDB Familiarity with Celery, Rabbit MQ for background task queues Data pre-processing techniques GIT, Docker, Jenkins, Swagger, Cloud AWS/Azure Must To Have Skills:Proficiency in Machine Learning. Good To Have Skills:Experience with TensorFlow, Natural Language Processing (NLP), and Big Data technologies. Strong understanding of statistical analysis and machine learning algorithms. Experience with data visualization tools such as Tableau or Power BI. Experience in implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms. Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.Additional Information: The candidate should have a minimum of 7.5 years of experience in Machine Learning. The ideal candidate will possess a strong educational background in statistics, mathematics, computer science, or a related field, along with a proven track record of delivering impactful data-driven solutions. Qualification BE",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"numpy, machine learning, machine learning algorithms, statistics, data munging, image processing, python, natural language processing, scikit-learn, falcon, big data technologies, power bi, pandas, deep learning, tableau, tensorflow, celery, devops, jenkins, agile",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Role & responsibilities   Summary: As a Data Platform Engineer, you will be responsible for assisting with the data platform blueprint and design, encompassing the relevant data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and utilizing your expertise in MongoDB, Database Management, Grafana, SQL Query, and Non SQL. Roles & Responsibilities: - Assist with the design and implementation of data platform components, utilizing MongoDB, Database Management, Grafana, SQL Query, and Non SQL. - Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. - Develop and maintain data platform infrastructure, ensuring high availability and scalability. - Implement and maintain data security protocols and procedures, ensuring data privacy and integrity. - Troubleshoot and resolve data platform issues, utilizing your expertise in MongoDB, Database Management, Grafana, SQL Query, and Non SQL. Professional & Technical Skills: - Must To Have Skills: Expertise in MongoDB, Database Management, Grafana, SQL Query, and Non SQL. - Good To Have Skills: Experience with AWS CloudFormation. - Strong understanding of data platform design and implementation. - Experience with data security protocols and procedures. - Experience with troubleshooting and resolving data platform issues. Additional Information: - The candidate should have a minimum of 5 years of experience in MongoDB and Database Management. - The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.",1.60E+11,16-04-2024,15-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Database Management, MongoDB, Grafana, NoSQL, SQL Queries",-,9am-6pm,"Full Time, Temporary/Contractual",Adore Technosoft Opc,Organization,Adore Technosoft Opc,https://www.naukri.com/hotjobs/images/v3/Adore_Jan24.gif,Bengaluru,Bengaluru,-,-,-,"70,000-1 Lacs P.A ",,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
DataStage Professional,"Responsibilities Datastage developer with good SQL knowledge.Good understanding of data warehouse concepts and data modellingHigh proficiency in ETL processes and SQL Experience in low and high level designs and providing efficient solutions to complex requirementsStrong experience in sourcing data from different types of databases (SQL, DB2, Cloud) to assist in building effective ETL flowsGood to have:Exp. on other ETL toolsAirFlow, GitHub, CI/CD pipelines Preferred Skills: ETL->Datastage  Educational Requirements Bachelor of Engineering  Service Line Data & Analytics Unit *  Location of posting is subject to business requirements",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"DataStage, DB2, GitHub, Cloud, CI/CD, ETL, SQL",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://www.naukri.com/hotjobs/images/v3/infosys_nov13.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AI Platform Engineer,"Project Role : AI Platform Engineer Project Role Description : Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing.  Must have skills : Natural Language Processing (NLP) Good to have skills : NA Minimum  12  year(s) of experience is required Educational Qualification : BE Summary :As an AI Platform Engineer, you will be responsible for developing applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, and natural language processing. Your typical day will involve working with NLP, developing and deploying AI models, and collaborating with cross-functional teams.  Roles & Responsibilities:-Build predictive models, develop advanced algorithms that extract and classify information from large datasets quantify model performance  Evaluate emerging technologies that may contribute to our analytical platform Identify and exploit new patterns in data using various techniques Worked with ML data mining toolkits like NLP, Semantic Web, R, Core NLP, NLTK etc. Information retrieval libraries like Lucene, SOLR fast paced, test driven, collaborative and iterative programming environment. Professional & Technical Skills: Work with developers to integrate front end with backend, knowledge in Insurance, Banking and Capital markets domains Strong coding ability in Python, R programming. Should know about crunching billions of datapoints for statistical modeling, data mining for insights and recommendation solutions. Additional Information: The ideal candidate will possess a strong educational background in computer science, mathematics, or a related field, along with a proven track record of delivering impactful AI-driven solutions. Qualification BE",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,144,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"nltk, coding, python, natural language processing, r, algorithms, scikit-learn, numpy, artificial intelligence, docker, sql, deep learning, tensorflow, spacy, java, git, data science, linux, keras, mysql, data structures, hadoop, machine learning, pandas, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Principal Engineer - Backend,"   As a Principal Software Development Engineer, you will be playing a pivotal role in enabling Zeta to deliver to its ambitions. You will drive initiatives that will create a lasting impact on millions of individuals across the globe. You will work with an amazing peer group that fuels this ambition.        ?     What would you do here?        Designing and building large components or multiple services     Ensure high quality of architecture and design of systems     Create the optimum technical solution considering all the non-functional requirements     Functionally decompose complex problems into simple, straight-forward solutions     Articulate precisely both technical and business requirements by engaging with architects, data scientists, businesses and product managers     Mentor team members through technical discussions, design and ideation through white-boarding     Help managers arrive at a growth plan for the team members     Contribute strategically by working with tech leaders to maximize the productivity of teams by instilling an effective development environment     Evaluate the technical needs and select appropriate software, hardware, scalability and security requirement and suggest integration methods     Perform code and design reviews         What are we looking for?        8+ years of experience building microservices     Experience in Object-oriented design and programming     Strong experience in architecting and building distributed systems     Strong knowledge of data structures, algorithms, and designing for performance     Strong knowledge of cloud technologies like AWS/Google Cloud/Azure     Proficient with RDBMS     Strong knowledge on data stores, database design, data modelling and SQL queries     Strong knowledge of one or more big data processing stacks     Excellent code quality     Experience working on one or more large scale Java applications / platforms     Knowledge of Cryptography and Network Security     You have studied distributed systems like Dynamo, HBase, various messaging and queuing systems and understand nuances of Time, Clocks, and Ordering of Events, rate control, load distribution     You can smell fraud, transaction risks and abuse a mile away     ",1.50E+11,15-03-2024,13-06-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Object oriented design, Backend, RDBMS, Database design, Network security, Back office, Data structures, Distribution system, Core banking, HBase",-,9am-6pm,"Full Time, Permanent",Zeta Inc.,Organization,Zeta Inc.,https://img.naukimg.com/logo_images/groups/v1/481500.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer: Data Platforms,"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe. You'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience. Your Role and Responsibilities As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs. Your primary responsibilities include: Design, build, optimize and support new and existing data models and ETL processes based on our clients business requirements. Build, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization. Coordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too. Required Technical and Professional Expertise Must have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python Hbase, Hive Good to have Aws -S3,  athena ,Dynomo DB, Lambda, Jenkins GIT Developed Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).  Developed Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations..  Preferred Technical and Professional Expertise Understanding of Devops. Experience in building scalable end-to-end data ingestion and processing solutions  Experience with object-oriented and/or functional programming languages, such as Python, Java and Scala""",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"scala, spark, hadoop, big data, python, hive, pyspark, hadoop spark, git, java, postgresql, devops, jenkins, data ingestion, etl, mongodb, hbase, data analysis, dynamo db, oozie, microsoft azure, machine learning, data engineering, sql server, lambda expressions, sqoop, aws",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Accenture Delivery Architectures (ADA)-AI Platform Engineer,"Project Role :AI Platform Engineer  Project Role Description :Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing. Must have skills :Accenture Delivery Architectures (ADA)  Good to have skills :No Technology Specialization Minimum 10+ year(s) of experience is required  Educational Qualification : Key Reponsibilities :1 Lead architecture, design, cross business area projects and development teams for AI applications2 Identify and explore the framework, NLP tools, VA platform, architecture, or tech-stack can be used to create a solution and the connection between the components Technical Experience :1 Knowledge of applications using MEAN stack, Angular JS, Nodejs, Python2 Understanding of UI technologies like HTML, JavaScript, jQuery3 Hands-on experiences on VA tools like Microsoft Bot Framework, Google Dialog flow, RASA, Amazon Lex at least 24 Knowledge of:Big Data, LAMP Linux, Apache, MySQL, PHP, DevOps, PaaS a:Custom Frameworks, Spring related technologies, UML, OO Design, Spring Boot, Microservice and JEE Architecture b:Cloud AWS/Azure/Google c:Artificial intelligence / Machine Professional Attributes :Good team handling good verbal and written communication. Qualification NA",1.30E+11,13-04-2024,12-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"mean stack, ui technologies, artificial intelligence, paas, mysql, rasa, amazon lex, jquery, microservices, spring, apache, aws cloud, uml, devops, linux, html, big data, microsoft bot framework, microsoft azure, google, lamp, javascript, spring boot, angular, node.js, php",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Designer,"Project Role : Data Designer Project Role Description : Utilizes data science methodology to inform the design research process. Employs statistical and quantitative data analysis to enhance solution development.  Must have skills : Data Science Good to have skills : NA Minimum  10+  year(s) of experience is required Educational Qualification : std Accenture requirements Project Role :Data Designer Project Role Description :Utilizes data science methodology to inform the design research process. Employs statistical and quantitative data analysis to enhance solution development.  Must have Skills :Data Science, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :A:Having broad technology background, with focus on developing software applications in Python for production ready applications B:Writing container-based code and testing code, debugging programs and integrating applications with third-party web services, deployment Work with cross functional team to scale the architecture supporting C:AI models Support data needs for model training/ensemble the models Architecture, security reviews, NFRs Experience in Agile, DevOps, driving best practices Technical Experience :A:Using APIs in python Flask, Falcon or Django, Kafka ML and python libraries as scikit-learn, numpy and pandas B:Machine learning, Deep learning, Image Processing Familiarity with Apache SOLR, MongoDB C:Familiarity with Celery, Rabbit MQ for background task queues Data pre-processing techniques GIT, Docker, Jenkins, Swagger, Cloud AWS/Azure Professional Attributes :Good Communication skills, Good in Analytics and collaboration Educational Qualification:std Accenture requirementsAdditional Info :",1.30E+11,13-04-2024,12-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, data science, jenkins, python, machine learning, scikit-learn, falcon, numpy, swagger, research, docker, deep learning, celery, git, apache, aws cloud, devops, solr, mongodb, ml, image processing, microsoft azure, rabbitmq, pandas, django, kafka, agile, flask",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
vRealize Puppet and VRA,"As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.  You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain. You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews. You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes. You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional Requirements: Configure Vblock switches and network configurations experience. Vblock upgradation from old environment to latest environment. Collaborate with solution architects to design Vblock and VxRail solutions that meet business and technical requirements. Develop detailed infrastructure designs, considering scalability, resilience, performance, and security aspects. Deploy, configure, and integrate Vblock and VxRail systems within the existing IT environment. Ensure proper installation and configuration of hardware, software, and networking components. Monitor the health, performance, and availability of Vblock and VxRail systems. Proactively identify and resolve issues, applying updates, patches, and fixes as needed. Provide advanced troubleshooting and technical support for Vblock and VxRail-related incidents. Collaborate with cross-functional teams to resolve complex technical issues. Implement security best practices to safeguard Vblock and VxRail environments.  Preferred Skills: Devops->Azure->Puppet  Additional Responsibilities: Mandatory Skills- Puppet and VRA  Educational Requirements Bachelor of Engineering  Service Line Cloud & Infrastructure Services *  Location of posting is subject to business requirements",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Puppet and VRA, vrops, cisco ucs, vmware vsphere, vmware esx server, vmware, microsoft azure, ansible, docker, puppet, esxi, high availability, nsx, vsphere, p2v, devops, vrealize orchestrator, linux, powershell, microsoft windows, vcenter, aws, vmotion",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://www.naukri.com/hotjobs/images/v3/infosys_nov13.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills : SAP for Retail Store Operations Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 years of full time education Project Role :Data Platform Engineer Project Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have Skills :SAP for Retail Store OperationsGood to Have Skills : No Industry SpecializationJob Requirements :Key Responsibilities :1 S/4HANA IS Retail Consultant specializing in Store Operations - Analyze and Design SAP Retail processes following all project guidelines / best practices and SAP standards2 Prepare/Work on Configuration documents, Functional Design, Test Cases and Scenario, Functional Testing, Defect and Incident Resolution3 Work with Business/Onshore team to gather business requirements and generate project deliverables 4 Configure and customize SAP S/4HANA to meet the unique Store Operation requirements Technical Experience :1 Hands-On Experience in S/4 HANA Retail Implementation Projects2 Hands-On Experience in Configuration and Customization to meet clients Store OperationsPOS, Inventory, Assortment Planning, Store Analytic Reports, Promotions requirements3 Generating the project document and deliverables Functional Design, Test Scripts, Process documents etc4 Strong knowledge of retail industry processes and store operations5 Strong problem-solving skills and the ability to analyze complex data Professional Attributes :1 Good communications skills 2 Leadership skills 3 Ability to work in a team environment, effectively interacting with others Educational Qualification:15 years of full time educationAdditional Info : Qualification 15 years of full time education",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sap, retail outlets, retail store operations, sap s hana, data modeling, team management, channel sales, retail sales, business development, primary sales, distribution, sales, retail, marketing, sales promotion, secondary sales, sap hana, distributor handling",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills : Teradata BI Good to have skills : Data Warehousing Minimum  12  year(s) of experience is required Educational Qualification : B.E or B.Tech must Project Role :Data Platform Engineer Project Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have Skills :Teradata BIGood to Have Skills : No Technology Specialization, Data WarehousingJob Requirements :Key Responsibilities :1 Gather client requirements and translate the same into system requirements2 Lead effort to design, build and configure applications3 Drive discussions with clients to understand business problems,how Teradata Vantage fits in implementation and solutioning4 Develop the solution blueprint and scoping,estimation,staffing for delivery project5 Drive Discovery activities,design workshops with client,lead strategic road mapping and operating model design discussions Technical Experience :1 Hands on with Teradata Sql,utilities like Bteq,Fast Load,Multi Load,TPT2 Working exposure on datawarehousing concepts like SCD and CDC3 Good SQL skills,including use of derived tables,unions,multitable,joins,Indexes etc4 Exp in UNIX shell scripts to write wrapper scripts to call ETL Jobs Professional Attributes :1 Strong requirement analysis and technical solutioning skill in Data and Analytics2 Excellent writing, communication, and presentation skills3 Eagerness to learn and develop self on an ongoing basis4 Excellent client facing and interpersonal skills Educational Qualification:B.E or B.Tech mustAdditional Info :Knowledge of Teradata Architecture, on performance tuning and best practices Qualification B.E or B.Tech must",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,144,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"presentation skills, teradata bi, teradata architecture, shell scripting, requirement analysis, mload, performance tuning, fastexport, data warehousing, tpump, sql, plsql, bteq, unix shell scripting, data modeling, scd, etl, informatica, unix, teradata sql",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
S&C Global Network - AI - Risk Analytics - Quantexa - Consultant,"Entity :- Accenture Strategy & Consulting Team :- Global Network Data & AI Practice :- CFO & EV - Risk and Compliance Analytics Title :- Specialist / Sr Analyst - I&F Decision Sci Practitioner Job location :- Bangalore, Chennai, Gurgaon, Mumbai, Hyderabad, Pune Job  Summary Key Responsibilities: Working with clients to solve business problems in the area of fraud, compliance and financial crime Use leading open source big-data tools, such as Spark, Hadoop, Scala and Elasticsearch. You should be comfortable with working with high profile clients, on their sites. Managing, transforming and cleansing high volume data into Quantexa Entities and Networks Writing defensive, fault tolerant and efficient code for data processing Developing scoring algorithms to identify high risk activities Using emerging and open source technologies such as Spark, Hadoop, and Scala Implementing systems delivering automated data ingestion, scoring and alert generation Implementing deployment pipelines using modern CI/CD and deployment technologies such as Jenkins/Bamboo/Kubernetes/Openshift Presenting project results to clients Qualification In order to excel in this position , Must have Experience:- Overall, 4 7 years of experience in Risk and Compliance Analytics domain Must have at least recent 1 years of experience in applying Quantexa in Financial domain. Certification in Quantexa is highly preferred. Education:- BE/B.Tech in Engineering/Computer Science or relevant domain Proven big data experience, either from an implementation or a data science prospective, Excellent technical skills including expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch. Experience of building data processing pipelines for use in production hands off batch systems, including either (or preferably both) traditional ETL pipelines and/or analytics pipelines. Strong coding experience in the likes of Scala, Java or Python, Strong client facing, communication and presentation skills, Enthusiasm to learn and develop emerging technologies and techniques. Exhibit strong technical communication skills with demonstrable experience of working in rapidly changing client environments. Demonstrate strong analytical and problem-solving skills and the ability to debug and solve technical challenges with sometimes unfamiliar technologies.",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, scala, presentation skills, coding, java, kubernetes, continuous integration, rest, openshift, microsoft azure, business intelligence, azure devops, sql, microservices, bamboo, pipeline, elastic search, etl pipelines, spark, jenkins, hadoop, etl, finance",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Accenture Delivery Architectures (ADA)-AI Platform Engineer,"Project Role :AI Platform Engineer  Project Role Description :Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing. Must have skills :Accenture Delivery Architectures (ADA)  Good to have skills :NA Minimum 10+ year(s) of experience is required  Educational Qualification : Key Reponsibilities :1 Lead architecture, design, cross business area projects and development teams for AI applications 2 Identify and explore the framework, NLP tools, VA platform, architecture, or tech-stack can be used to create a solution and the connection between the components. Technical Experience : 1 Knowledge of applications using MEAN stack, Angular JS, Nodejs, Python 2 Understanding of UI technologies like HTML, JavaScript, jQuery 3 Hands-on experience on VA tools like Microsoft Bot Framework, Google Dialog flow, RASA, Amazon Lex at least 2 4 Knowledge of:Big Data, LAMP Linux, Apache, MySQL, PHP, DevOps, PaaS a:Custom Frameworks, Spring related technologies, UML, OO Design, Spring Boot, Microservice and JEE Architecture b:Cloud AWS/Azure/Google c:Artificial intelligence / Machine Professional Attributes :Good team handling good verbal and written communication.",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"jquery, uml, paas, python, node.js, mean stack, rasa, amazon lex, jee, artificial intelligence, microservices, spring, apache, aws cloud, devops, linux, mysql, html, big data, microsoft bot framework, microsoft azure, google, lamp, javascript, spring boot, angular, php, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
S&C Global Network - AI - Risk Analytics - Quantexa - Consultant,"Entity :- Accenture Strategy & Consulting Team :- Global Network Data & AI Practice :- CFO & EV - Risk and Compliance Analytics Title :- Specialist / Sr Analyst - I&F Decision Sci Practitioner Job location :- Bangalore, Chennai, Gurgaon, Mumbai, Hyderabad, Pune Job  Summary Key Responsibilities: Working with clients to solve business problems in the area of fraud, compliance and financial crime Use leading open source big-data tools, such as Spark, Hadoop, Scala and Elasticsearch. You should be comfortable with working with high profile clients, on their sites. Managing, transforming and cleansing high volume data into Quantexa Entities and Networks Writing defensive, fault tolerant and efficient code for data processing Developing scoring algorithms to identify high risk activities Using emerging and open source technologies such as Spark, Hadoop, and Scala Implementing systems delivering automated data ingestion, scoring and alert generation Implementing deployment pipelines using modern CI/CD and deployment technologies such as Jenkins/Bamboo/Kubernetes/Openshift Presenting project results to clients Qualification In order to excel in this position , Must have Experience:- Overall, 4-7 years of experience in Risk and Compliance Analytics domain Must have at least recent 1 years of experience in applying Quantexa in Financial domain. Certification in Quantexa is highly preferred. Education:- BE/B.Tech in Engineering/Computer Science or relevant domain Proven big data experience, either from an implementation or a data science prospective, Excellent technical skills including expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch. Experience of building data processing pipelines for use in production hands off batch systems, including either (or preferably both) traditional ETL pipelines and/or analytics pipelines. Strong coding experience in the likes of Scala, Java or Python, Strong client facing, communication and presentation skills, Enthusiasm to learn and develop emerging technologies and techniques. Exhibit strong technical communication skills with demonstrable experience of working in rapidly changing client environments. Demonstrate strong analytical and problem-solving skills and the ability to debug and solve technical challenges with sometimes unfamiliar technologies.",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, scala, presentation skills, coding, java, continuous integration, kubernetes, rest, openshift, microsoft azure, business intelligence, azure devops, sql, microservices, bamboo, pipeline, elastic search, etl pipelines, spark, jenkins, hadoop, etl, finance",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,"Chennai, Gurugram, Bengaluru","Chennai, Gurugram, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer," This role is part of the UK&I Ingestion programme, running within the UK&I Data Office, which is modernising the UK&I s data ingestion approach, delivering new pipelines on cloud-based technologies. This is a strategic initiative for Experian, to standardise and rationalise the UK&I technology estate, increase speed to load, and reduce the time taken to on-board new data to the UK&I region, as well as drive revenue for our EDQ business.         ?       Knowledge, Skills and Experience           Terraform - Intermediate AWS Core (EC2, S3, EKS, Lambda)        - Expert AWS Logging (Cloudwatch, Cloudtrail)      - Intermediate AWS MWAA , AWS Aurora Postgres, EMR Serverless - Intermediate      - Expertise of the full development lifecycle      - Expertise of supporting a complex technical environment      - Expertise in Python     - Expertise in Web Services (RESTFUL, gRPC, GraphQL) design, development & integrations      - Expertise in Framework like .Net, - Experience in Business Modelling, Architectural Mechanisms, Design Patterns, Performance tuning etc     - Expertise in Security and Best-Practices Aptitude and attitude to learn new things at pace Ability and desire to adapt High energy, positive and collaborative approach    Qualifications               What youll bring     This role is part of the UK&I Ingestion programme, running within the UK&I Data Office, which is modernising the UK&I s data ingestion approach, d",60524500412,06-05-2024,04-08-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Software Product,"Performance tuning, Web services, Architecture, Cloud, Manager Technology, Design development, Business modeling, AWS, Python",-,9am-6pm,"Full Time, Permanent",Experian,Organization,Experian,https://img.naukimg.com/logo_images/groups/v1/4594669.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
PLM Platform Engineer,"   DevOps mindset and passion for modern Open-Source technologies    Work in a global IT support team with internal\/external, onsite\/offshore colleagues    Responsibility for the Product Lifecycle Management platform, based on PTC Windchill and including Linux & Windows servers, SQL databases and Open Shift applications    Ability to plan and carry out stabilization and scalability improvement measures    Document architecture and operations details with implementation\/operations partner and secure these in global Enterprise Architecture Management tools    Support global end-user community to solve technical issues    Ability to coordinate deliverables and dependencies across organization    Problem solving and analytic ability are the key skills required for this position          You are best equipped for this task if you have:             Minimum 4 years work experience in IT    Expert know-how in Linux system administration, Windows system administration experience is a plus    Expert know-how in configuration \/ infrastructure as code and common automation tools (Ansible, Puppet, Rundeck, )    Hands-on knowledge in SQL (MSSQL, PL\/SQL, SQL Plus, )    Experiences with Kubernetes, Openshift, EKS or similar and associated automation tools like Helm, Kustomize or ArgoCD    Experiences in common software development tools (Git, Jira)    Know-how of CI\/CD best practices and frameworks (Jenkins, Gitlab CI, )    Deep knowledge in IT application monitoring (Dynatrace, ELK, Prometheus, Grafana, )    Experience in administration of Java (web-) applications    Knowledge of authentication, authorization, access delegation and Single Sign-On (SAML, OpenID) and tools like Ping Federate or similar    Basic Knowledge in administrating a product lifecycle management platform like PTC Windchill is a plus    Software (web-)development experience in a common language (Python, JavaScript, Java, ) is a plus    Proficient skills with the Linux shell (bash)    Good hands-on skills in a further scripting language like Python    Experience with Ansible is desirable, otherwise the willingness to learn it    Solid knowledge of the Java stack (JVM, Apache Tomcat, Apache HTTP Server, )    Basic knowledge with a JavaScript framework (like React) is a plus   ",2.91E+11,29-09-2023,28-12-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Electronic Components / Semiconductors,"PLM, Linux, Web development, Javascript, PLSQL, HTTP, Open source, JIRA, Python, Windows System Administration",-,9am-6pm,"Full Time, Permanent",Infineon,Organization,Infineon,https://img.naukimg.com/logo_images/groups/v1/587312.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Analytics Test Engineer,"Qualcomm Technologies, Inc is looking for Data Analytics Test Engineer to join our dynamic team and embark on a rewarding career journey                 5+ years of Quality Assurance/Testing experience          3+ years of Data Quality experience, or QA experience with a focus on data, data warehousing, reporting          Experience working with Amazon Web Services, querying and working with data in various AWS services          Experience in testing web-based and Android based applications          Strong SQL experience, with knowledge of AWS Redshift or columnar databases          Experience with Performance Test Design, Development and load testing execution for server and web UI products          Experience in running performance tests using Apache JMeter, LoadRunner or similar tools        Excellent testing skills and knowledge of test planning, processes and execution with the ability to demonstrate a clear understanding of the software development life cycle and quality processes      Enhance existing and create new automation strategies to enable performance and product testing at scale.      Skilled in issue identification, documentation, tracking and resolution      Ability to methodically troubleshoot and root cause complex system issues                        Minimum Qualifications:        Bachelors degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Test Engineering or related work experience.    OR    Masters degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Systems Test Engineering or related work experience.    OR    PhD in Engineering, Information Systems, Computer Science, or related field.                The Ideal Candidate will have:            Experience working with AWS services      Familiar with Android Debug Bridge (ADB) and familiar with Qualcomm Products and Device tools: QXDM, APEX, AXIOM      Programming experience in Python, Java, Perl for the purposes of developing automation scripts.                                                                                                              ",2.40E+11,24-02-2024,24-05-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Telecom / ISP,"Automation, Manager Quality Assurance, Test design, Perl, Data quality, Apache, SQL, Python, Android",-,9am-6pm,"Full Time, Permanent","Qualcomm Technologies, Inc",Organization,"Qualcomm Technologies, Inc",https://img.naukimg.com/logo_images/groups/v1/356782.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Full Stack Engineer,"Project Role : Full Stack Engineer Project Role Description : Responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. Use development skills to deliver innovative solutions that help our clients improve the services they provide. Leverage new technologies that can be applied to solve challenging business problems with a cloud first and agile mindset.  Must have skills : Machine Learning Good to have skills : Cloud Infrastructure, Gen AI Minimum  5  year(s) of experience is required Educational Qualification : 15 Years of full time education  Machine Learning Engineer Job DescriptionA Machine Learning Engineer is a professional who is skilled and specializes in designing and developing machine learning systems, possess expertise in statistics, programming, and data science. The role involves creating efficient self-learning applications and contributing to advancements in artificial intelligence. Qualifications: Strong programming skills (Python, R, or similar languages). Knowledge of data science concepts, including feature engineering, model evaluation, and validation. Expertise in statistics and understanding of machine learning algorithms. Familiarity with deep learning frameworks (e.g., TensorFlow, PyTorch). Experience with cloud platforms (AWS, Google Cloud, Azure) for model deployment. Ability to work collaboratively in cross-functional teams.Responsibilities:?Study and Transform Data Science Prototypes:Work with data to create models, perform statistical analysis, and train and retrain systems to optimize performance.Explore and preprocess data, ensuring it's suitable for machine learning tasks.Design Machine Learning Systems:Design and construct sophisticated machine learning models.Choose appropriate algorithms and architectures based on the problem domain and data characteristics.?Research and Implement ML Algorithms and Tools:Staying updated with the latest developments in the field, they explore new algorithms and tools.Implement machine learning algorithms, adapting them to specific use cases.Develop Machine Learning Applications:Build applications that leverage machine learning models.Collaborate with software engineers to integrate ML components into larger systems.Deploy and Maintain Machine Learning ModelsEnsure scalability and reliability, they deploy models in cloud platforms such as AWS.Regular performance evaluations, identifying bottlenecks, and implementing optimization strategies.Passionate about pushing the boundaries of AI technology and creating innovative self-learning applications, consider joining our team as a Machine Learning Engineer! Qualification 15 Years of full time education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"financial analysis, financial management, vlookup, budgeting, business administration, Ability to learn, financial modelling, auditing, accounting, credit appraisal, financial statement analysis, finance",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Full Stack Engineer,"Project Role : Full Stack Engineer Project Role Description : Responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. Use development skills to deliver innovative solutions that help our clients improve the services they provide. Leverage new technologies that can be applied to solve challenging business problems with a cloud first and agile mindset.  Must have skills : Machine Learning Good to have skills : Cloud Infrastructure, Gen AI Minimum  3  year(s) of experience is required Educational Qualification : 15 Years of full time education Machine Learning Engineer Job DescriptionA Machine Learning Engineer is a professional who is skilled and specializes in designing and developing machine learning systems, possess expertise in statistics, programming, and data science. The role involves creating efficient self-learning applications and contributing to advancements in artificial intelligence. Qualifications: Strong programming skills (Python, R, or similar languages). Knowledge of data science concepts, including feature engineering, model evaluation, and validation. Expertise in statistics and understanding of machine learning algorithms. Familiarity with deep learning frameworks (e.g., TensorFlow, PyTorch). Experience with cloud platforms (AWS, Google Cloud, Azure) for model deployment. Ability to work collaboratively in cross-functional teams.Responsibilities:?Study and Transform Data Science Prototypes:Work with data to create models, perform statistical analysis, and train and retrain systems to optimize performance.Explore and preprocess data, ensuring it's suitable for machine learning tasks.Design Machine Learning Systems:Design and construct sophisticated machine learning models.Choose appropriate algorithms and architectures based on the problem domain and data characteristics.?Research and Implement ML Algorithms and Tools:Staying updated with the latest developments in the field, they explore new algorithms and tools.Implement machine learning algorithms, adapting them to specific use cases.Develop Machine Learning Applications:Build applications that leverage machine learning models.Collaborate with software engineers to integrate ML components into larger systems.Deploy and Maintain Machine Learning ModelsEnsure scalability and reliability, they deploy models in cloud platforms such as AWS.Regular performance evaluations, identifying bottlenecks, and implementing optimization strategies.Passionate about pushing the boundaries of AI technology and creating innovative self-learning applications, consider joining our team as a Machine Learning Engineer! Qualification 15 Years of full time education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"accounts receivable, accounts payable, accounting, agile, finance, fixed assets, profitability, Information gathering, accruals, general ledger, reconciliation, indirect taxation, expense management, claims, sox",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Full Stack Engineer,"Project Role : Full Stack Engineer Project Role Description : Responsible for developing and/or engineering the end-to-end features of a system, from user experience to backend code. Use development skills to deliver innovative solutions that help our clients improve the services they provide. Leverage new technologies that can be applied to solve challenging business problems with a cloud first and agile mindset.  Must have skills : Machine Learning Good to have skills : Cloud Infrastructure, Gen AI Minimum  5  year(s) of experience is required Educational Qualification : 15 Years of full time education  Machine Learning Engineer Job DescriptionA Machine Learning Engineer is a professional who is skilled and specializes in designing and developing machine learning systems, possess expertise in statistics, programming, and data science. The role involves creating efficient self-learning applications and contributing to advancements in artificial intelligence. Qualifications: Strong programming skills (Python, R, or similar languages). Knowledge of data science concepts, including feature engineering, model evaluation, and validation. Expertise in statistics and understanding of machine learning algorithms. Familiarity with deep learning frameworks (e.g., TensorFlow, PyTorch). Experience with cloud platforms (AWS, Google Cloud, Azure) for model deployment. Ability to work collaboratively in cross-functional teams.Responsibilities:?Study and Transform Data Science Prototypes:Work with data to create models, perform statistical analysis, and train and retrain systems to optimize performance.Explore and preprocess data, ensuring it's suitable for machine learning tasks.Design Machine Learning Systems:Design and construct sophisticated machine learning models.Choose appropriate algorithms and architectures based on the problem domain and data characteristics.?Research and Implement ML Algorithms and Tools:Staying updated with the latest developments in the field, they explore new algorithms and tools.Implement machine learning algorithms, adapting them to specific use cases.Develop Machine Learning Applications:Build applications that leverage machine learning models.Collaborate with software engineers to integrate ML components into larger systems.Deploy and Maintain Machine Learning ModelsEnsure scalability and reliability, they deploy models in cloud platforms such as AWS.Regular performance evaluations, identifying bottlenecks, and implementing optimization strategies.Passionate about pushing the boundaries of AI technology and creating innovative self-learning applications, consider joining our team as a Machine Learning Engineer! Qualification 15 Years of full time education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data science, pytorch, python, machine learning, statistics, algorithms, c++, css, Work ethic, docker, ansible, gen, java, git, gcp, devops, linux, jenkins, data structures, html, mysql, mongodb, microsoft azure, javascript, r, node.js, full stack, cloud infrastructure",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer - Data Modeling Techniques and Methodologies,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Data Modeling Techniques and Methodologies Good to have skills : Extract Transform & Load Minimum  5  year(s) of experience is required Educational Qualification : A Engineering graduate preferably Computer Science graduate 15 years of full time education Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance.  Roles & Responsibilities: Overall 5+ years of experience working in DWH, Data Analytics projects Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure. Deploy infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure successful delivery of cloud-based solutions. Stay updated with the latest advancements in cloud technologies and integrate innovative approaches for sustained competitive advantage. MUST have done fresh data modeling for 2 or more projects Have strong understanding of Datawarehouse concept Well versed with Data modeling tool like Erwin Experience of Rationalization of data model to move to Self service mode Good to have Cloud (Azure) exposure or experience Professional & Technical Skills: Must To Have Skills:Strong experience in Data Modeling Techniques and Methodologies. Good To Have Skills:Experience in cloud technologies such as AWS, Azure, or Google Cloud Platform. Experience in deploying infrastructure and platform environments. Experience in creating a proof of architecture to test architecture viability, security, and performance. Experience in collaborating with cross-functional teams to ensure successful delivery of cloud-based solutions. Additional Information: The candidate should have a minimum of 5 years of experience in Data Modeling Techniques and Methodologies. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful cloud-based solutions. This position is based at our Bengaluru office. Qualification A Engineering graduate preferably Computer Science graduate 15 years of full time education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data warehousing, data modeling, gcp, data analytics, aws, kubernetes, networking, erwin, docker, ansible, git, java, devops, linux, paas, jenkins, shell scripting, mysql, cloud computing, python, maven, vmware, microsoft azure, cloud platform, saas, cloud infrastructure",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Staff Engineer - Cloud,"     - Qualcomm is hiring multiple software engineers at Senior and Staff level to help architect and build its next generation data processing platform to support Autonomous Driving R&D efforts      Our goal is to design and build a highly scalable, efficient, and modular data platform      This platform will be used by engineers to run re-simulation pipelines, machine learning workloads, perform in-depth data analysis/analytics, visualize results, and more      Excellent communication and planning skills are critical in this role as well be working with internal teams and external partners            - Responsibilities for this position include:      - Work with team leads to understand use cases and requirements      - Build proof-of-concepts to validate proposed designs and provide feedback      - Implement containerized workloads, microservices, distributed messaging systems and highly scalable distributed processing services.      - Drive software engineering best practices within immediate and external teams      - Support users of the platform      - Collaborate in global cross functional team          Preferred Qualifications:      Years of Exp -> 10+ yrs          Must :      - Industry experience in designing and implementing scalable solutions.      - Strong in object oriented programming concepts, DSA and programming.      - Familiarity using a programming language such as Python, Go, C/C++, Java. -> clean coding in these languages, code analysis tools.      - Strong in building CI/CD pipelines on cloud.      - Strong in RDBMS, NoSQL DB technologies like Posgresql, mongodb etc      - Experience in supporting applications post production like debugging, documentation, support.      - Building tools for automating engineering processes, data processing like SPARK.      - Taking ownership of an application as well as helping in allocating tasks to team members and unblock them from issues.      - Quick Learner and able to demonstrate POCs.      - Open to take new challenges and learn new tech-stacks and technologies.      - Hands-on experience using managed services from one or more of the major cloud vendors: AWS, GCP, Azure      - Experience with containerized platforms like docker and Kubernetes.      - Experience in designing, developing, testing, and deploying applications in the cloud.      - Experience with building orchestrations and workflow management tools such as Airflow, Prefect, Argo or Cloud native technologies like AWS Batch, Azure Data factory.      - Understanding existing code, prepare documentation and working on bug fixes and enhancements.      - Experience working with distributed processing architecture like spark etc.      - Experience in Agile and SaFe way of working.      - Experience in recruiting candidates.                            Minimum Qualifications:          Bachelors degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Engineering or related work experience.    OR    Masters degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.    OR    PhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.      2+ years of work experience with Programming Language such as C, C++, Java, Python, etc.              Good to have :      - Experience in Data Engineering and Data Warehousing.      - Experience with building data pipelines on cloud.      - Experience working with Spark, Hadoop, Hive, or other Apache Foundation frameworks.      - Experience Autonomous Driving R&D applications would be added advantage.      - Experience in working on migration projects such as code migration, data migration etc.      - Experience working with spring boot framework      - Experience working on code builds such as Bazel.  ",2.21E+11,22-12-2023,21-03-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,Telecom / ISP,"Computer science, C++, Data analysis, Simulation, Coding, GCP, Debugging, Machine learning, Analytics, Python",-,9am-6pm,"Full Time, Permanent","Qualcomm Technologies, Inc",Organization,"Qualcomm Technologies, Inc",https://img.naukimg.com/logo_images/groups/v1/356782.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Backend Distributed System Engineer - SMTS,"   This is a great opportunity for ambitious engineers who want technical growth in distributed systems development, data processing, as well as security. Your talent will find significant outreach and business impact, contributing to the CSO s security goals.       Here's what you'll do         Design and implement highly available, resilient, and scalable distributed systems to continuously determine cloud misconfigurations and drift across Salesforce s public cloud resources.     Build observability and remediation services to help Salesforce s engineers, leaders, and internal customers to determine security risk and act on remediating the misconfigurations.     Operate in an fast paced - agile development environment, including participating in daily scrums.     Own and deliver initiatives adding new features to meet ever-growing product demands.     Support the team s engineering and operational excellence by participating in architecture/code/operational reviews, identifying and fixing issues.     Work cross-functionally with product management and other partner teams to complete large-scale projects with impact across the company.     Adapt to change quickly and eagerly: changing requirements, changing priorities, changing strategies.             Required Skills         Industry experience of 4+ years with a M.Sc/B.E degree.     4+ years of deep understanding of object-oriented programming and profeciency with at least one object-oriented programming language (Java, Go, Python) - Python GoLang preferred.     Experience building highly performant, highly-available (99.99%), and highly fault-tolerant, large-scale distributed systems using native public cloud stack..     Experience building ETL pipelines using frameworks like Apache Airflow, Kafka, Spark etc     Experience with building systems using services on AWS.     Strong knowledge and experience working with SQL and NoSql Databases.     Demonstrated experience building services with Docker and Kubernetes.     Experience with Scrum or other agile development methodologies, with attention to code quality, and delivering secure code.     Self-starter, independent, and driven with the required emotional intelligence to work in an environment that values team success, collaboration, and business outcomes.     Excellent oral and written communication.             Nice-to-Have Skills             Knowledge or experience of AI/ML.     Experience with building systems using services on Google Cloud Platform (GCP) or Azure     Strong knowledge of security fundamentals including, authentication/authorization frameworks (eg, SSO, SAML, OAuth, etc), secure transport (eg, TLS), and identity management (eg, certificates, PKI) is nice to have.     Deep knowledge of observability frameworks like Splunk, Graffana, Prometheus     ",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Internet,"Product management, Backend, Data processing, Scrum, Apache, Distribution system, SQL, Python, Salesforce, Identity management",-,9am-6pm,"Full Time, Permanent",Salesforce,Organization,Salesforce,https://img.naukri.com/logo_images/v3/1882140.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Accenture Delivery Architectures (ADA)-AI Platform Engineer,"Project Role :AI Platform Engineer  Project Role Description :Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing. Must have skills :Accenture Delivery Architectures (ADA)  Good to have skills :No Technology Specialization Minimum 10+ year(s) of experience is required  Educational Qualification : Key Reponsibilities :1 Lead architecture, design, cross business area projects and development teams for AI applications2 Identify and explore the framework, NLP tools, VA platform, architecture, or tech-stack can be used to create a solution and the connection between the components Technical Experience :1 Knowledge of applications using MEAN stack, Angular JS, Nodejs, Python2 Understanding of UI technologies like HTML, JavaScript, jQuery3 Hands-on experiences on VA tools like Microsoft Bot Framework, Google Dialog flow, RASA, Amazon Lex at least 24 Knowledge of:Big Data, LAMP Linux, Apache, MySQL, PHP, DevOps, PaaS a:Custom Frameworks, Spring related technologies, UML, OO Design, Spring Boot, Microservice and JEE Architecture b:Cloud AWS/Azure/Google c:Artificial intelligence / Machine Professional Attributes :Good team handling good verbal and written communication. Qualification NA",1.30E+11,13-04-2024,12-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"mean stack, ui technologies, artificial intelligence, paas, mysql, Work ethic, rasa, amazon lex, jquery, spring, apache, aws cloud, uml, linux, html, big data, microsoft bot framework, microsoft azure, google, lamp, javascript, spring boot, angular, node.js, php",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Accenture Delivery Architectures (ADA)-AI Platform Engineer,"Project Role :AI Platform Engineer  Project Role Description :Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing. Must have skills :Accenture Delivery Architectures (ADA)  Good to have skills :No Technology Specialization Minimum 10+ year(s) of experience is required  Educational Qualification : Key Reponsibilities :1 Lead architecture, design, cross business area projects and development teams for AI applications2 Identify and explore the framework, NLP tools, VA platform, architecture, or tech-stack can be used to create a solution and the connection between the components. Technical Experience :1 Knowledge of applications using MEAN stack, Angular JS, Nodejs, Python2 Understanding of UI technologies like HTML, JavaScript, jQuery3 Hands-on experience on VA tools like Microsoft Bot Framework, Google Dialog flow, RASA, Amazon Lex at least 24 Knowledge of:Big Data, LAMP Linux, Apache, MySQL, PHP, DevOps, PaaS a:Custom Frameworks, Spring related technologies, UML, OO Design, Spring Boot, Microservice and JEE Architecture b:Cloud AWS/Azure/Google c:Artificial intelligence / Machine Professional Attributes :Good team handling good verbal and written communication. Qualification NA",1.30E+11,13-04-2024,12-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"software development, json, html, agile software development, communication skills, continuous integration, redux, ux, ibm cloud, jest, cloud technologies, javascript, react.js, node.js, git, Time management, written communication, aha, graphql",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Development Engineer,"Project Role : Software Development Engineer Project Role Description : Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.  Must have skills : Google BigQuery Good to have skills : NA Educational Qualification : minimum 15 years of full time education Project Role :Software Development Engineer Project Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work. Must have Skills :Google BigQueryGood to Have Skills : Job Requirements :Key Responsibilities :Analyze and model client market and key performance data Use analytical tools and techniques to develop business insights and improve decisionmaking 1:Data Proc PubSub Data flow Kalka Streaming Looker SQL No FLEX2:Proven track record of delivering data integration data warehousing soln3:Strong SQL And Handson Pro in BigQuery SQL languageExp in Shell Scripting Python No FLEX4:Exp with data integration and migration projects Oracle SQL Technical Experience :Google BigQuery1:Expert in Python NO FLEX Strong handson knowledge in SQL NO FLEX Python programming using Pandas NumPy deep understanding of various data structure dictionary array list tree etc experiences in pytest code coverage skills2:Exp with building solutions using cloud native services:bucket storage Big Query cloud function pub sub composer and Kubernetes NO FLEX3:Pro with tools to automate AZDO CI CD pipelines like ControlM GitHub JIRA confluence CI CD Pipeline Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills presentation skills ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:minimum 15 years of full time education Additional Information : Qualification minimum 15 years of full time education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sql, flex, bigquery, python, google, continuous integration, kubernetes, confluence, data warehousing, numpy, control-m, java, git, shell scripting, html, mysql, jira, github, software development, oracle, javascript, sql server, pandas, oracle sql, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer,"     Minimum 3-4 years of Pyspark Hands on experience      Minimum 2-3 years of Data bricks experience     Strong knowledge on SQL     Hands on experience on Informatica, IICS     Good communication skills     Experience working on Git     Experience working agile mode     Experience of CI/CD pipelines and cloud deployment     Design, develop, and implement end-to-end data solutions leveraging Azure services like Azure Data Factory, Azure Databricks, Azure SQL Database, and Azure Synapse Analytics, ADLS etc.     Experience of Snowflake a Plus     Minimum 6 Years of total experience     ",3.00E+11,30-04-2024,29-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Insurance,"GIT, SQL database, Cloud, Agile, Deployment, Informatica, Analytics",-,9am-6pm,"Full Time, Permanent",Chubb,Organization,Chubb,-,"Warangal, Hyderabad, Nizamabad","Warangal, Hyderabad, Nizamabad",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Development Engineer,"Project Role : Software Development Engineer Project Role Description : Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.  Must have skills : Microsoft Azure Analytics Services Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : BTech or 15 years of full time Education Summary :As a Software Development Engineer, you will be responsible for analyzing, designing, coding, and testing multiple components of application code using Microsoft Azure Analytics Services. Your typical day will involve working with cross-functional teams, performing maintenance, enhancements, and development work to deliver impactful data-driven solutions.  Roles & Responsibilities: Design, develop, and maintain Azure Analytics Services solutions, including data ingestion, storage, ETL processing, and visualization. Collaborate with cross-functional teams to analyze, design, code, and test multiple components of application code across one or more clients. Perform maintenance, enhancements, and development work to ensure the quality, performance, and scalability of Azure Analytics Services solutions. Utilize strong analytical and problem-solving skills to troubleshoot and resolve complex technical issues related to Azure Analytics Services solutions. Stay updated with the latest advancements in Microsoft Azure Analytics Services, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Strong experience in Microsoft Azure Synapse Analytical service and Azure Data Factory. Good To Have Skills:Experience with other Azure services such as Azure Databricks, Snowflake.  Solid understanding of data warehousing concepts, including data modeling, ETL, and data visualization. Experience with programming languages such as Python, SQL, and R. Strong understanding of cloud computing concepts and architectures. Experience with Agile methodologies and DevOps practices.Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Analytics Services. The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Gurugram office. Qualification BTech or 15 years of full time Education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"analytics services, azure analytics, azure data factory, data warehousing concepts, software engineering, azure databricks, snowflake, azure synapse, microsoft azure, data warehousing, sql, devops, data visualization, cloud computing, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Senior Software Engineer, Actimize","So, what?? the role all about? ? As a Senior Software Engineer at Actimize, you'll lead the development of advanced software solutions. Collaborate closely with teams, write high-quality code, and drive innovation in Actimize's products. Your expertise will be crucial in optimizing performance and integrating components effectively. Join us in shaping the future of financial technology. ? ? How will you make an impact?   ?   Your proficiency in  Python  and database management will shape innovative software solutions. ?   By collaborating closely with the team, you'll design, implement, and optimize robust applications, ensuring high-quality results. ? Have you got what it takes? ?   Experience: 4-6 Years. ?   Has working experience with vector embedding and DB in LLM applications. ?   Hands-on experience with database, preferably MySQL and MongoDB a plus?  ?   Proficiency in Linux environments and command-line tools.?  ?   In depth and hands-on experience working with  AWS , with good understanding of AWS networking. ? ? You will have an advantage if you also have: ?   Strong problem-solving skills and the ability to think critically.?  ?   Working knowledge of K8s, Helm   ML engineering would be plus. What?? in it for you? ? Join an ever-growing, market disrupting, global company where the teams ??comprised of the best of the best ??work in a fast-paced, collaborative, and creative environment! As the market leader, every day at NICE is a chance to learn and grow, and there are endless internal career opportunities across multiple roles, disciplines, domains, and locations. If you are passionate, innovative, and excited to constantly raise the bar, you may just be our next NICEr! ? ?   Enjoy NICE-FLEX!  ? At NICE, we work according to the NICE-FLEX hybrid model, which enables maximum flexibility: 2 days working from the office and 3 days of remote work, each week. Naturally, office days focus on face-to-face meetings, where teamwork and collaborative thinking generate innovation, new ideas, and a vibrant, interactive atmosphere. ? ? Requisition ID:  3911 Reporting into:  Tech Manager ? Role Type:  Individual Contributor ? ?",70524009763,07-05-2024,05-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Software Product,"kubernetes, linux internals, hibernate, helm, sql, microservices, plsql, docker, spring, database management, java, linux, software solutions, jenkins, actimize, mysql, software engineering, mongodb, ml, rest, python, oracle, engineering, command, javascript, sql server, spring boot, aws, unix",-,9am-6pm,"Full Time, Permanent",NICE,Organization,NICE,https://img.naukimg.com/logo_images/groups/v1/4656921.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Developer - AWS,"       You and the team will be responsible for developing and implementing our data synchronization platform     In this role, you will have responsibilities and tasks concerning solid implementation of cloud infrastructure solutions including deployment, maintenance, and stable operation                 What s in it for you   :          As a Cloud Developer, you will support us in enabling our on-premises systems to communicate and synchronize certain data with and to separate data storage in the cloud to connect to third-party and internal systems     If you are a cloud enthusiast or even an AWS SME and passionate about technology, development, automation, and architecture then you will be the best fit for the position, and as part of your role help us ensure that our applications and infrastructure are designed and implemented to the highest security standards thus maintaining and enhancing business trust, compliance (GDPR) ensuring alignment with strategic and division plans, and enabling transparency into technology investments across the enterprise                 Let s talk about the Responsibilities       Working independently to design solutions to deliver business requirements and align to architectural and IT roadmaps through the following activities:         Collaborating with the API development team in Bangalore along with teams in Europe.         Working with the team lead directly and with other Solution Architects and lead Developers to agree upon architectural solutions.         Documentation and presentation of solutions in detail to allow an estimation of the effort required for delivery.         Implementing solutions according to enterprise SaaS standards         Implement the technical best practices and principles on API and Microservice architecture.          Design and develop reusable application programming interfaces for use across the Medical Devices/Applications in MEDIFOX DAN         Write unit and integration tests, in automated test environments to ensure code quality.               Solution Delivery       Working with the project team consisting of Scrum Masters, Business Analysts, Developers, and Testers to facilitate the delivery of solutions through the following activities:         Provide accurate estimations for new modules and suggest ways towards solutions from your expertise.         Have close contact and regular touchpoints with your team lead and Bangalore.         Mirroring back a full understanding of the solution and elaborating on the solution implementation where required to achieve this goal.         Supporting the development and test teams in diagnosing and resolving architectural and technical issues in a consultative capacity throughout the project               Let s talk about Required Qualifications Experience           Working experience in Solution Engineering/Development, preferably in online application development or large complex enterprise SaaS-based applications for at least 4+ years.          Working experience of Cloud Services (especially AWS) modules and respective programming techniques understanding and implementation along with multiple deployments.         Experience in implementing, deploying, and operating highly available, scalable, secure, and fault-tolerant systems using Amazon Web Services (AWS) products like DMS, SCT Redshift, Lambda, Glue, S3, EMR, Kinesis, DynamoDB, SQL RDBMS         Experience in creating scalable data-focused systems on AWS cloud.         Working knowledge of backend engineering / API development, Enterprise architecture using any object-oriented language like Java, Python, TypeScript, NodeJS, Microservices, Containerization, DevOps.         Experience with API concepts and technologies such as REST, JSON, XML, SOAP, GraphQL, and Swagger and technology parts like CI/CD pipelines, NoSQL (E.g., MongoDB, DynamoDB), GIT (E.g., Gitlab / Bitbucket / GitHub), Message queues (Kafka, RabbitMQ) and security technologies such as encryption, digital signatures and hashing (MD5, AES, RSA), SSL, SAML, XACML, Oauth         In-depth knowledge of relational databases (Postgres, oracle) and ORMs (Entity Framework, Dapper)         Basic understanding of CI/CD pipelines, Knowledge of Docker and Kubernetes         Creative problem-solving skills and good communication skills               Let s talk about Desired Qualifications Experience           AWS Solution Architect Associate/Professional or TOGAF Certification         Hands-on experience with medical platform FHIR (   Fast Healthcare Interoperability Resources)    /GDPR-compliant concepts and implementations         Knowledge on C#, C++ and .Net technology stack       ",2.90E+11,29-02-2024,29-05-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"C++, Automation, TOGAF, XML, Application development, Oracle, associate professional, Solution Architect, SQL, Python",-,9am-6pm,"Full Time, Permanent",Matrixcare India,Organization,Matrixcare India,https://img.naukimg.com/logo_images/groups/v1/4611461.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Remote Google Cloud Platform(GCP) developer,"   PradeepIT is looking for Google Cloud Platform(GCP) developer     Experience range: 6 - 10 years     Ability to start from scratch by setting up the complete environment     Attitude to explore and learn knowledge areas in GCP.     Hands-on experience in API development, Data base exposure.     Work Location Kochi             Other information         Role: Software Development      Industry: IT Services & Consulting     Functional Area : Engineering - Software & QA     Role : Software Development               ",2.60E+11,26-03-2024,24-06-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"IT services, Software QA, GCP, Consulting, Cloud, Database, Software engineering",-,9am-6pm,"Full Time, Permanent",Pradeepit Consulting Services,Organization,Pradeepit Consulting Services,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Lead Developer - Golang,"       About this job:       As a    Lead Developer - Golang    at Gamelight you will be responsible for building, improving and maintaining our Golang Backend. Moreover, you will be responsible for challenging and improving our Tech Stack and leading our development team.           What you will do:         Lead our development of highly scalable software systems that run all over the globe and handle the backend of our Android and iOS Apps     Introduce new ideas and solutions to further improve our 50+ Backend Microservices     Make use of our AWS cloud-based infrastructure including DynamoDB, RDS, SQS, Cloudwatch, ECS, IVS, Elasticache and many more             Your profile:         Minimum 2 years of experience in Developing Web Application in Go     Knowledge about Databases (MySQL, NoSQL, Redis)     Experience in Docker & AWS Cloud Services (or one of the competitor cloud services)     Fluent English               ",2.60E+11,26-03-2024,24-06-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Backend, SAP, NoSQL, Cloud Services, MySQL, Silicon, Open source, Adobe, Android, Salesforce",-,9am-6pm,"Full Time, Permanent",Pradeepit Consulting Services,Organization,Pradeepit Consulting Services,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Database Developer,"  Strong      Data Obfuscation Skills using any version of   Broadcom TDM (Test      Data Manager)  , or Informatica  TDM/MDM Data      Masking using Broadcom TDM Functions  or using Informatica TDM      Functions   Data      profiling using Broadcom TDM   At      least 1-2 years of proven experience on CA TDM tools and       2   years on any ANSI SQL Database.   ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ETL Tool, PLSQL, SQL Queries",-,9am-6pm,"Full Time, Temporary/Contractual",Net2Source LLP  ,Organization,Net2Source LLP  ,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
ML/AI Developer,"         We are looking for an expert in machine learning to help us extract value from our data     You will lead all the processes from data collection, cleaning, and preprocessing, to training models and deploying them to production     The ideal candidate will be passionate about artificial intelligence and stay up-to-date with the latest developments in the field           Responsibilities              Understanding business objectives and developing   models that help to achieve them, along with metrics to track their progress          Managing available resources such as hardware, data, and personnel so that deadlines are met          Analyzing the ML algorithms that could be used to solve a given problem and ranking them by their success probability          Exploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect performance when deploying the model in the real world          Verifying data quality, and/or ensuring it via data cleaning          Supervising the data acquisition process if more data is needed          Finding available datasets online that could be used for training          Defining validation strategies          Defining the preprocessing or feature engineering to be done on a given dataset          Defining data augmentation pipelines          Training models and tuning their hyperparameters          Analyzing the errors of the model and designing strategies to overcome them          Deploying models to production          Solve complex problems with multilayered data sets, and optimize existing machine learning libraries and frameworks          Stay up to date with developments in the machine learning industry          Skills              Proficiency with a deep learning framework such as TensorFlow or Keras          Proficiency with Python and basic libraries for machine learning such as scikit-learn and pandas          Familiarity with data structures, data modeling, and software architecture          Expertise in visualizing and manipulating big datasets          Proficiency with OpenCV          Familiarity with Linux          Ability to select hardware to run an ML model with the required latency        ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Training, Linux, Data modeling, Machine learning, Data collection, Data structures, Deployment, Data quality, Hardware, Python",-,9am-6pm,"Full Time, Permanent",Novisync Solutions India,Organization,Novisync Solutions India,-,Visakhapatnam,Visakhapatnam,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Generative AI/ML Developer," ?   1. Architect and develop advanced generative AI solutions leveraging state-of-the-art language models (LLMs) such as GPT, LLaMA, PaLM, BLOOM, and others.       2. Implement frameworks like LangChain, Anthropics Constitutional AI, OpenAIs Whisper, Hugging Face, TensorFlow, PyTorch, and Prompt Engineering techniques to build robust and scalable AI applications.       3. Develop and maintain APIs using Pythons FastAPI, Flask, or Django for integrating AI capabilities into various systems.       4. Design and implement machine learning models, including supervised, unsupervised, and reinforcement learning techniques.       5. Explore and implement cutting-edge techniques like Few-Shot Learning, Reinforcement Learning, Multi-Task Learning, and Transfer Learning for AI model training and fine-tuning.       6. Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions.       7. Stay abreast of the latest advancements in the fields of Generative AI, Machine Learning, and Deep Learning, contributing to the development of new algorithms and techniques.       8. Optimize AI models for performance, efficiency, and scalability, ensuring seamless integration with cloud platforms like AWS, Google Cloud, or Azure.   ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Training, deep learning, Django, Artificial Intelligence, Machine learning, Cloud, AWS, Python",-,9am-6pm,"Full Time, Permanent",Aqusag Technologies India,Organization,Aqusag Technologies India,-,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
TS-Cloud App Developer,"Seeking Cloud App Developer in Bangalore/Pune proficient in Azure, cloud services, and data pipelines. Experience with Kinesis/Kafka/IoT Core, ECS/Fargate/EKS/ECR, and Lambda/S3/Athena. Knowledge in DevOps/CICD be a plus.",50524001028,05-05-2024,03-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Azure, cloud services, data pipelines, Kafka, Kinesis, S3, Eks, IOT, DevOps, CI/CD pipelines, ECS, ECR, fargate, Lambda, Athena",-,9am-6pm,"Full Time, Permanent",Srivango,Organization,Srivango,https://img.naukimg.com/logo_images/groups/v1/3469306.gif,"Pune, Bengaluru","Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure synapse developer,   Skill combination    -      ?     Looking for resources with strong Azure Synapse experience who are self-starters who could immediately jump into the weeds to see what the client has and help to re-architect and develop their warehouses into a single instance.      ?     Looking for well rounded individuals who are looking to bring value added results to the team.         ? ,3.01E+11,30-09-2023,29-12-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Architect, HR Executive, Networking, Manager Technology, Business solutions",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Snowflake Developer,"   We are looking for a Snowflake Developer with 5-10 years of experience to join our team in Bangalore     The ideal candidate should have a strong background in Snowflake and at least 3 years of hands-on experience in this technology     Additionally, proficiency in one of the following: Spark, PySpark, or Snowpark is required     Familiarity with at least 1 cloud platform is also a must         Candidate Qualifications               5-10 years of experience in IT     Expertise in Snowflake with at least 3 years of hands-on experience     Proficiency in one of the following: Spark, PySpark, or Snowpark     Experience with at least 1 cloud platform   ",2.61E+11,26-08-2023,24-11-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Scalability, Networking, spark, Cloud, Manager Technology, Business solutions",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Lead / Developer,"   Google VPC, Cloud compute engine, Cloud App engine, google API gateway, cloud functions, cloud run, load balancing, cloud web hosting, cloud IAM,     Persistent Disks [block storage], Filestore [network file storage], and Cloud Storage [object storage], Cloud SQL , cloud data store,     GCP management & monitoring Tool (Monitoring, Logging , Error Reporting, Trace, Cloud Console)     Cloud deployment manager             Key Skills                  Experience in Terraform, Python / Nodejs / Java, Cloud SDK   ",2.61E+11,26-08-2023,24-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"google api, GCP, Cloud, cloud storage, Web hosting, SDK, Load balancing, Monitoring, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AZURE SRE Developer,"     Primary Skills:     Azure Devops SRE, Infra devops, Snowflake, ARM, anisble, powershell.             ?       Responsibilities:           Contributes to the formulation and maintenance of a SILK disaster recovery and business continuity plan to ensure timely and effective restoration of IT services and business resumption in the event of a disaster.         Provides input on the selection of an Enterprise BCDR solution(s) which meets the cost and schedule constraints and satisfies recoverability, availability and business continuity targets, primarily focused in Azure cloud.         Supports key projects/initiatives as a BC/DR expert to mitigate risk for the business.         Identifies and analyzes the gaps in current BC/DR plans and provides recommendations to close gaps.         Develops and executes runbooks and functional validation workbooks for performing failover, and recovery.         Reviews and provides input to policies, standards, and standard operating procedures.         Performs training for internal IT teams and staff as required to mature BC/DR program.             Skill Required    :           10+ years of overall IT experience focused on Distributed global infrastructure         5+ years of Expertise in conducting Business Impact Analysis, creating Business Continuity & Disaster Recovery Plans, conducting Tabletop Exercises, and developing failover solutions in public cloud (Azure).         Hands on experience with developing automation scripts/ pipelines for the implementation for DR failover and recovery.         Working experience of data replication strategies, virtual platforms, and other technologies deemed vital to recovery and continuity goals         Production experience in Cloud technologies - Azure IaaS, PaaS, networking, Azure functions, Azure automation and runbooks, workbooks, Insights, Security center, Azure Monitor, Log Analytics.         Hands on experience with IaC tools like ADO, ARM, Bicep, ansible, PowerShell, python, azure-cli         Technical and Operational expertise in Windows/Linux /AKS, SQL and N0-SQL DB s, IaaS, PaaS, Data, BCDR, Security, Management, Storage, Networking, Monitoring, Identity and Connectivity         Multi-region and Disaster Recovery experience in Azure public cloud.         Azure Virtual Network, VWAN, Express route, Load Balancer (L4/L7), Traffic Manager, CDN, Azure DNS, routing & routing protocols like BGP, firewall concepts         Azure Governance, Security, Monitoring, Workbooks, Compliance, and cost awareness         Azure Virtual Machines, Containers and/or Kubernetes (infrastructure perspective)         Azure Storage Account, Disk, Snapshot, Backup, Site Recovery, file sync, Data Lake     ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"IT services, Automation, Networking, Linux, DNS, Windows, Operations, Analytics, SQL",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Power BI Developer,"   B.Tech. / B.E. / M.Tech. / M.S. in Computer Science with 4+ yrs of experience.     Experience in design and development of highly -scalable applications and platform development     in product-based companies or R&D divisions.     Very strong experience in Power BI.     Hands-on experience in Developing Power BI Reports and Dashboards.     Experience in creating and deploying parameterized reports using power BI.     Experience in creating hierarchies in Power BI reports using Data visualizations like Bar chart, Line chart, pie charts,     forecast charts.     Experience on publishing Power BI reports of dashboards in Power BI server and Scheduling the dataset to refresh for     upcoming data in power BI server.     Experience in generating Drill through and Drill down reports with Drop down menu option, sorting the data,     and defining subtotals in Power BI.     Expertise in DAX expressions     Exposure to gathering requirements and translating to solutions.     Implemented various basic calculations including Custom Aggregations, Custom Calculated Fields, Date Calculations,     Conditional Filters, Logic and conditional calculations etc.     Created reports with various parameters and input controls for more user flexibility in viewing the reports     Performance oriented design and development of Power Bi Reports.     Exposure to design & development of Power BI Semantic Data models.     Good with Performance turning using Performance analyzer, DAX studio.     CI/CD development environments/tools: Git, Maven, Jenkins, Azure DevOps     Demonstrated ownership for development and design of a few modules/features in a product.     Working knowledge of RDBMS tools like SQL and DB-2 database technologies     Hands on development skills to prototype technical solutions.     Ability to adapt to change quickly, willingness to learn new and emerging technologies     Good communication and interpersonal skills     Knowledge on cloud platforms like Microsoft Azure / Google Cloud Platform     Awareness of Agile (Scrum) methodologies   ",2.31E+11,23-08-2023,21-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Maven, Interpersonal skills, Prototype, Publishing, GIT, RDBMS, power bi, Scheduling, SQL",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
SENIOR CLOUD DEVELOPER,"         Required skills and qualifications               Hands on experience in developing cloud-based applications and distributed systems             3-5 years of experience in architecting, designing, developing, and implementing cloud solutions             Understanding of and experience with the five pillars of a well-architected framework             Experience in several of the following areas: database architecture, ETL, business intelligence, big data, machine learning, advanced analytics             Proven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts               Preferred skills and qualifications               Bachelors degree (or equivalent) in computer science, information technology, or mathematics             Hands on experience any of modern programming languages, standards, and frameworks - Java, Python, GoLang, React, node JS             Knowledge of web services, API, REST, and RPC             Deep knowledge in data storage systems - SQL, NoSQL, File, Block              Cloud Certification (AWS expertise an additional advantage)           ",2.21E+11,22-11-2023,20-02-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Computer science, Web services, Business Analyst, Cloud, Programming, Business intelligence, Information technology, Distribution system, SQL, Python",-,9am-6pm,"Full Time, Permanent",Global Pharma Tek,Organization,Global Pharma Tek,https://img.naukimg.com/logo_images/groups/v1/1012164.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Cloud Developer,"           Develop and implement technical efforts to design, build, and deploy cloud applications, including large-scale data processing, computationally intensive statistical modelling, and advanced analytics             Participate in all aspects of the software development lifecycle for cloud solutions, including planning, requirements, development, testing, and quality assurance             Troubleshoot incidents, identify root causes, fix, and document problems, and implement preventive measures             Educate teams on the implementation of new cloud-based initiatives, providing associated training when necessary             Demonstrate exceptional problem-solving skills, with an ability to see and solve issues before they affect business productivity                 Required skills and qualifications               Hands on experience in developing cloud-based applications and distributed systems             3-5 years of experience in architecting, designing, developing, and implementing cloud solutions             Understanding of and experience with the five pillars of a well-architected framework             Experience in several of the following areas: database architecture, ETL, business intelligence, big data, machine learning, advanced analytics             Proven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts               Preferred skills and qualifications               Bachelor s degree (or equivalent) in computer science, information technology, or mathematics             Hands on experience any of modern programming languages, standards, and frameworks - Java, Python, GoLang, React, node JS             Knowledge of web services, API, REST, and RPC             Deep knowledge in data storage systems - SQL, NoSQL, File, Block              Cloud Certification (AWS expertise an additional advantage)           ",2.21E+11,22-11-2023,20-02-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"advanced analytics, Manager Quality Assurance, Web services, Cloud, Software development life cycle, Business intelligence, Information technology, Distribution system, SQL, Python",-,9am-6pm,"Full Time, Permanent",Global Pharma Tek,Organization,Global Pharma Tek,https://img.naukimg.com/logo_images/groups/v1/1012164.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
SENIOR CLOUD DEVELOPER,"         Hands on experience in developing cloud-based applications and distributed systems             3-5 years of experience in architecting, designing, developing, and implementing cloud solutions             Understanding of and experience with the five pillars of a well-architected framework             Experience in several of the following areas: database architecture, ETL, business intelligence, big data, machine learning, advanced analytics             Proven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts               Preferred skills and qualifications               Bachelors degree (or equivalent) in computer science, information technology, or mathematics             Hands on experience any of modern programming languages, standards, and frameworks - Java, Python, GoLang, React, node JS             Knowledge of web services, API, REST, and RPC             Deep knowledge in data storage systems - SQL, NoSQL, File, Block              Cloud Certification (AWS expertise an additional advantage)         ",2.21E+11,22-09-2023,21-12-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"Web services, Business Analyst, Cloud, Programming, Business intelligence, Information technology, Distribution system, SQL, Python",-,9am-6pm,"Full Time, Permanent",Capleo Global,Organization,Capleo Global,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Lead/Developer,"                         Google VPC, Cloud compute engine, Cloud App engine, google API gateway, cloud functions, cloud run, load balancing, cloud web hosting, cloud IAM,      Persistent Disks [block storage], Filestore [network file storage], and Cloud Storage [object storage], Cloud SQL , cloud data store,      GCP management monitoring Tool (Monitoring, Logging , Error Reporting, Trace, Cloud Console)      Cloud deployment manager                Key Skills                          Experience in Terraform, Python / Nodejs / Java, Cloud SDK                  ",20923500408,02-09-2023,01-12-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"google api, GCP, Cloud, Web hosting, SDK, Load balancing, Business solutions, Monitoring, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : AWS Glue Good to have skills : Python (Programming Language), PySpark Minimum  3  year(s) of experience is required Educational Qualification : 15 years education is must Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance using AWS Glue and other related technologies.  Roles & Responsibilities: Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure using AWS Glue and other related technologies. Deploy infrastructure and platform environments, create a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure the successful delivery of cloud-based solutions. Provide technical guidance and support to team members and stakeholders on cloud-based solutions and related technologies. Professional & Technical Skills: Must To Have Skills:Experience in AWS Glue. Good To Have Skills:Experience in Python (Programming Language), PySpark. Strong understanding of cloud-based solutions and related technologies. Experience in deploying infrastructure and platform environments. Experience in creating a proof of architecture to test architecture viability, security, and performance. Additional Information: The candidate should have a minimum of 3 years of experience in AWS Glue. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful cloud-based solutions. This position is based at our Mumbai office. Qualification 15 years education is must",10524911122,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, pyspark, aws glue, cloud infrastructure, aws, kubernetes, vmware, microsoft azure, networking, matrix, cloud platform, docker, ansible, java, git, gcp, devops, linux, paas, jenkins, mysql, shell scripting, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : AWS Glue Good to have skills : Python (Programming Language), PySpark Minimum  3  year(s) of experience is required Educational Qualification : 15 years education is must Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance using AWS Glue and other related technologies.  Roles & Responsibilities: Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure using AWS Glue and other related technologies. Deploy infrastructure and platform environments, create a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure the successful delivery of cloud-based solutions. Provide technical guidance and support to team members and stakeholders on cloud-based solutions and related technologies. Professional & Technical Skills: Must To Have Skills:Experience in AWS Glue. Good To Have Skills:Experience in Python (Programming Language), PySpark. Strong understanding of cloud-based solutions and related technologies. Experience in deploying infrastructure and platform environments. Experience in creating a proof of architecture to test architecture viability, security, and performance. Additional Information: The candidate should have a minimum of 3 years of experience in AWS Glue. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful cloud-based solutions. This position is based at our Mumbai office. Qualification 15 years education is must",10524910707,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, pyspark, aws glue, cloud infrastructure, aws, kubernetes, vmware, microsoft azure, networking, matrix, cloud platform, docker, ansible, java, git, gcp, devops, linux, paas, jenkins, mysql, shell scripting, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Spring Boot Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 year Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Spring Boot.  Roles & Responsibilities: Collaborate with Integration Architects and Data Architects to design and implement data platform components using Spring Boot. Assist with the development of the data platform blueprint and design, ensuring cohesive integration between systems and data models. Implement and maintain data platform components, ensuring high availability and scalability. Troubleshoot and resolve issues related to data platform components, working closely with cross-functional teams. Stay updated with the latest advancements in data platform technologies and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Proficiency in Spring Boot. Strong understanding of data platform components and their integration with systems and data models. Experience with implementing and maintaining data platform components, ensuring high availability and scalability. Experience with troubleshooting and resolving issues related to data platform components. Solid grasp of data platform technologies and their integration with other systems. Experience with cloud-based data platforms such as AWS or Azure. Additional Information: The candidate should have a minimum of 5 years of experience in Spring Boot. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Mumbai office.",10524909886,01-05-2024,30-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, spring boot, data modeling, troubleshooting, aws, sosl, visualforce, rest, soql, sfdc, machine learning, triggers, dashboards, javascript, computer assembling, apex, salesforce, salesforce crm, data loader, java, installation, html, apex classes",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Technology Platform Engineer,"Project Role : Technology Platform Engineer Project Role Description : Creates production and non-production cloud environments using the proper software tools such as a platform for a project or product. Deploys the automation pipeline and automates environment creation and configuration.  Must have skills : Oracle Procedural Language Extensions to SQL (PLSQL) Good to have skills : Security/Data Provisioning Minimum  3  year(s) of experience is required Educational Qualification : 15 Years full time education Summary :As a Technology Platform Engineer, you will be responsible for creating production and non-production cloud environments using the proper software tools such as a platform for a project or product. Your typical day will involve deploying the automation pipeline and automating environment creation and configuration using Oracle Procedural Language Extensions to SQL (PLSQL).  Roles & Responsibilities: Design, develop, and maintain Oracle PLSQL code for cloud-based applications. Create and maintain cloud environments using automation tools such as Terraform and Ansible. Collaborate with cross-functional teams to ensure the successful deployment of cloud-based applications. Troubleshoot and resolve issues related to cloud-based applications and environments. Professional & Technical Skills: Must To Have Skills:Proficiency in Oracle Procedural Language Extensions to SQL (PLSQL). Good To Have Skills:Experience with Terraform and Ansible. Strong understanding of cloud-based application development and deployment. Experience with containerization technologies such as Docker and Kubernetes. Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform. Additional Information: The candidate should have a minimum of 3 years of experience in Oracle Procedural Language Extensions to SQL (PLSQL). The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful cloud-based solutions. This position is based at our Mumbai office. Qualification 15 Years full time education",10524909341,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, sql, docker, ansible, oracle procedural language, python, oracle, oracle apps technical, microsoft azure, plsql, agriculture, java, gcp, devops, linux, jenkins, terraform, shell scripting, html, aws, xml publisher reports, oracle reports",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Microsoft Azure Modern Data Platform, Databricks Unified Data Analytics Platform, Python (Programming Language) Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform, ensuring cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services, collaborating with Integration Architects and Data Architects, and utilizing your expertise in data platform components.  Roles & Responsibilities: Assist with the blueprint and design of the data platform, encompassing the relevant data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Implement and maintain data platform components using Microsoft Azure Data Services. Develop and maintain data pipelines, ensuring data quality and integrity. Troubleshoot and resolve data platform issues, working with cross-functional teams as needed. Professional & Technical Skills: Must To Have Skills:Expertise in Microsoft Azure Data Services. Good To Have Skills:Experience with Microsoft Azure Modern Data Platform, Databricks Unified Data Analytics Platform, and Python (Programming Language). Strong understanding of data platform components and architecture. Experience with data pipeline development and maintenance. Ability to troubleshoot and resolve data platform issues. Solid grasp of data quality and integrity best practices. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, data engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of education",80524912180,08-05-2024,06-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, python, microsoft azure, data engineering, data quality, hive, data analytics, data warehousing, data architecture, machine learning, sql server, sql, nosql, amazon ec2, java, data modeling, spark, kafka, mysql, hadoop, sqoop, aws, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : SAP CPI for Data Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of fulltime education Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance using SAP CPI for Data Services.  Roles & Responsibilities:1 Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure using SAP CPI for Data Services.2 Deploy infrastructure and platform environments, create a proof of architecture to test architecture viability, security, and performance.3 Collaborate with cross-functional teams to ensure the successful delivery of cloud application solutions.4 Stay updated with the latest advancements in cloud computing and data integration technologies, integrating innovative approaches for sustained competitive advantage.  Professional & Technical Skills:1 Must To Have Skills:Experience in SAP CPI for Data Services.2 Good To Have Skills:No Industry Specialization.3 Strong understanding of cloud computing and data integration technologies.4 Experience in deploying infrastructure and platform environments.5 Experience in creating a proof of architecture to test architecture viability, security, and performance. Professional Attributes :1 Should have good communication skillsEducational qualification :1 Minimum 15 years of fulltime education Qualification Minimum 15 years of fulltime education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloud computing, data services, sap, sap cpi, data integration, kubernetes, networking, docker, ansible, java, git, gcp, devops, linux, paas, jenkins, shell scripting, sap hana, python, vmware, microsoft azure, cloud platform, sap pi, cloud infrastructure, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills : Microsoft Azure Data Services, Microsoft SQL Server Integration Services SSIS Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Should have completed Graduation Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Microsoft Azure Data Services and Microsoft SQL Server Integration Services (SSIS). Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Microsoft Azure Data Services and Microsoft SQL Server Integration Services (SSIS). Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data integration solutions using Microsoft Azure Data Services and Microsoft SQL Server Integration Services (SSIS). Design and implement data storage solutions using Microsoft Azure Data Services. Ensure data quality and integrity by implementing appropriate data validation and cleansing techniques. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services and Microsoft SQL Server Integration Services (SSIS). Must To Have Skills:Strong understanding of data integration and data storage solutions. Good To Have Skills:Experience with other Microsoft Azure services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics. Good To Have Skills:Experience with data visualization tools such as Power BI or Tableau. Good To Have Skills:Knowledge of programming languages such as Python or Java. Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Noida office. Qualification Should have completed Graduation",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, ms sql server administration, microsoft azure, software engineering, data integration, azure databricks, python, azure synapse, power bi, azure data factory, sql server integration services, sql, data quality, tableau, java, data modeling, ssis",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance. Must have skills : Application Packaging, Microsoft 365 Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : BE or Equivalent Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance.  Roles & Responsibilities: Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure. Deploy infrastructure and platform environments. Create a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure successful deployment of cloud application solutions. Professional & Technical Skills: Proficiency in Application Packaging. Experience with Microsoft 365. Strong understanding of cloud infrastructure and platform environments. Experience with cloud application solution deployment. Experience with architecture viability, security, and performance testing. Additional Information: The candidate should have a minimum of 5 years of experience in Application Packaging. The ideal candidate will possess a strong educational background in software engineering, computer science, or a related field, along with a proven track record of delivering impactful cloud application solutions. This position is based at our Gurugram office. Qualification BE or Equivalent",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"application packaging, performance testing, packaging, cloud infrastructure, software engineering, kubernetes, python, vmware, microsoft azure, networking, cloud platform, docker, ansible, git, java, gcp, devops, linux, jenkins, shell scripting, aws, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Microsoft Intune Good to have skills : Cloud Network Operations Minimum  2  year(s) of experience is required Educational Qualification : BE or Equivalent Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance using Microsoft Intune and Cloud Network Operations.  Roles & Responsibilities: Deploy infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance. Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure. Collaborate with cross-functional teams to ensure successful delivery of cloud-based solutions. Provide technical support and troubleshooting for cloud-based solutions. Professional & Technical Skills: Must To Have Skills:Experience in Microsoft Intune. Good To Have Skills:Experience in Cloud Network Operations. Experience in designing, building, testing, and deploying cloud application solutions. Strong understanding of cloud infrastructure and platform environments. Experience in providing technical support and troubleshooting for cloud-based solutions. Additional Information: The candidate should have a minimum of 2 years of experience in Microsoft Intune. The ideal candidate will possess a strong educational background in software engineering or a related field, along with a proven track record of delivering impactful cloud-based solutions. This position is based at our Gurugram office. Qualification BE or Equivalent",80524911348,08-05-2024,06-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"intune, cloud platform, cloud infrastructure, troubleshooting, software engineering, kubernetes, python, vmware, microsoft azure, networking, docker, ansible, git, java, gcp, devops, linux, paas, jenkins, shell scripting, mysql, aws, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google-Cloud Engineer,"Role Description  Senior Engineer is responsible for developing and delivering elements of engineering solutions to accomplish business goals. Awareness is expected of the important engineering principles of the Bank. Root cause analysis skills, developed through addressing enhancements and fixes to products. Build reliability and resiliency into solutions through early testing, peer reviews and automating the delivery lifecycle. Successful candidate should be able to work independently on medium to large sized projects with strict deadlines. Should be able to work in a cross-application, mixed-technical environment and must demonstrate solid hands-on development track record while working on an agile methodology. The role demands working along-side a geographically dispersed team. The position is required as part of the build out of AFC Tech internal development team in India. The overall team will primarily deliver improvements in anti-financial crime capabilities that are major components of the regulatory portfolio, addressing various regulatory commitments to mandated monitors. Your key responsibilities  Analysing data sets and designing and coding stable and scalable data ingestion workflows, also integrating into existing workflows. Working with team members and stakeholders to clarify requirements and provide the appropriate ETL solution. Work as a senior developer for developing analytics algorithm on top of ingested data. Work as a senior developer for various data sourcing in Hadoop, also GCP. Own unit-testing, UAT deployment, End-User sign-off & Prod Go-Live. Ensuring new code is tested, both at unit level and at system level. Design, develop and peer review new code/functionality. Operate as a team member of an Agile Scrum team. Your Skills & Experience  More than 8 years of coding experience in reputed organizations. Hands on experience on BitBucket & Jenkins actions. Proficient in Hadoop, Python, Spark, SQL, Unix, Pentaho, Hive. Basic understanding of On-Prem, Edge & GCP data security. Hands on development experience on large ETL/Big Data systems, GCP being a big plus. Hands on experience on CloudBuild, ArtifactRegistry, CloudDNS, CloudLoadBalancing etc. Hands on experience on DataFlow, CloudComposer, Cloud Storage, Data Proc etc. Basic understanding of Data Quality dimensions like Consistency, Completeness, Accuracy, Lineage etc. Hands on business and systems knowledge gained in a regulatory delivery environment. Banking experience, regulatory and cross-product knowledge. Passionate about test driven development. Data visualisation experience in Tableau.",70524910204,07-05-2024,05-08-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Investment Banking / Venture Capital / Private Equity,"Google Cloud, Unix, Hadoop, Tableau, Cloud Build, Cloud DNS, DataFlow, SQL, Artifact Registry, Cloud Storage, Hive, Cloud Composer, Spark, Pentaho, Python",-,9am-6pm,"Full Time, Permanent",Deutsche Bank,Organization,Deutsche Bank,https://img.naukimg.com/logo_images/groups/v1/468918.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer: Data Platforms,"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe. You'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience. Your Role and Responsibilities As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs. Your primary responsibilities include: Design, build, optimize and support new and existing data models and ETL processes based on our clients business requirements. Build, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization. Coordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too. Required Technical and Professional Expertise Must have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python Hbase, Hive Good to have Aws -S3,  athena ,Dynomo DB, Lambda, Jenkins GIT Developed Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).  Developed Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations..  Preferred Technical and Professional Expertise Understanding of Devops. Experience in building scalable end-to-end data ingestion and processing solutions  Experience with object-oriented and/or functional programming languages, such as Python, Java and Scala""",60524904417,06-05-2024,04-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"dynamo db, pyspark, java, jenkins, hbase, hive, data analysis, oozie, microsoft azure, machine learning, hadoop spark, data engineering, sql server, git, lambda expressions, postgresql, devops, data ingestion, sqoop, aws, etl, mongodb",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer: Data Platforms,"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe. You'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience. Your Role and Responsibilities As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs. Your primary responsibilities include: Design, build, optimize and support new and existing data models and ETL processes based on our clients business requirements. Build, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization. Coordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too. Required Technical and Professional Expertise Must have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python Hbase, Hive Good to have Aws -S3,  athena ,Dynomo DB, Lambda, Jenkins GIT Developed Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).  Developed Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations..  Preferred Technical and Professional Expertise Understanding of Devops. Experience in building scalable end-to-end data ingestion and processing solutions  Experience with object-oriented and/or functional programming languages, such as Python, Java and Scala""",60524902243,06-05-2024,04-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"scala, spark, hadoop, big data, python, hive, pyspark, hadoop spark, git, java, postgresql, devops, jenkins, data ingestion, etl, mongodb, hbase, data analysis, dynamo db, oozie, microsoft azure, machine learning, data engineering, sql server, lambda expressions, sqoop, aws",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Engineer: Data Platforms,"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe. You'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat. Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting. In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience. Your Role and Responsibilities As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs. Your primary responsibilities include: Design, build, optimize and support new and existing data models and ETL processes based on our clients business requirements. Build, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization. Coordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too. Required Technical and Professional Expertise Must have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python Hbase, Hive Good to have Aws -S3,  athena ,Dynomo DB, Lambda, Jenkins GIT Developed Python and pyspark programs for data analysis.. Good working experience with python to develop Custom Framework for generating of rules (just like rules engine).  Developed Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations..  Preferred Technical and Professional Expertise Understanding of Devops. Experience in building scalable end-to-end data ingestion and processing solutions  Experience with object-oriented and/or functional programming languages, such as Python, Java and Scala""",60524902142,06-05-2024,04-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"scala, spark, hadoop, big data, python, hive, pyspark, hadoop spark, java, git, postgresql, devops, jenkins, data ingestion, etl, mongodb, hbase, data analysis, dynamo db, oozie, microsoft azure, machine learning, data engineering, sql server, lambda expressions, sqoop, aws",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Principal Data Platforms Engineer,"       Design and Implementation of technical integrations to support the Observability solutions at SPGI         Design and implementation of data pipeline automations across multi cloud technologies         Evaluation, testing and implementation of new innovative product features to help drive value amongst internal stakeholders         Automation and scripting to ensure robust and efficient solution delivery         Creation of detailed documentation and architectural diagrams and the presentation of the solution to peers and stakeholders                 What We re Looking For:                 Basic Required Qualifications:             Minimum Bachelors degree in a technical subject         Cloud (AWS preferred) certification         Proven scripting / coding experience in at least one of: Python / Ruby / Powershell / Perl         Experience working with code repositories (eg git) and CI/CD pipelines                 Soft skills required:             Ability to present ideas and designs to both technical and non-technical audiences at all levels         Strong written and verbal communication skills         Strong documentation skills                 Additional Preferred Qualifications       :           Experience working in an Agile (Scrum) team         Experience with Terraform         Experience with Enterprise Observability tools and platforms       ",30524500891,03-05-2024,01-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Financial Services,"Supply chain, Automation, Architecture, Coding, Workflow, Wellness, Perl, HTTP, Ruby, Python",-,9am-6pm,"Full Time, Permanent",Osttra,Organization,Osttra,-,"Noida, Hyderabad, Gurugram","Noida, Hyderabad, Gurugram",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Lead Software Engineer (Python),"                 ?       Minimum 7 to 12 years of software development/engineering experience is required     Strong computer science fundamentals in Data structures and algorithm design     Experience with designing software systems and strong understanding of Django / Flask     Experience in using public cloud services, e.g. AWS     Experience with Container technologies such as Docker, AWS ECS (EC2, S3, Lambda, DynamoDB, API Gate)             Nice to have             Good to have exposure on Asynchronus Programming (asyncio / aiohttp)             Technologies             Python + AWS Developer     Python 3     OOP     API Development(Django / Flask / FastAPI)     Good to have exposure on Asynchronus Programming (asyncio / aiohttp)     AWS (Intermediate / Advanced knowledge is a must)     AWS Services - EC2, IAM, VPC, RDS, EBS, Aurora, Lambda, API Gateway, SNS, SQS, Cloudwatch, DynamoDB     Databases (MySQL or Postgres)     Unit Testing     Git     Docker / Terraform                 ",2.60E+11,26-03-2024,24-06-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SAP, GIT, MySQL, Data structures, Silicon, Unit testing, Adobe, Open source, Python",-,9am-6pm,"Full Time, Permanent",Pradeepit Consulting Services,Organization,Pradeepit Consulting Services,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer - (Data Platform Team),"As an Sr Software Engineer here you are passionate about using software-based approaches to solve complex data-driven challenges and automate those solutions Within our organization, youll lead efforts aimed at scaling our existing data offerings and establish the technical strategy for how we can better equip engineers and leaders with Data Platform Youll build a deep understanding of our digital streaming service and use that knowledge, coupled with your engineering, infrastructure, data, and cloud knowledge, to optimize and evolve how we understand our technical ecosystem To be successful, youll need to be deeply technical and capable of holding your own with other strong peers You possess excellent collaboration and diplomacy skills You have experience practicing infrastructure-as-code, data lake management, AI/ML Knowledge, and Analytics In addition, youll have strong systems knowledge and troubleshooting abilities Primarily responsible for administrating Open Source, Snowflake and Databricks environments Identify, tune, and fix the performance issues on priority. Implement security, roles, users and privileges using the best practices. Provide database/objects recovery as need on basis. Design, identify and implement high performing and highly available data warehouse solutions. Create Snowflake/Databricks utilization and capacity/cost plans, identifies, and provide solution for abnormalities. Proactively identify, design, and implement process improvements like automating manual processes, optimizing data flow, increase scalability etc. Stay up-to-date with the latest features and best practices in Snowflake/Databricks administration Provide 24X7 On-call support by participating in rotation schedule. What to Bring : Bachelor's degree in computer science, information systems, or information technology. 5 + years of experience as a database administrator. Minimum 3+ years of hands-on Distributed Systems.  Minimum 5+ years of hands-on administration experience, from setting up the Databricks/Snowflake environments to successfully administering it (AWS preferred) Experience with the various database platforms Snowflake, Databricks and NoSQL. Experience with GitHub, GitHub Actions and Workflows. Experience with Optimization Queries.  Experience automating, scripting, and streamlining processes for efficiency and accuracy utilizing, Python etc. Some experience with AWS with knowledge of S3, EC2, IAM, Security, Lambda, Networking etc. Ability and experience with the development of processes and procedures to standardize Database installations and configuration. Extensive experience with implementation and maintenance of Disaster Recovery and High availability. Ability to work on unusually complex technical problems and provide solutions that are highly innovative and ingenious. Ability to provide technical documentation and project plans for technical staff members. Excellent communication, presentation, and customer relationship skills. Excellent organizational and time management skills to handle multiple tasks simultaneously Preferred Experience: Experience with Looker, Terraform, and Kafka is plus.",1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,TV / Radio,"Senior Software Engineer, S3, Networking, Workflows, GitHub, AWS services, Security, GitHub Actions, IAM, EC2, NoSQL experience, Snowflake, Databricks, AWS, Lambda, Query optimization, Python",-,9am-6pm,"Full Time, Permanent",Discovery Communications,Organization,Discovery Communications,https://www.naukri.com/hotjobs/images/v3/WARNER_Oct22.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Engineering IC3,"     Are you interested in working for one of the most exciting fast-growing teams in MicrosoftThen look at the Microsoft Defender Platform Engineering team.            ?       You will be building cloud solutions meeting scales that few companies in industry are required to support, that leverage state-of-the-art technologies to deliver holistic protection to a user base of 10 s of thousands of States level organizations, enterprises, and businesses around the globe. The        Microsoft Defender Platform Engineering team is responsible for delivering a constantly evolving set of services and solutions to meet the challenging landscape of our ever-evolving attackers.           Bachelors Degree in Computer Science or related technical field AND 2+ years technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python OR equivalent experience.          Proven experience with 2+ years working on Kubernetes.          Proven experience with 5+ years of Linux hands on system experience.   Significant hands-on experience in of one or more of the following programming languages: C#, Python, Go(or other equivalent) including Object-Oriented programming concepts.          Mastering CI\CD concepts and hands on implementations experiences, specifically GIT. Proven experience with 5+ years as a Platform Engineer.          Proven experience with building services, supportability, and monitoring Infrastructures.         Ability to manage and deliver multiple project phases at the same time. Strong analytical and problem solving and organizational skills.         Excellent written and oral communication skills         Ability to deal with the ambiguity associated with working in a fast-paced and changing environment. Leadership skills: Sound problem resolution, judgment, negotiating and decision-making skills             If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form .       ?     ?               The Platform Engineering team provides leadership, direction and accountability for application architecture, Cloud design, Infrastructures development and end to end implementation. You will independently determine and develop architectural approaches and Infrastructure solutions, and operate our production services. Strong collaboration skills will be required to work closely with other engineering teams to ensure services/systems are highly stable and performant and meet the expectations of internal and external customers and users.     ",1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,Software Product,"Computer science, C++, GIT, Linux, Coding, Javascript, application architecture, microsoft, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Microsoft,Organization,Microsoft,https://img.naukimg.com/logo_images/groups/v1/614576.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Staff Software Engineer - Backend,"Has a track record of having built multiple high-performance, stable, scalable systems that have been successfully shipped to customers in production. Drives best practices and sets standards for the team. Is a key influencer in the teams strategy and contributes significantly to the team planning. Shows good judgment making trade-offs between immediate and long-term business needs. Is a result-driven creative thinker who drives innovation and produces delightful experiences for our customers. Is an advocate for data-driven decision-making, has an insatiable curiosity, and loves to invent and innovate to solve difficult problems. Has a strong point of view but remains open-minded. Takes ownership of their work and consistently delivers results in a fast-paced environment. You are a collaborative leader that makes other engineers and team members around you more productive, by sharing your knowledge and helping to tie-break key technical decisions. You play a leading role in designing and developing major functional changes to existing software systems, or new ones, involving yourself and other engineers. You are a key influencer in your teams strategy and contribute significantly to team planning. You show good judgement making trade-offs between immediate and long-term business needs. You troubleshoot a production issue by reviewing source code, logs, operational metrics, stack trace, etc. to pinpoint a specific problem and then resolve it. You identify root causes and identify learnings to improve both development processes and system design. Provide guidance on design, coding, and operational best practices. You propose and create best practices proactively where none exist. You make high-impact decisions driving how and what software gets built. You mentor junior engineers, overseeing their designs, code quality, and integration into a team. Your success is judged as much on your own productivity as on the positive impact you have on the engineers around you. What to Bring : Proficient in Java or other JVM languages. 9-13 Years of total experience and should have experience in Java and a few other languages. Experience and deep understanding of Docker, Kubernetes, and AWS. Great understanding of distributed systems challenges, micro-service-based architectures, and asynchronized communication (e.g. using gRPC and Kafka). Ability to implement alerting, metrics, and logging using tools like Prometheus, CloudWatch, Kibana, PagerDuty. Practical knowledge of persistence and caching solutions such as PostgreSQL, Redis, ElasticSearch, Caffeine. Familiar with asynchronous, non-blocking, functional/reactive styles of programming. Hands-on experience with frameworks such as Spring WebFlux, Vert.x, Node.js.",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,108,Engineering - Software & QA,Data Platform Engineer,TV / Radio,"JAVA, kubernetes, algorithms, load runner, redis, docker, spring, elastic search, coding, git, postgresql, linux, jenkins, mysql, html, data structures, prometheus, kibana, mongodb, amazon cloudwatch, jira, jvm, python, pagerduty, grpc, javascript, node.js, kafka, aws",-,9am-6pm,"Full Time, Permanent",Discovery Communications,Organization,Discovery Communications,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer - Security,"   The Security Development team builds and maintains critical security services such as Detection and Response Automation, Secrets Management, and Identity and Access Management.                  We provide the necessary libraries, frameworks, and services to help our engineers build securely.      About the Role    As a Senior Software Engineer on the Security Development team you will:        Design and implement authentication and authorization services for the IAM platform.      Write and maintain IAM platform documentation.      Build secure-by-default libraries and frameworks that help standardize security in our codebases.      Contribute to the monitoring and stability of the teams existing services.      Work cross-functionally with Engineering, Product, Security, and IT stakeholders to build solutions that balance requirements.      Skills Needed      Bachelors degree in Computer Science, Information Security, or a related field      8+ years of experience building new backend services in a high-level programming language (e.g., Golang, Python).      Experience implementing authentication and authorization services to a standard such as OAuth2, OpenID Connect (OIDC), or SAML.      Deep understanding for what high quality systems are made of across application security, performance, testing, documentation, and operational excellence.      A sense of ownership in what you ship.      Preference towards automating busy work in testing, workflows, and operations tasks.      Ability to communicate complex topics in an effective and concise manner.      Bonus points if      Experience building microservice-oriented architectures on a Cloud computing platform (e.g., AWS EKS, GKE, Kubernetes)      Experience with Infrastructure as Code / Platform-as-a-Service (e.g., Terraform, Kubernetes)      Passion for security at work and outside of work (e.g., presenting at security conferences, contributing/creating open source security tools, etc).    ",91023500966,09-10-2023,07-01-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Internet,"Computer science, Cloud computing, Automation, Access management, Information security, Performance testing, Application security, Open source, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Open Door,Organization,Open Door,https://img.naukimg.com/logo_images/groups/v1/642944.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer (Backend),"   Software development in Python and its associated ecosystem (Flask/Django)     Microservices and RESTful APIs: implementation and consumption     Conceptual knowledge of distributed systems -- clustering, asynchronous messaging, streaming, scalability & performance, data consistency, and high availability.     Experience with building SaaS will be a plus     Good understanding of databases (relational, NoSQL) and caching     Understanding distributed event store and streaming ( Kafka)     Experience with cloud-native platforms like Kubernetes will be a big plus     Terraform and Ansible is a plus         Education & Experience         Bachelor's degree (or higher) in Computer Science from a reputed university     At least 6+ years of experience in backend software development, in product companies or tech start-ups.   ",51023501303,05-10-2023,03-01-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Backend, NoSQL, Scalability, SAAS, Cloud, UPS, Distribution system, Python, microservices",-,9am-6pm,"Full Time, Permanent",Breachlock,Organization,Breachlock,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Senior Software Engineer (Python, GCP)","       We are seeking a talented software developer with a strong background in Python, Google Cloud Platform (GCP), and Kafka to join our team. The ideal candidate will have 4+ years of experience in software development, with a focus on building scalable and efficient solutions.                         ?           Job Roles and Responsibilities:                     4+ years of experience in software development with proficiency in Python, GCP, and Kafka.                 Design, develop, test, and deploy high-quality software solutions using Python, GCP, and Kafka.                 Optimize and refactor code to improve performance and maintainability.                 Troubleshoot and debug issues to ensure smooth system operation.                 Strong understanding of software development methodologies and best practices.                 Experience with cloud-based technologies and services, preferably GCP.                 Experience with containerization technologies like Docker and Kubernetes.                 Familiarity with DevOps practices and tools.                 Knowledge of data engineering concepts and tools.                 Knowledge of Kafka for building real-time streaming applications.                 Excellent problem-solving and analytical skills.                 Ability to work independently and collaboratively in a team environment.                 Strong communication skills and ability to effectively interact with stakeholders.                     Collaborate with cross-functional teams to gather requirements and ensure successful project delivery.                 Stay updated with the latest technologies and best practices in software development.         ",50424502368,05-04-2024,04-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Software Product,"Analytical skills, Software development methodologies, GCP, devops, Cloud, Debugging, Project delivery, Troubleshooting, Software solutions, Python",-,9am-6pm,"Full Time, Permanent",Quovantis,Organization,Quovantis,https://img.naukimg.com/logo_images/groups/v1/1281168.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Kafka Developer- Job Code,"     Design, develop, and deploy Python applications and services that leverage Kafka for real-time data processing and streaming.          Integrate Kafka into existing systems and develop custom Kafka connectors as needed.          Collaborate with software developers and DevOps engineers to design and implement scalable and fault-tolerant Kafka clusters.          Optimize Kafka configurations, performance tuning, and troubleshooting to ensure reliable and efficient message processing.          Develop monitoring and alerting solutions to proactively identify and resolve Kafka-related issues.      ",90524500379,09-05-2024,07-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Performance tuning, devops, Data processing, Deployment, Troubleshooting, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Uvj Technologies,Organization,Uvj Technologies,-,Kochi,Kochi,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Senior Developer,"  We are seeking a seasoned Data Platform Senior Developer Position with a robust background in Java, GCP Platform (like BigQuery, Google Cloud Storage, Cloud Jobs, Compute Engine, Pub/Sub etc), JMS, Spring Framework and AWS components to build and develop cloud platforms applications and development initiatives.This is an excellent opportunity to be part of a team based out of Gurgaon and to work with colleagues across multiple regions globally. Responsibilities: Lead the design, implementation, and management of enterprise-grade Data based usage and date platform using Java, GCP, AWS, JMS and other leading technologies. Architect high-performing distributed systems that are scalable and resilient, employing modern SRE (Site Reliability Engineering) practices to ensure system reliability and performance. Develop and oversee a comprehensive Data Platform strategy that includes lifecycle management, governance, and monetization. Champion Data Platform standardization, reusable code, and system interoperability across various business units. Drive the adoption of Data Platform led connectivity to support the integration of disparate systems and building a centralized data source. Mentor and manage a team of engineers and developers, fostering a culture of technical excellence and innovation. Enhance API security protocols and frameworks, ensuring robust security measures are in place to protect sensitive data and systems. Lead SRE teams to implement automation tools, continuous integration/continuous deployment (CI/CD) processes, and real-time monitoring systems. Stay ahead of industry trends and developments in Data Platform technologies, distributed systems, and enterprise architecture.   What Were Looking For:  Bachelor?? or Master?? degree in Computer Science, Information Technology, Engineering, or a related field. 10-15 years of professional experience in software development with a focus on Java, AWS/GCP, and distributed system architectures. Strong knowledge of SRE principles and practices including automation, data management, and systems monitoring. Experience with modern cloud services (AWS / Azure / GCP) and implementing enterprise-scale cloud-native applications. Proficient in engaging with senior stakeholders and translating technical details into business-friendly language. Outstanding problem-solving, strategic thinking, and negotiation skills.",90524006881,09-05-2024,07-08-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,FinTech / Payments,"Java, Cloud, Data Management, Azure, GCP, AWS",-,9am-6pm,"Full Time, Permanent",EDGE Executive Search,Organization,EDGE Executive Search,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : PySpark Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : Graduate Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using PySpark.  Roles & Responsibilities: Collaborate with Integration Architects and Data Architects to design and implement data platform components using PySpark. Assist with the development and deployment of data pipelines and ETL processes. Ensure data quality and integrity by implementing data validation and cleansing techniques. Optimize data processing and storage using PySpark and other Big Data technologies. Troubleshoot and resolve data platform issues, working closely with cross-functional teams. Professional & Technical Skills: Must To Have Skills:Proficiency in PySpark. Good To Have Skills:Experience with Big Data technologies such as Hadoop, Hive, and Spark. Strong understanding of data modeling and database design principles. Experience with data pipeline development and ETL processes. Familiarity with data validation and cleansing techniques. Solid grasp of distributed computing principles and parallel processing. Additional Information: The candidate should have a minimum of 7.5 years of experience in PySpark. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Mumbai office. Qualification Graduate",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pyspark, database design, data modeling, design principles, parallel computing, hive, algorithms, python, c++, data validation, data pipeline, machine learning, distributed computing, sql, data quality, java, spark, data structures, hadoop, big data, etl, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Apache Spark Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : Graduate Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and working with Apache Spark to develop and maintain the data platform.  Roles & Responsibilities: Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Assist with the blueprint and design of the data platform components. Develop and maintain the data platform using Apache Spark. Troubleshoot and debug issues with the data platform. Ensure the data platform meets the required performance and scalability standards. Professional & Technical Skills: Must To Have Skills:Experience with Apache Spark. Good To Have Skills:Experience with Hadoop, Hive, and other big data technologies. Strong understanding of data modeling and database design principles. Experience with data integration and ETL processes. Experience with cloud-based data platforms such as AWS or Azure. Additional Information: The candidate should have a minimum of 7.5 years of experience in Apache Spark. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Mumbai office. Qualification Graduate",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"database design, data modeling, spark, design principles, data integration, hive, c++, hibernate, microservices, spring, java, design patterns, oops, data structures, multithreading, hadoop, mvc, etl, big data, c#, rest, microsoft azure, sql server, javascript, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Technology Platform Engineer,"Project Role : Technology Platform Engineer Project Role Description : Creates production and non-production cloud environments using the proper software tools such as a platform for a project or product. Deploys the automation pipeline and automates environment creation and configuration.  Must have skills : Oracle Procedural Language Extensions to SQL (PLSQL) Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 years Full time education Summary :As a Technology Platform Engineer, you will be responsible for creating production and non-production cloud environments using the proper software tools such as a platform for a project or product. Your typical day will involve deploying the automation pipeline and automating environment creation and configuration using Oracle Procedural Language Extensions to SQL (PLSQL).  Roles & Responsibilities: Design, develop, and maintain Oracle PLSQL code for cloud-based applications. Create and maintain cloud environments using automation tools such as Terraform and Ansible. Collaborate with cross-functional teams to ensure the successful deployment of cloud-based applications. Troubleshoot and resolve issues related to cloud-based applications and environments. Professional & Technical Skills: Must To Have Skills:Proficiency in Oracle Procedural Language Extensions to SQL (PLSQL). Good To Have Skills:Experience with Terraform and Ansible. Strong understanding of cloud-based application development and deployment. Experience with containerization technologies such as Docker and Kubernetes. Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform. Additional Information: The candidate should have a minimum of 3 years of experience in Oracle Procedural Language Extensions to SQL (PLSQL). The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful cloud-based solutions. This position is based at our Mumbai office. Qualification 15 years Full time education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, microsoft azure, sql, terraform, oracle procedural language, python, oracle, oracle apps technical, docker, ansible, plsql, agriculture, java, gcp, devops, linux, jenkins, shell scripting, html, aws, xml publisher reports, oracle reports",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Data Bricks ( ADB ) professional,"Experience with Azure Data Bricks, Data Factory Experience with Azure Data components such as Azure SQL Database, Azure SQL Warehouse, SYNAPSE Analytics Experience in Python/ Pyspark Programming. Experience with Azure Databricks/ ADB Experience with building CI/CD pipelines in Data environments Primary Skills Azure Data Bricks (ADB) Python, Pyspark, Scala Azure SQL Azure Data Factory Secondary Skills Excellent verbal and written communication and interpersonal skills Ability to work independently and within a team environment.",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Azure Data Bricks, Azure Data Factory, Pyspark, Scala, CI/CD, SQL, Python",-,9am-6pm,"Full Time, Permanent",Capgemini,Organization,Capgemini,https://www.naukri.com/hotjobs/images/v3/captech_jun20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Collibra Data Governance-Data Platform Engineer,"Project Role :Data Platform Engineer  Project Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills :Collibra Data Governance  Good to have skills :Cassandra Database Minimum 10+ year(s) of experience is required  Educational Qualification :Bachelors degree in Computer Science,IT Key Reponsibilities :1Knowledge of Collibra operating model, workflow BPMN development, and how to integrate various applications or systems with Collibra Good communication 2Design of Data Governance Organization including steering committee, data governance office, stewardship layer and other working groups 3Setup people and processes including relevant roles,responsibilities and controls, data ownership,workflows and common processes Technical Experience : 1Working exp of Collibra operating model, Custom workflow BPMN development using groovy, and how to integrate various applications or systems with Collibra 2Develop and configure all Collibra customized workflows 3Develop API REST, SOAP to expose the metadata functionalities to the end users Professional Attributes :1 Be a self starter and a fast learner 2 Possess strong problem solving skills with the ability to methodically analyze and resolve tech challenges 3 Possess strong written, verbal,comm,analyitical,tech,inter personal and presentation skills 4 Excellent exp in interaction and handling of clients10to12yrs of exp in DataQuality,Governance,Collibra Solution Architect Cert will be an added quality",1.30E+11,13-04-2024,12-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"presentation skills, cassandra database, groovy, cassandra, data governance, rest, metadata, python, css, eda, maven, manual testing, api rest, openerp, sql server, sqlalchemy, sql, java, seaborn, data modeling, postgresql, mysql, soap, third party integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
S&C Global Network - AI - Risk Analytics - Quantexa - Consultant,"Entity :- Accenture Strategy & Consulting Team :- Global Network Data & AI Practice :- CFO & EV - Risk and Compliance Analytics Title :- Specialist / Sr Analyst - I&F Decision Sci Practitioner Job location :- Bangalore, Chennai, Gurgaon, Mumbai, Hyderabad, Pune Job  Summary Key Responsibilities: Working with clients to solve business problems in the area of fraud, compliance and financial crime Use leading open source big-data tools, such as Spark, Hadoop, Scala and Elasticsearch. You should be comfortable with working with high profile clients, on their sites. Managing, transforming and cleansing high volume data into Quantexa Entities and Networks Writing defensive, fault tolerant and efficient code for data processing Developing scoring algorithms to identify high risk activities Using emerging and open source technologies such as Spark, Hadoop, and Scala Implementing systems delivering automated data ingestion, scoring and alert generation Implementing deployment pipelines using modern CI/CD and deployment technologies such as Jenkins/Bamboo/Kubernetes/Openshift Presenting project results to clients Qualification In order to excel in this position , Must have Experience:- Overall, 4-7 years of experience in Risk and Compliance Analytics domain Must have at least recent 1 years of experience in applying Quantexa in Financial domain. Certification in Quantexa is highly preferred. Education:- BE/B.Tech in Engineering/Computer Science or relevant domain Proven big data experience, either from an implementation or a data science prospective, Excellent technical skills including expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch. Experience of building data processing pipelines for use in production hands off batch systems, including either (or preferably both) traditional ETL pipelines and/or analytics pipelines. Strong coding experience in the likes of Scala, Java or Python, Strong client facing, communication and presentation skills, Enthusiasm to learn and develop emerging technologies and techniques. Exhibit strong technical communication skills with demonstrable experience of working in rapidly changing client environments. Demonstrate strong analytical and problem-solving skills and the ability to debug and solve technical challenges with sometimes unfamiliar technologies.",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, scala, presentation skills, coding, java, continuous integration, kubernetes, rest, openshift, microsoft azure, business intelligence, azure devops, sql, microservices, bamboo, pipeline, elastic search, etl pipelines, spark, jenkins, hadoop, etl, finance",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,"Mumbai, Gurugram, Chennai","Mumbai, Gurugram, Chennai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Pyspark & Databricks certified Developer,"             ?       Designing and implementing data ingestion pipelines from multiple sources using Azure Databricks     Developing scalable and re-usable frameworks for ingesting of data sets     Integrating the end to end data pipleline - to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times     Working with event based / streaming technologies to ingest and process data     Working with other members of the project team to support delivery of additional project components (API interfaces, Search)     Evaluating the performance and applicability of multiple tools against customer requirements                 Requirements         Bachelors degree and at least one year of experience designing, developing, deploying and/or supporting data pipelines using Databricks     Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS     Hands on experience in performance tuning and optimizing code running in Databricks environment     Proficient in programming languages like Pyspark and Python     Good understanding of SQL, T-SQL and/or PL/SQL     Demonstrated analytical and problem-solving skills particularly those that apply to a big data environment     Willingness to work on-site or remote, as needed                     ",2.60E+11,26-03-2024,24-06-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Performance tuning, SAP, Analytical, PLSQL, Silicon, Open source, Adobe, big data, Python, Salesforce",-,9am-6pm,"Full Time, Permanent",Pradeepit Consulting Services,Organization,Pradeepit Consulting Services,-,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Asterisk Developer," Role :- Senior Asterix Product Developer Experience: 8 Years plus  Location: Mumbai CTC : 40 LPA Brief: We're need to hire for a candidate, for the VOIP/Asterisk/VOICE/Video/Collaboration Applications, Person who has worked on scaling the Asterisk platform and have hands-on experience Ideal candidate who has worked on development of Product like CCAAS and have experience in integrating the same other services Roles and Responsibilities 1. Diagnose and troubleshoot problems raised by customers and internal users. 2. Take complete ownership of supporting customer VoIP deployments by debugging issues and providing fixes. 3. Provide resolution through system configuration or data updates. 4. Supporting VoIP services/ applications on Linux & Windows platform 5. Ensuring Applications and Client Use Cases are working as per regular test cases. Automating manual tasks related to application monitoring.. 6. Expertise in SIP call flow analysis and debugging 7. Experience in debugging Asterisk based applications is a must Skill Set: Scripting: Perl, Python, Shell SQL: Ability to write and execute SQL queries. Computers: Linux GNU utilities, TCP/IP, Simple Routing Telecom: SIP, SBC, VoIP, Asterisk & Wireshark ",1.30E+11,13-04-2024,12-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,BPO / Call Centre,"Asterisk, IP Telephony, Dialer, Cisco VOIP, VOIP, Avaya",-,9am-6pm,"Full Time, Permanent",Eureka Outsourcing Solutions,Organization,Eureka Outsourcing Solutions,https://img.naukimg.com/logo_images/groups/v1/3234710.gif,"Thane, Mumbai (All Areas)","Thane, Mumbai (All Areas)",-,-,-,25-40 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google Cloud Full Stack Developer,"Responsible fordesign, development, implementation, operation improvement, debug cloudenvironments in GCP & Cloud Management Platform. Performs engineering design evaluations for new environment builds,Architect,implement Required Candidate profile 2-8 years of cloud-based development experience (GCPPreferable) GCPCertification - Associate Cloud Engineer/Professional Cloud Architect/Professional Cloud Developer Knowledge in Cloudplatforms: GCP",10524906298,01-05-2024,30-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Film / Music / Entertainment,"Google Cloud, GKE, Composer, BigTable, GCP SKD, GCE, VPC, GAE, Dataproc, iOS, Android, Cloud SQL, GCS, JavaScript, Dataflow",-,9am-6pm,"Full Time, Permanent",Pivotroots Digital,Organization,Pivotroots Digital,-,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Cloud Platform Developers," Total development experience 10 to 15 years.      Development Experience on AWS at least 3 years (MUST)      AWS Certification will be an added advantage.        Requirement:          Excellent communication skills, ability to state complex subjects simply for a variety of audiences          Self-starter who can come up to speed quickly and identify the problems that need to be solved          Experience on event driven serverless microservices architecture will have an added advantage          Technology stack          Proficiency in one or more areas: Python, Java, Angular.js, Node.js and Typescript          Experience with serverless AWS databases: Aurora and Dynamo DB          Experience on AWS Visual Step functions in association with Lambda and AWS SNS, SQS          Conversant with working on DevOps platforms preferably on AWS DevOps tools          Monitoring and Tracking using AWS Cloud Watch, Cloud Trail and Config          Cloud application development with hands-on experience working with cloud architecture          Hands on experience with building application using AWS services such as ECS, EKS, EC2, S3.          Knowledge of serverless architecture and microservices development          REST Web Services development          Duties and Responsibilities:          Should be able to work in Agile methodology.          Should be able to deliver quality code.          Should be able to code and test the code.          Keep up to the technology trends and drive the state-of-the-art solutions.          Build prototypes/POCs for the proposed solutions and provide technical leadership to the teams in designing and implementing the solutions          Assist management with estimations and dependency/risk analysis    ",1.80E+11,18-03-2024,16-06-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Architecture, Pharma, Cloud, Technical leadership, Life sciences, Application development, Risk analysis, AWS, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Silverline Technologies,Organization,Silverline Technologies,-,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Java Developer,"   Contribute to all stages of software development lifecycle      Design, implement and maintain Java-based applications that can be high-volume and low-latency      Analyze user requirements to define business objectives      Envisioning system features and functionality      Define application objectives and functionality      Ensure application designs conform with business goals      Develop and test software      Identify and resolve any technical issues arising      Create detailed design documentation      Propose changes to current Java infrastructure      Develop technical designs for application development      Develop multimedia applications      Write well designed, testable code      Conducting software analysis, programming, testing, and debugging      Manage Java and Java EE application development      Develop documentation to help users      Transforming requirements into stipulations      Prepare and produce releases of software components      Support continuous improvement, investigating alternatives and technologies, and presenting for architectural review    ",31023500478,03-10-2023,01-01-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Java EE, Architecture, Debugging, Software development life cycle, Investigation, Programming, Application development, Management, Continuous improvement, Testing",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Python (Programming Language), Microsoft Azure Analytics Services, Synapse Minimum  5  year(s) of experience is required Educational Qualification : Min 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services and other relevant data platform components.  Roles & Responsibilities: Assist with the design and implementation of data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines and ETL processes to support data integration and transformation. Implement data security and privacy measures to ensure compliance with regulatory requirements. Troubleshoot and resolve data platform issues, working with cross-functional teams to identify and resolve root causes. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Proficiency in Python (Programming Language), Microsoft Azure Analytics Services, and Synapse. Strong understanding of data platform components and architecture. Experience with data integration and transformation using ETL processes. Knowledge of data security and privacy regulations and best practices. Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Hyderabad office.",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, data services, analytics services, information technology, microsoft azure, hive, c#, azure data lake, azure analytics, power bi, data warehousing, data pipeline, azure data factory, machine learning, sql server, data modeling, etl, ssis, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Lead,"   Nihilent Technologies Pvt Ltd is looking for Python Lead to join our dynamic team and embark on a rewarding career journey     The Python Lead will be responsible for leading a group of Python developers, ensuring the successful completion of Python projects, and contributing to the development of Python-based applications and systems     Responsibilities:Team Leadership: Lead, mentor, and inspire a team of Python developers, providing guidance, support, and technical expertise     Project Management: Manage Python projects from inception to completion, ensuring deadlines are met, and the projects are delivered successfully     Code Review: Conduct regular code reviews, ensuring coding standards, best practices, and quality are maintained     Software Development: Participate in the design, development, and maintenance of Python-based applications and systems     Collaboration: Collaborate with cross-functional teams, including product managers, designers, and quality assurance, to deliver high-quality software products     Technical Expertise: Stay up-to-date with the latest Python developments and trends and apply this knowledge to improve the development process     Problem Solving: Analyze complex technical problems, propose solutions, and lead the implementation of those solutions     Documentation: Maintain and update documentation related to Python code, architecture, and project specifications     Performance Optimization: Identify and resolve performance bottlenecks and issues in Python applications     Code Testing: Implement and oversee effective testing procedures, including unit testing and integration testing   ",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, software development, unit testing, engine calibration, python development, problem solving, engine testing, sql server, sql, coding, emission, java, integration testing, code review, html, performance optimization, engine development, architecture",-,9am-6pm,"Full Time, Permanent",Nihilent,Organization,Nihilent,https://img.naukimg.com/logo_images/groups/v1/675066.gif,"Kolkata, Pune, Chennai","Kolkata, Pune, Chennai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Python (Programming Language) Good to have skills : Ansible on Microsoft Azure Minimum  3  year(s) of experience is required Educational Qualification : A:Btech or equivalent br/>Key Responsibilities :1:Drive the code conversion effort from legacy system to target state system 2:Modify and convert existing legacy jobs and schedules into modern Apache airflow jobs 3:Create new ingestion framework to support ingestion from various type of sources like oracle, Teradata, db2, sql server, etc 4:Develop framework for tracking data lineage, ensuring data quality, and improving discoverability of data br/> Technical Experience :1:Must to have skills:Python 2:Good to have knowledge skills:Big Data technologies such as Hadoop, Hive, Spark 3:Good to have knowledge experience building software in Python/Scala/Java, PL/SQL 5:Good to have knowledge Experience with Jenkins Pipeline, Bit bucket, Python Unit Test code development tools br/> Professional Attributes :A:Good Communication skills br/> Educational Qualification :A:Btech or equivalent Qualification A:Btech or equivalent",20524909937,02-05-2024,31-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"scala, java, spark, python, microsoft azure, hive, kubernetes, networking, bitbucket, plsql, sql, ansible, docker, git, gcp, devops, linux, paas, jenkins, shell scripting, mysql, hadoop, big data, cloud computing, maven, vmware, cloud platform, saas, cloud infrastructure, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Python (Programming Language) Good to have skills : Ansible on Microsoft Azure Minimum  3  year(s) of experience is required Educational Qualification : A:Btech or equivalent br/>Key Responsibilities :1:Drive the code conversion effort from legacy system to target state system 2:Modify and convert existing legacy jobs and schedules into modern Apache airflow jobs 3:Create new ingestion framework to support ingestion from various type of sources like oracle, Teradata, db2, sql server, etc 4:Develop framework for tracking data lineage, ensuring data quality, and improving discoverability of data br/> Technical Experience :1:Must to have skills:Python 2:Good to have knowledge skills:Big Data technologies such as Hadoop, Hive, Spark 3:Good to have knowledge experience building software in Python/Scala/Java, PL/SQL 5:Good to have knowledge Experience with Jenkins Pipeline, Bit bucket, Python Unit Test code development tools br/> Professional Attributes :A:Good Communication skills br/> Educational Qualification :A:Btech or equivalent Qualification A:Btech or equivalent",20524908998,02-05-2024,31-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"bitbucket, plsql, sql, hadoop, python, hive, kubernetes, scala, networking, ansible, docker, java, git, spark, gcp, devops, linux, paas, jenkins, shell scripting, mysql, big data, cloud computing, maven, vmware, microsoft azure, cloud platform, saas, cloud infrastructure, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS EMR Admin - Chennai,"SUMMARY                 Looking For AWS EMR Admin ProfessionalsFor Global IT MNC @Chennai         Greetings From 2COMS Group!             Location: Chennai (WFO)             Experience: 5+ years             Must - Have: AWS EMR Admin           Roles and Responsibilities:           Skills :      ?    AWS EMR, EC2, AWS S3, Cloud Formation Template, Batch data, AWS CodePipeline services   Nice to Have :?      ?  EKS    Responsibility :?      ? Full hands-on role so need to have good admin knowledge on AWS EMR, EC2, AWS S3, Cloud      ?  Formation Template, Batch data, etc.      ?  Managing and Deploying EMR Clusters, with the knowledge of AWS account and IAM, experience on admin-related tasks of EMR Persistent Cluster and Transient Cluster.      ? Good knowledge of AWS Cloud Formation, cluster setup, and AWS network.      ? Have hands-on on Infrastructure as a Code for Deployment like Terraform.      ?  Experience in AWS health monitoring and optimization.      ?  Experience with Hadoop and Big Data will be an added advantage                         Requirements           Benefits    Salary benefits and other perks ",3.00E+11,30-04-2024,29-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"template, aws iam, data, amazon redshift, emr, eks, docker, ansible, cloud, route3, git, iam, aws emr, devops, linux, jenkins, hadoop, big data, cloud computing, amazon cloudwatch, python, dynamo db, aws administration, vpc, aws cloudformation, amazon rds, aws lambda, amazon ec2, terraform, aws",-,9am-6pm,"Full Time, Permanent",2coms,Organization,2coms,https://img.naukimg.com/logo_images/groups/v1/467982.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft 365 Good to have skills : Microsoft Exchange Online Minimum  3  year(s) of experience is required Educational Qualification : 15 years or above in education qualification is required  br/>Key Responsibilities :1 Designing solutions on the M365 platform SharePoint, Exchange, Yammer, Microsoft Teams, Viva, Stream, Planner, To Do, Sway, Microsoft Lists, Microsoft Forms, Microsoft Whiteboard, OneDrive, OneNote, Outlook, MS Project2 Implementation of new mailbox, migration old mailbox to O3653 Responsibility for patching4 Responsibility for scanning vulnerabilities and take action against it br/> Technical Experience :1 Excellent knowledge of O365 Exchange Online and Exchange on-premises, including migration from legacy environments2 Strong understanding about SMTP, Autodiscover, Azure AD and eDiscovery3 Strong Knowledge on third party migration tools like Quest4 Strong knowledge of PowerShell Scripting5 Excellent troubleshooting/problem solving skills br/> Professional Attributes :1 Good verbal and written communication skills2 Good analytical and presentation skills3 Must have good interpersonal and leadership skills4 Willingness to learn and grow and ability to work under pressure5 Flexible and adaptable br/> Educational Qualification :15 years or above in education qualification is required Qualification 15 years or above in education qualification is required",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"smtp, exchange online, troubleshooting, azure active directory, exchange server, dns, microsoft azure, sharepoint, microsoft teams, data modeling, active directory, exchange administration, powershell, microsoft windows, ms exchange, windows server, adfs, dhcp",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Indore,Indore,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Python, DJango/flask","Diverse Lynx is looking for Python, DJango/flask to join our dynamic team and embark on a rewarding career journey      As a Python Developer specializing in Django or Flask, you will be responsible for designing, developing, and maintaining web applications using the Python programming language      This role involves collaborating with cross-functional teams, understanding project requirements, and delivering high-quality software solutions        Key Responsibilities:        Web Application Development:Design and develop robust and scalable web applications using Python, Django, Flask, or other relevant frameworks      Implement backend logic, database interactions, and server-side processing      Database Design and Integration:Design and optimize database schemas to support application functionality      Integrate applications with databases and ensure data integrity      API Development:Create RESTful APIs to facilitate data communication between the frontend and backend systems      Ensure API security and performance      Frontend Integration:Collaborate with frontend developers to integrate server-side logic with user interfaces      Ensure seamless communication between the frontend and backend components      Code Reviews and Quality Assurance:Participate in code reviews to maintain code quality and ensure best practices      Conduct thorough testing of applications, identify bugs, and implement solutions      Collaboration with Cross-Functional Teams:Work closely with UX/UI designers, product managers, and other stakeholders to understand project requirements      Collaborate with cross-functional teams to deliver integrated solutions      Scalability and Performance Optimization:Optimize application performance and ensure scalability      Identify and address bottlenecks in the application architecture      Security Implementation:Implement security best practices in coding and configuration      Conduct security audits and implement measures to protect against vulnerabilities      Documentation:Create and maintain documentation for code, APIs, and system architecture      Ensure documentation is up-to-date and accessible to relevant team members      Continuous Learning:Stay updated on industry trends, new technologies, and best practices      Apply new knowledge and skills to enhance development processes    ",2.60E+11,26-02-2024,26-05-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"schema, rest, python, configuration, user interface designing, scalability, database design, coding, django, system architecture, software solutions, code review, html, api, performance optimization, flask, programming, web application development, architecture",-,9am-6pm,"Full Time, Permanent",Diverse Lynx,Organization,Diverse Lynx,https://img.naukimg.com/logo_images/groups/v1/4554388.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Solutions Engineer,"     Engage with both current and prospective clients to understand their technical and business challenges     Present and demonstrate SingleStore product offering to Fortune 500 companies.     Enthusiastic about the data analytics and data engineering landscape     Provide valuable feedback to product teams based on client interactions     Stay up to date with database technologies and the SingleStore product offerings       Qualifications       Minimum of 3 years experience in a technical pre-sales role     Broad range of experience within large-scale database and/or data warehousing technologies     Excellent presentation and communication skills, with experience presenting to large corporate organizations     Experience with Kubernetes and Linux     Ability to communicate complex technical concepts for non-technical audiences.     Strong team player with interpersonal skills     Preferred experience with data engineering tools Apache Spark, Apache Flink, Apache Airflow     Demonstrated proficiency in ANSI SQL query languages     Demonstrated proficiency in Python, Scala or Java     Understanding of private and public cloud platforms such as AWS, Azure, GCP, VMware     ",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Product management, VMware, SAN, Linux, GCP, Presales, ANSI, Analytics, Recruitment, Python",-,9am-6pm,"Full Time, Permanent",Singlestore,Organization,Singlestore,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
DataOps Engineer,"Hi Folks, Greetings of the day!!! Please go through the below Job description, If you're interested in the below requirement, Kindly share resume to vsputtapaka@adaequare.com Designation: DataOps Engineer Experience: 6-9 years Location: Hyderabad  Job Description: Responsibilities: 1. 6+ years direct experience working in IT Infrastructure 2. Must have experience in Identity and Access Management, Networking concepts/ VPN, Alerting and Monitoring for implementing/maintaining cloud solutions in virtualized environments. 3. Strong in Terraform & Ansible 4. Experience in advanced areas of networking, including Linux, software-defined networking, network virtualization, open protocols, application acceleration and load balancing, DNS, virtual private networks and their application to PaaS and IaaS technologies 5. Experience in implementation of few of the following: Networking, DevOps, Security, Compute, Storage, Containers, Kubernetes 6. Self-starter with in-depth hands-on work experience with large-scale implementations of Google Cloud Platform (GCP) and/or with AWS or Azure. 7. Experience migrating applications from the enterprise to public cloud providers (e.g. Amazon, Google, or Microsoft) 8. Relevant cloud certifications such as GCP Cloud Architect or GCP Cloud Engineer. 9. Experience with Google DataProc, DataFlow, AirFlow, BigQuery 10. Experience with Big-Data technology such as Spark, Hadoop, Kafka is nice to have 11. Experience in developing monitoring solutions for infrastructure (ideally in Elastic) 12. Demonstrated hands-on experience with cloud solutions 13. Ability to learn and apply new technologies quickly 14. Understanding of Agile, SCRUM, and Continuous Delivery. Mandatory Skills: GCP / AWS, Terraform, Ansible, IAM, Jenkins, Storage, DataProc, DataFlow, AirFlow, Powershell, Docker and Kubernetes. Good to have:  Spark, Hadoop, Kafka.  Thanks & Regards Venkat Srikanth Lead Talent Acquisition  Mobile: 8977883049 Email Id: vsputtapaka@adaequare.com",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Airflow, Jenkins, GCP, Ansible, Data Flow",-,9am-6pm,"Full Time, Permanent",Adaequare,Organization,Adaequare,https://www.naukri.com/hotjobs/images/v3/adaequare.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Python Specialist,"Role & responsibilities     Should have experience on CDW(Snowflake/Redshift), data ingestion/transformation (AWS Glue, Fivetran, dbt, Airflow, kafka, etc.) and reasonably good experience with DevOps and CI/CD tool chain. Considering this is a Squad lead role, person is expected to know data-engineering design and good in communication & planning and as well to carry out following tasks/activities. Azure SQL Database access for data ingestion Detailed Data analysis [understand data structures] DSL registration Schemas registrations [Data hub] across transform, enrich. etc Data validation in Transform zone Reporting Data model design Create Source to target mapping [STTM] document. Business transformation/Use case pipeline development System Testing Code promotion",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, Azure, AWS",-,9am-6pm,"Full Time, Permanent",Orcapod Consulting Services,Organization,Orcapod Consulting Services,https://img.naukri.com/logo_images/v3/602389.gif,"Pune, Chennai, Bengaluru","Pune, Chennai, Bengaluru",-,-,-,7-12 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer," The Data Platform team is responsible for building infrastructure, services and tooling to support data engineering and data discovery. We look to deliver a robust and reliable platform that meets our users requirements, while also testing out new and emerging technologies to add to our stack. Key to the team s mentality is continuously making improvements, automation of processes, using metadata to drive optimization and help other data teams to get stuff done.            You will have the opportunity to play a part in a wide array of projects rooted in DAZN s unique and exciting data, whilst demonstrating ground-breaking tools and techniques that support our ambitious plans for product development and the company s rapid global growth to change the way the world watches sport!            Some of the latest Data Engineering projects include:        Move existing services & pipelines to Kubernetes      Improve existing services and add new ones to the Data Platform      Delivering a data discovery and metadata engine      Building out an exchange layer to securely move data to other data platforms      Building out an event stream and enabling hybrid batch / real time use case       ?     As our new Cloud Engineer, you ll have the opportunity to:        Help shape the future of the Data Platform that we are building      Build infrastructure for high performant, low latency and high-volume data pipelines across multiple cloud services      Work collaboratively with different stakeholders -Data Engineers and Platform Operations - working on tooling to help them work more effectively and deliver data products      Guide fellow engineers in technologies and tools evaluation, platform governance policies, utilisation efficiency and cost optimisation     Develop strong analytical and communication skills in a global team with a variety of background and experiences        You ll be set up for success if you have:        Several years of experience as a Cloud or DevOps Engineer      Extensive experience in building cloud infrastructure on one of more of AWS, GCP or Azure      Hands-on knowledge of Kubernetes and the Kubernetes ecosystem, such as Helmand ArgoCD or alternatives      Strong understanding of managing cloud services, including Networking, Load Balancing and Security      Extensive experience with infrastructure-as-code, such as Terraform or Ansible, and CI/CD processes      Hands-on knowledge of writing clean and robust code in one or more of Python, Java, Bash, Node or Typescript      Working knowledge of storage solutions, such as object storage, DataLakes and Warehouses        Even better if you have:        Experience using AWS Services, such as EKS, S3, Kinesis, API Gateway, EventBridge, Lambda, EMR and IAM      Exposure to GCPservices, such as GCS, BiqQuery, CloudFunctions, Pub/Sub, Dataproc and IAM      Knowledge of Data Domain technologies and/or real-time streaming applications      Familiarity with Snowflake, Apache Airflow and dbt    ",2.30E+11,23-02-2024,23-05-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Fitness & Wellness,"Automation, metadata, data domain, Networking, GCP, Analytical, Apache, Load balancing, infrastructure services, Python",-,9am-6pm,"Full Time, Permanent",DAZN,Organization,DAZN,https://img.naukimg.com/logo_images/groups/v1/2445484.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Cloud Engineer,"Job Title: Cloud Migration Engineer Location: Bangalore Max Budget: 18 LPA (DOE) Job Description: We are seeking a skilled Cloud Migration Engineer to join our team for an exciting opportunity involving the migration of databases from on-premise infrastructure to AWS cloud. The ideal candidate will have extensive hands-on experience with Java and PL/SQL, along with a strong understanding of database systems and architecture. Must-Have Skills: Proficiency in migrating databases from on-premise environments to AWS cloud. Expertise in Java programming for developing migration scripts and tools. Very strong hands-on experience with PL/SQL for database development and optimization. Good-to-Have Skills: Familiarity with Angular for front-end development. Understanding of RESTful API design and development. Knowledge of Python for scripting and automation. Experience with ETL (Extract, Transform, Load) tools for data migration and integration.",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"PLSQL, AWS, Java, REST, ETL Tool, Angular, Python",-,9am-6pm,"Full Time, Permanent",Photon,Organization,Photon,https://www.naukri.com/hotjobs/images/v3/photon_feb14.gif,Bengaluru,Bengaluru,-,-,-,14-18 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Sr. Staff Data Engineer ( Data Platform),"We are looking for a highly motivated Sr Staff Data Engineer to build a state-of-the-art data platform to solve various data-driven use cases across the organization.  This platform will host various data products such as but not limited to, Subscription, Content, and Product Analytics, Personalization and Recommendation, Marketing & Ad-Sales enablement.  You will be charged with building a new core data platform in the cloud - handles both streaming and batch data processing, capable of solving any big data initiatives in scope now or evolve in future as well. You will be helping data engineers, analysts, and scientists perform their functions by building highly scalable capabilities across the platform. This individual will bring in his/her expertise in a wide variety of big data processing frameworks (both open source and proprietary), large scale database systems (OLAP and OLTP), stream data processing, API Development, Machine learning operationalization, and cloud automation to build and support all the data needs across our platform. Take lead role in translating various business requirements in to engineering architecture. Build software across our entire cutting-edge data platform, including event driven data processing, storage, and serving through scalable and highly available APIs, with cutting-edge technologies. Change how we think, act, and utilize our data by performing exploratory and quantitative analytics, data mining, and discovery. Think of new ways to help make our data platform more scalable, resilient and reliable and then work across our team to put your ideas into action. Work closely with data analysts and business stake holders to make data easily accessible and understandable to them. Ensure data quality by implementing re-usable data quality frameworks. Develop and enforce data engineering, security, data quality standards through automation. Participate in supporting the platform 24X7. Be passionate about growing team - hire and mentor engineers and analysts. Be responsible for cloud cost and improving efficiency. What to Bring : Bachelors degree in computer science or similar discipline 15+ years of experience in software engineering and/or data engineering Ability and willingness to learn any new technologies and apply them at work in order to stay ahead of the curve. Expertise in at least few programming languages - Java, Scala, Python or similar. Expertise in building and managing large volume data processing (both streaming and batch) platform is a must. Expertise in stream processing systems such as Kafka, Kinesis, Pulsar or Similar Expertise in distributed data processing frameworks such as Apache Spark, Flink or similar. Expertise in SQL and No-SQL Apache Cassandra, DynamoDB, MySQL Expertise in OLAP databases such as Snowflake or Redshift. Experience in operationalizing and scaling machine models is a huge plus. Experience with variety of data Tools & frameworks (example: Apache Airflow, Druid) will be a huge plus. Experience with Analytics Tools such as Looker, Tableau is preferred. Cloud (AWS) experience is preferred Direct to consumer digital business experience is preferred Strong interpersonal, communication and presentation skills.",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,180,Engineering - Software & QA,Data Platform Engineer,TV / Radio,"JAVA, scala, amazon redshift, kinesis, sql, apache, stream processing, spark, apache pulsar, mysql, software engineering, api, snowflake, python, dynamo db, data processing, airflow, presentation skills, data engineering, oltp, apache flink, tableau, cassandra, olap, aws",-,9am-6pm,"Full Time, Permanent",Discovery Communications,Organization,Discovery Communications,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python with Cloud Professional," Work with cloud platforms (any) to deploy, manage, and scale applications    as needed.    Troubleshoot and debug issues in production and testing environments.    Stay updated with the latest technologies and best practices in Python    development.          Skills Required:              Design, develop, and maintain robust Python-based backend systems and API    Strong experience in any cloud deployment with Docker, fastapi, flask,    GCP/Azure/AWS    Collaborate with cross-functional teams to define, design, and ship new    features.    Implement and optimize performance-critical code for high-throughput and    low-latency applications.    Understand and work with the threading limitations of Python and    multi-process architecture.    Implement and maintain websockets for real-time communication between    server and client applications.    Utilize object-oriented programming principles to ensure scalability,    reusability, and maintainability of codebase.    Implement multi-tenancy solutions to support multiple users or tenants    securely.    Containerize applications using Docker for easy deployment and scalability.          Primary Skills:        Python, API Development, Data Structures, Algorithms    Docker, fastapi, flask, GCP/Azure/AWS    Experience around libraries such as Numpy, Matplotlib, Pandas or similar    will be a big benefit though not mandatory  ",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Backend, Architecture, Scalability, GCP, Data structures, Deployment, Troubleshooting, Object oriented programming, Python, Testing",-,9am-6pm,"Full Time, Permanent",JK Technology Services,Organization,JK Technology Services,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
MLOps Engineer,     ?Experience in managing machine learning projects endtoend with focus on Azure MLOps and open source MLOps       ?Design and implement cloud solutions build MLOps on Azure cloud     ?Knowledge on building pipelines using synapse data bricks end to end     Build CICD pipelines orchestration by Azure DevOps or similar tools     ?Collect clean and preprocess data from various sources including databases APIs and thirdparty platforms and ensure data quality and integrity     Experience with MLOps tools such as ModelDB Kubeflow Pachyderm and Data Version Control DVC     Collaborate with data scientists data engineers and architect and document the processes     ?Deploy and integrate machine learning models into production systems and applications to automate decisionmaking processes for realtime data processing and analysis     ?Monitor model performance evaluate accuracy and implement necessary improvements to enhance predictive capabilities using automated monitoring and alarm tools     Experience in API integration and data feeds with social analytics Facebook Instagram Twitter     Experience in supporting model builds and model deployment for IDEbased models and autoML tools experiment tracking model management version tracking model training Dataiku Datarobot Kubeflow MLflow neptuneai will be an addon model hyperparameter optimization model evaluation and explainability SHAP Tensorboard     Experience in Databricks Azure DataLake Gen2 and Unity Catalog     ?Utilize Big Data technologies eg Hadoop Spark to efficiently process and analyze largescale datasets distributed across different sources     ?Proficiency in programming languages such as Python R SQL and familiarity with data manipulation libraries eg Pandas NumPy     ?Handson experience with machine learning frameworks eg TensorFlow Scikitlearn Keras PyTorch and deep learning techniques is preferred   ,1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Version control, orchestration, Machine learning, Programming, Data processing, Data quality, Open source, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Mouri Tech,Organization,Mouri Tech,https://img.naukimg.com/logo_images/groups/v1/4612691.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Engineer - E,"   Nisum is a leading global digital commerce firm headquartered in California,  with services spanning digital strategy and transformation,  insights and analytics,  blockchain,  business agility,  and custom software development.  Founded in 2000 with the customer-centric motto     Building Success Together     ,   Nisum has grown to over 1, 800 professionals across the United States,  Chile, Colombia,  India,  Pakistan and Canada.  A preferred advisor to leading Fortune 500 brands,  Nisum enables clients to achieve direct business growth by building the advanced technology they need to reach end customers in today s world,  with immersive and seamless experiences across digital and physical channels.          What You'll Do       Build our platforms,  systems and infrastructure using your solid expertise in coding.     Work closely with product development teams,  provide hands-on engagement to develop,  direct and implement reliable,  secured and cost-effective cloud solutions.     Participate with a cross department-functional team to establish a cloud operational governance framework.     It often involves routine grunt work on service requests to assist other teams with platform services.                What You Know       A minimum of 5-9 years of proven professional experience is required.     Deep understanding of Linux,  networking,  cloud design pa7erns,  API's,  and security.     Strong working experience with managing containers (Docker/LXC) in a production environment using the container orchestration services like AWS EKS.     At least 3+ years of experience working with AWS Infrastructure services with emphasis on IAM,  Network,  EC2,  Lambda,  S3,  CloudWatch,  CloudTrail and in general overall Security.     Strong knowledge and implementation history of Terraform,  Packer,  Ansible,  Chef,  Jenkins or any other similar tooling.     Experience on at least one scripting language - Shell,  Python etc.       Nice to have skills:      Experience with other public cloud platforms like Azure and GCP is a bonus.     Knowledge and working experience in implementing one or more Observability platforms like Prometheus,  InfluxDB,  Dynatrace,  Grafana,  Splunk etc.  to measure telemetry data like logs,  metrics    and traces.     Solid professional coding experience in at least one programming language,  preferably Java.     Experience with BigData platforms,  like AWS EMR,  Databricks,  Cloudera,  Hortonworks etc.     Experience with open source technologies like Hadoop,  Hive,  Presto,  Spark,  Airflow etc.                Education       BS in Computer Science or related fields; MS preferred               Benefits       In addition to competitive salaries and benefits packages,  Nisum India offers its employees some unique and fun extras:      Continuous Learning -   Year-round training sessions are offered as part of skill enhancement certifications sponsored by the company on a need basis.  We support our team to excel in their field.       Parental Medical Insurance   - Nisum believes our team is the heart of our business and we want to make sure to take care of the heart of theirs.  We offer opt-in parental medical insurance in addition to our medical benefits.       Activities -  From the Nisum Premier League's cricket tournaments to hosted Hack-a-thon,  Nisum employees can participate in a variety of team-building activities such as skits,  and dances performance in addition to festival celebrations.       Free Meals -   Free snacks and dinner is provided on a daily basis,  in addition to subsidized lunch.                 Nisum is an Equal Opportunity Employer and we are proud of our ongoing efforts to foster diversity and inclusion in the workplace.       ",70324502736,07-03-2024,05-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Linux, Networking, Open soure, Coding, GCP, Analytis, Medial insurane, Team building, Python, Computer siene",-,9am-6pm,"Full Time, Permanent",Nisum,Organization,Nisum,https://img.naukimg.com/logo_images/groups/v1/4657475.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Lead (Data Science Engineer),"     Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc               Infrastructural thinking in the design and architectural integration / optimization of the analytical platforms and related (Data Reservoir)     Determine upgrades and improvements to current data architectures     Together with the Business Analyst, the Data Scientist and the contact persons at specific business partners, responsible for building and prioritizing a long-term analytical roadmap     Thinking along and developing E2E automated AI platforms (and everything needed for this) to build insights and bring them to the customer in an insightful and efficient way     Support the Data Scientists in translating their needs into specific research questions and hypotheses that can be addressed in a data science project (trajectory)     Rigorous in the control and stability and reliability of the produced code that must be output correctly     Responsible for the delivery and follow-up of the analytical solutions within the lifecycle management     Coaching/Training team members on data engineering processes         Profile:         Should have 10+ years of experience in Database/DWH background and at least 4+ years of experience in Data engineering with Python (pandas, NumPy ..etc)     Should be proficient in understanding databases and expertise in understanding complex SQL s and able to write SQL queries in efficient way     Knowledge of the advanced technologies related to big data in order to be able to use the most adequate technology in function of the need     Should have experience in version control with Git and has worked on most of the features of it (CI/CD, Git runners, Branch strategy etc)     Software engineering: general programming knowledge (including code testing approaches; DRY coding; concepts of software development service oriented; OOPS)     Should define follow the best practices in Data engineering     Assemble large, complex data sets that meet functional / non-functional business requirements       Our Proposition:        A challenging role in Data engineering to show case your capability     Help realize Colruyt Group vision in becoming Europe number one Data-Driven Organization     and enrich your personal experience in technology space and personal development     Autonomy, Personal Leadership work life balance are our core group ethos     Colruyt Group believes Organization grows to the extent people grows . We invest to enhance     craftsmanship of co-workers. This will help you to work on latest and cutting-edge technologies                             Qualification: BE/BTech                       ",50424502582,05-04-2024,04-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SQL queries, Version control, GIT, Architecture, data science, Coding, Analytical, Manager Technology, big data, Python",-,9am-6pm,"Full Time, Permanent",Colruyt It Consultancy India,Organization,Colruyt It Consultancy India,https://img.naukimg.com/logo_images/groups/v1/4588185.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloudera Platform Engineer,"     Design and implement Cloudera-based data platforms, including cluster sizing, configuration, and optimization.               Install, configure, and administer Cloudera Manager and CDP clusters, managing all aspects of the cluster lifecycle.     Monitor and troubleshoot platform performance, identifying and resolving issues in a timely manner.       Review the maintain the data ingestion and processing pipelines on the Cloudera platform.     Collaborate with data engineers and data scientists to design and optimize data models, ensuring efficient data storage and retrieval.       Implement and enforce security measures for the Cloudera platform, including authentication, authorization, and encryption.       Manage platform user access and permissions, ensuring compliance with data privacy regulations and internal policies.     Experience in creating Technology Road Maps for Cloudera Platform. Stay up-to-date with the latest Cloudera and big data technologies, and recommend and implement relevant updates and enhancements to the platform.     Experience in Planning, testing, and executing upgrades involving Cloudera components and ensuring platform stability and security.     Document platform configurations, processes, and procedures, and provide training and support to other team members as needed.         Requirements:         Proven experience as a Cloudera platform engineer or similar role, with a strong understanding of Cloudera Manager and CDH clusters.       Expertise in designing, implementing, and maintaining scalable and high-performance data platforms using Cloudera technologies such as Hadoop, Spark, Hive, Kafka.       Strong knowledge of big data concepts and technologies, data modeling, and data warehousing principles.       Familiarity with data security and compliance requirements, and experience implementing security measures for Cloudera platforms.       Proficiency in Linux system administration and scripting languages (eg, Shell, Python).       Strong troubleshooting and problem-solving skills, with the ability to diagnose and resolve platform issues quickly.       Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.       Experience on Azure Data Factory/Azure Databricks/Azure Synapse is a plus                                  Qualification: BTech/BE                       ",50424502580,05-04-2024,04-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloudera, Linux system administration, Compliance, Data modeling, data security, data privacy, big data, Troubleshooting, Data warehousing, Python",-,9am-6pm,"Full Time, Permanent",Colruyt It Consultancy India,Organization,Colruyt It Consultancy India,https://img.naukimg.com/logo_images/groups/v1/4588185.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Synapse,Azure Synapse + Power BI,40524002624,04-05-2024,02-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Azure Synapse, Power Bi, synapse",-,9am-6pm,"Full Time, Permanent",Hexaware Technologies,Organization,Hexaware Technologies,https://img.naukimg.com/logo_images/groups/v1/12466.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Cloud Architect with EKS - SSE,"   Hands on and deep experience working with Google Data Products (e.g. BigQuery, Pub-Sub, Dataproc, Looker etc.)        Experience in migrating on-premise Hadoop workflows to GCP        Experience in architecting the data pipeline in GCP from multitude of sources(Streaming & Batch).        Strong hands-on experience in big data ecosystem (Hive, Oozie, Shell Scripting)        Experience in Spark (Scala/Python/Java) and Kafka.    E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.          Devops knowledge            Analyze / Review the requirement, prepare the design document, system test plan as per requirements.        Execute project specific development activities in accordance to applicable standards and quality parameters.        Developing / Reviewing Code        Setting up the right environment for the projects.        Ensure delivery within schedule by adhering to the engineering and quality standards.        Own & deliver end to end projects within GCP for Payments Data Platform        Able to work under pressure on deliverables, P1 Violations, Incidents.        Provide Weekly & Monthly Project Updates to stake holders & Management.        Should be fluent and clear in communications (Written & Verbal)""        Be a trusted technical advisor to customers and solve complex Cloud Infrastructure and DevOps challenges    Cloud Architecture: Develop hybrid cloud reference architecture,      Define target state cloud architecture, Experience with private and public cloud architectures, pros/cons, and migration considerations        Skills:        Container Technology      Helm      Kubernetes      Terraform      Kubernetes Administrator    ",2.90E+11,29-03-2024,27-06-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Business consulting, CGI, GCP, Shell scripting, SCALA, Manager Technology, Test planning, digital transformation, Python, Recruitment",-,9am-6pm,"Full Time, Permanent",CGI,Organization,CGI,https://img.naukimg.com/logo_images/groups/v1/1402790.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Platform Engineer,"   We are seeking an experienced Azure Platform Engineer to join our team     As the Azure Platform Engineer, you will be responsible for designing, implementing, and managing Azure cloud-based solutions     Your role will involve collaborating with cross-functional teams, troubleshooting issues, and ensuring the reliability and scalability of our Azure infrastructure         Key responsibilities include:         Designing and implementing Azure infrastructure and services.     Managing Azure resources and optimizing their performance.     Collaborating with architects and developers to deploy applications on the Azure platform.     Troubleshooting and resolving issues related to Azure services and infrastructure.     Ensuring the security and compliance of Azure environments.         Candidate Qualifications:         A minimum of 3 years of experience as an Azure Platform Engineer or a related role.     Proficiency in Terraform, DevOps, and Azure.     Strong understanding of Azure infrastructure and services.     Experience in designing and implementing scalable Azure solutions.     Knowledge of Azure security and compliance best practices.         Required Skills:         Terraform     DevOps     Azure   ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Compliance, Scalability, devops, Cloud, Manager Technology, Deployment, Management, Troubleshooting",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data PlatformEngineer_Python,"   Background      Any Graduate           ?       Skills-    Python, Data Engineering, Scala, ETL, Apache Kafka, Kubernetes, Docker, ADF, Talend, Pentaho, Glue, Informatica, SSIS, Data Warehousing, MYSQL, MongoDB, Postgres SQL, Oracle, GraphDB, DevOps, Casandra, Agile Methodology, Data Quality, Azure, Snowflake, GITLab, Jenkins, RestAPI                3+ Years of professional Python experience         3+ Years of professional Data Engineering experience         3+ years of experience with Scala         3+ years of experience in Cloud ETL/ELT for creating data pipelines         3+ years of experience with Apache Kafka, Kubernetes, Docker         Any professional experience with any ETL\ELT tool (Talend, Pentaho, Glue, ADF, GCD Glue, ADF, Informatica,SSIS)         Any professional Data Warehousing\Data Modeling experience in any of the following areas: Dimensional, DataMart, DataVault, DataLake, DataMesh, GraphDB.           ?       Any professional experience with the following databases: MSSQL, Azure, MySql, PostgreSQL, Snowflake, MongoDb, MariaDB, Casandra, Oracle.         Proficient experience in Snowflake,Kafkapython         Knowledge of Agile methodologies and DevOps practices.         Knowledge of Snowflake, SnowSQL.         Knowledge of Datawarehouse Design Methodologies (3NF, Kimball, Data Vault)           ?       Knowledge of cloud orchestration tools (Airflow, ADF)         Knowledge of CI/CD         Knowledge of Data Products and Rest APIs         Any Data Quality Experience (GreatExpectations , ydata , pandas) a plus.         Experience with CI/CD tools such as Jenkins, GitLab, Azure Data Factory or TravisCI a plus   ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Data modeling, MySQL, Agile, Data quality, Informatica, Oracle, Apache, SSIS, Pentaho, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Java Platform Engineer - Java8 / Microservices / Kafka,"   You will join a team of insatiably curious data engineers, software architects, and product experts who never settle for ""good enough""      Our Java Platform teams tech stack is based on Java8 (Spring Boot) and RESTful web services      We typically build and deploy applications as cloud-native Kubernetes microservices and integrate with scalable technologies such as Kafka in Docker container environments      Our developers work in an agile process to efficiently deliver high value data driven applications and product packages                    Required Experience:        Minimum of Bachelor s Degree or its equivalent in Computer Science, Computer Information Systems, Information Technology and Management, Electrical Engineering or a related field.      Have experience working and strong understanding of object-oriented programing and cloud technologies      End to end experience delivering production ready code with Java8, Spring Boot, Spring Data, and API libraries      Strong experience with unit and integration testing of the Spring Boot APIs.      Strong understanding and production experience of RESTful APIs and microservice architecture.      Strong understanding of SQL databases and NoSQL databases and experience with writing abstraction layers to communicate with the databases.                Nice to haves (but not required):        Exposure to Kotlin or other JVM programming languages      Strong understanding and production experience working with Docker container environments      Strong understanding and production experience working with Kafka      Cloud Environments: AWS, GCP or Azure          ",2.31E+11,23-08-2023,21-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Electrical engineering, spring boot, NoSQL, GCP, Integration testing, Cloud, Agile, Information technology, SQL",-,9am-6pm,"Full Time, Permanent",Egen Solutions,Organization,Egen Solutions,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Data Platform Engineer,"     As a Senior Data Platform Specialist with experience as a DBA, we'll give you the opportunity to grow and develop your capabilities as well as supporting you through Microsoft certifications.              You'll have experience in:                          Installing, configuring, and administering and Microsoft SQL Server                  Handling general maintenance tasks including backups, index and statistics maintenance, consistency checking, patching etc.                  Troubleshooting and resolving issues                  Dealing directly with end-users and customers, and have excellent communication skills                          We support a wide range of SQL environments, the issues you encounter can be so diverse, if you are as passionate about SQL Server and Azure as we are, then the Dedicated Support Team might be the place for you.              We'll invest heavily in your on-going development through access to formal training, in-house development sessions, mentoring, and external SQL gatherings such as User Groups, SQL Saturdays, and of course SQL Bits.          We cover the cost of study for SQL Server and Azure exams to make sure you reach your full potential, and you will have personal development time allocated each week to ensure you have time to learn.                          Experience                  Installing, configuring, monitoring, maintaining and improving the performance of databases and data stores.              Develops and configures tools to enable automation of database administration tasks              Monitors performance statistics and creating reports              Identify and investigate complex problems and issues and to recommend corrective actions              Perform routine configuration, installation and reconfiguration of database related products.                      Requirements                      Experience of the above, working as a DBA                  Direct contact with customers/end-users on the phone and by email                  IT Service Desk experience              Experience with PostgreSQL          Experience with MongoDB          Experience with CosmosDB                      Benefits                  We care about your wellbeing so have invested resources and training help to ensure you can be at your best with us          Very friendly, inclusive culture, family-feel and strong camaraderie                  Company growth and new projects means on going career progression options                  Up to date technology, so collaboration has never been easier                  Cutting edge technology projects - exciting goals ahead      ",2.21E+11,01-02-2024,01-05-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Analytics / KPO / Research,"Training, Automation, Postgresql, Diversity and Inclusion, IT service desk, Database administration, Manager Technology, Troubleshooting, Monitoring, SQL",-,9am-6pm,"Full Time, Permanent",Coeo,Organization,Coeo,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Splunk Develper,"     Proven experience as a Splunk Developer or similar role.        Proficiency in Splunk platform components, SPL, and data visualization.        Strong scripting skills (e.g., Python, Bash) for data processing and automation.        Experience designing and implementing data ingestion strategies.        Knowledge of data parsing, transformation, and enrichment techniques        Skills:        Dev/Tools Infrastructure Supp      DevOps      English      Splunk      ",2.20E+11,22-03-2024,20-06-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Automation, Business consulting, CGI, Manager Technology, Data processing, splunk, data visualization, digital transformation, Python, Recruitment",-,9am-6pm,"Full Time, Permanent",CGI,Organization,CGI,https://img.naukimg.com/logo_images/groups/v1/1402790.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Programmer I,"   This is a fantastic opportunity to begin a career in Statistical Programming within a CRO that specialises in data-related services.      You will receive substantial training that will enable you to gain a clear appreciation and understanding of clinical trial reporting.      You will have the opportunity to work as part of global project teams, gaining exposure to high profile, global studies for major pharmaceutical clients.      Quanticate is the world leading data-focussed CRO, and we often work with our customers on their complicated clinical trials which require a high level of statistical programming input. We need talented individuals to help us fulfil our customers needs.        Our customers range from top global pharmaceutical companies where you can work as an integrated team member on a world leading clinical programme, to small biotechs that are taking their first steps in clinical development      We strongly advocate career development providing membership to professional societies, encouraging your involvement in their activities and committees.      Together we can help you build the career you want - developing your skills, working on challenging problems, to ultimately develop clinical therapies that matter        Provide project and technical support in the preparation, and review of high-quality programming deliverables and documentation, as well as the processes and standards required, to both colleagues and clients      Willingness to learn new skills, foundation knowledge of SAS, and good programming practices along with clinical trial reporting requirements.          Assist in programming derived SAS datasets as appropriate.      Assist in programming tables, figures and listings according to the SAP or to a specified client requirement.      Assist in performing quality control checks and complete quality control documentation for programming plans, specifications, outputs/derived datasets.      Have a good understanding of ICH/GCP guidelines.        Competitive Salary      Flexible working hours      Annual leave plus casual, sick leave as well as bank holidays      Medical Insurance      Accidental Coverage      5, 10, 15 years of service recognition awards      Death in service scheme      Long Term Disability Insurance    ",2.20E+11,22-01-2024,21-04-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Analytics / KPO / Research,"CRO, E-learning, Career development, SAP, SAS, Pharma, Clinical trials, Medical insurance, Manager Quality Control, Technical support",-,9am-6pm,"Full Time, Permanent",Quanticate International Ltd,Organization,Quanticate International Ltd,https://img.naukimg.com/logo_images/groups/v1/779378.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Migration Engineer,"                 Execute all activities related to the migration of data, applications, and other elements from local infrastructure to Azure cloud environment.       Work closely with IT project managers and lead developers to ensure successful execution of end-to-end migration activities.     Assess the existing infrastructure and make recommendations for changes to meet organizational goals.     Develop and implement best practices for migrating to Azure.     Troubleshoot any issues that arise during the migration process.                                Preferred Skills:                 Azure Certifications: AZ-104 (Azure Administrator), AZ-204 (Azure Developer), AZ-500 (Azure Security Engineer), or AZ-303 (Azure Architect).     Experience with other cloud services like AWS, Google Cloud, etc     Knowledge of programming and scripting languages such as PowerShell, Bash, SQL, .NET, Java, Python, PHP, Ruby, PERL, C++, and R.                   ",2.00E+11,20-03-2024,18-06-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Administration, C++, Powershell, Cloud Services, Programming, PHP, Perl, Ruby, SQL, Python",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Database Engineer,"   We are looking for a skilled Cloud Database Engineer to join our dynamic IT team      The ideal candidate will have expertise in designing, implementing, and maintaining cloud-based database solutions      If you have a strong background in database administration, cloud technologies, and a passion for optimizing performance and security, we invite you to apply for this key role        Responsibilities:        Cloud Database Design and Implementation:Design and implement scalable, high-performance, and secure cloud-based database solutions      Work with development teams to understand application requirements and ensure optimal database architecture      Database Administration:Perform routine database administration tasks, including backup and recovery, database tuning, and monitoring      Implement and enforce security measures to safeguard the integrity and confidentiality of data      Cloud Platform Expertise:Utilize cloud platforms (eg, AWS, Azure, GCP) to deploy and manage database instances      Stay current with cloud services and features to optimize database performance      Performance Optimization:Identify and resolve performance issues through query optimization, indexing, and other tuning techniques      Collaborate with development teams to optimize database access patterns      Data Migration and Integration:Plan and execute data migration projects between on-premises and cloud-based databases      Integrate databases with other cloud services and applications      Automation and Scripting:Develop automation scripts and tools to streamline database deployment, configuration, and maintenance tasks      Implement Infrastructure as Code (IaC) principles for database resources      Monitoring and Reporting:Establish monitoring and alerting systems for database performance and availability      Generate regular reports on database health, performance, and utilization      Collaboration and Knowledge Sharing:Collaborate with cross-functional teams, providing database expertise and guidance      Share knowledge with team members and contribute to continuous learning    ",1.90E+11,19-02-2024,19-05-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloud services, configuration, microsoft azure, cloud platforms, iac, data migration, database administration, cloud technologies, cloud platform, docker, ansible, scripting, automation, gcp, devops, design, linux, migration, integration, aws, architecture",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer Integration Platforms,"       As a Data Platform Engineer Integration Platforms, you will take advantage of your prior experiences and possess a strong ability to design (architect), build (engineer), and maintain (run) integration platforms used across the Digital Teams by developing and implementing sound processes, procedures, and the necessary documentation to ensure that each platform is maximized for the operational efficiency, availability and meets the technical and regulatory requirements for the Integration users      The Data Platform Engineer is additionally responsible for ensuring all the platforms are kept up to date with the latest software and security      Your strong and broad technical background encompassing both cloud native and data center hosted solutions along with the underlying operating system, database, network, security, and other various development and system integration principles and techniques will enhance your ability to be successful in this role      This is a highly visible position, and you will be a key contributor to how we modernize our platforms and change how we think about the solutions we apply      To be successful, you must have a deep understanding of file transfer capabilities, application integration patterns, and principles      You will be a key member of the team and must be comfortable driving technical ideas and communicating with both technical and non-technical individuals across the organization              Roles &    Responsibilities            Design, build, implement, and maintain integration platforms critical to the Sanofi Digital Data Team.      Establish best practices for various integration platforms performance, security, and resilience to failure.      Continuously analyze the performance and features of the various integration platforms enabling additional capacity and new features required for the business projects.      Gain an understanding of the current state of our integration platforms and then define future state architectures and execute the strategy.      Design platforms using automation tools such as Terraform, Ansible, and Shell for all phases of the solution s lifecycle.      Develop and document best practices around the integration platforms incorporating feedback from across the organization.      Ensure proper change management processes and procedures and regulatory and quality processes are followed.      Perform technical reviews of projects utilizing the integration platforms to enforce standards and best practices of each platform and promote sound governance of its usage.      Both contribute to and develop various dashboards and reports to gain insights into the performance and utilization of the platforms to improve operational effectiveness, future needs, and governance.      Maintain a strong cross-functional relationship with the various project and operational teams, continually sharing new features, functionality, process improvements, lessons learned, and other best practices across the team.      Actively contribute to the Digital Data Community.              Work Experience          5+ years of progressive experience working on enterprise integration platforms.      2+ years with Informatica PowerCenter.      2+ years with Informatica iPaaS and/or IDMC and/or IDQ and/or CDGC              Technical Skills          Strong Informatica iPaaS capabilities. This position is responsible for designing and maintaining the existing Informatica iPaaS product and modules such as Informatica Enterprise MFT, Informatica Cloud Application Integration, and Informatica Cloud Data Integration.      Knowledge of enterprise integration design patterns and data engineering best practices.      Experience with AWS data-related services such as EMR, Glue, S3, and Lambda.      Experience (and enjoyment) working with emerging technologies.      Experience with automation and DevOps capabilities such as Terraform, Ansible, and GitHub.      Knowledge of AWS is a plus              Minimum required skills          Experience working with a variety of cross-functional teams      Able to work in a fast-paced, constantly evolving environment and manage multiple priorities      Pragmatic and capable of solving complex issues      Service-oriented, flexible team player      Attention to detail & technical intuition      Excellent written, verbal, and interpersonal skills for executive-level communication and collaboration      Experience in the bio/pharmaceutical industry is a plus              Minimum preferred skills          Strong Regulatory Experience (GxP, Sarbanes-Oxley, French Data Protection Act (FDPA), General Data Protection Regulation (GDPR)      Good knowledge of AWS/Cloud Computing (Knowledge of Azure or GCP a plus)      Experience (and enjoyment) working with emerging technologies              Education          Bachelors Degree in Science or Information Management with a focus on Engineering.      Master s Degree in Computer Science or Engineering is preferred    ",1.81E+11,18-09-2023,17-12-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Cloud computing, Automation, Change management, French, HP data protector, System integration, Network security, Informatica, Information management",-,9am-6pm,"Full Time, Permanent",Sanofi,Organization,Sanofi,https://img.naukimg.com/logo_images/groups/v1/4627409.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
IT&D Senior Platform Engineer - Process Mining,"             If you thrive in a dynamic environment, possess 8+ years of data engineering experience, and has a knack of simplifying complex data architecture, then this opportunity is tailor-made for you.                          Join our team and spearhead the optimization of Celonis data extraction, connecting                      Celonis to diverse source systems, and continuously improving the Celonis platform                           In this pivotal role, you will:                             Architect, orchestrate, and optimize Celonis data pipelines, ensuring efficient data ingestion and processing while upholding rigorous security standards.       Lea the data extraction from ERP systems, streamlining processes and accelerating the solution delivery across process streams.     Lead the data integration activities, forging seamless connections between Celonis and diverse source systems, with varied complexity.     Leverage advanced knowledge of SQL, Python, and API integration to automate data engineering tasks, extract critical data, and build robust integrations tailored to specific business needs.     Collaborate closely with cross-functional teams to identify data challenges, architect data solutions, and deliver transformative outcomes.     Stay updated with the emerging data technologies and best practices, continuously iterating and optimizing data pipelines for maximum efficiency and adapting to the ever-evolving landscape.                            We seek a seasoned data engineer who demonstrates:                              Proven success in complex data integration scenarios (with more than 8 years of experience).                         Has hands-on experience in Celonis data engineering and platform engineering tasks.                         Expertise in designing, developing, and optimizing data pipelines, proficient in utilizing frameworks (exposure to any of the tools such                      s?                      Airflow, Luigi, Teradata, Talend, etc).                         In-depth knowledge of SAP data extraction methodologies, capable of optimizing processes for performance and scalability.                         Fluency in the languages of data: possessing proficiency in SQL, Python, and diverse API integration techniques (REST, SOAP, etc).                         A collaborative spirit and unwavering commitment to delivering high-quality solutions, thriving in a team-oriented environment.                         A passion for driving business value, believing in the power of data to optimize operations and unlock organizational excellence.                         Work with cutting-edge data technologies and solve complex technical challenges, pushing the boundaries of whats possible.                         Contribute to a company revolutionizing the way businesses operate, making a positive impact.             ",1.20E+11,12-03-2024,10-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,FMCG,"Mining, ERP, SAP, Scalability, Teradata, Solution delivery, SQL, Python, Data architecture, Data extraction",-,9am-6pm,"Full Time, Permanent",Reckitt,Organization,Reckitt,https://img.naukimg.com/logo_images/groups/v1/717262.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Infra Engineer Core(2x),"   Supplementing the current staffing shortage, these two resources should be able to handle both Azure and on-prem system engineering.          Same as Security, one of these Infra Engineers should function as Lead.          This Lead will handle translation and assist other engineers;          this Lead will be our point of contact, review requests, and consolidate requests into things you and I can review    ",81223500656,08-12-2023,07-03-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Automation, System engineering, Translation, Staffing, Service engineering, Manager Technology",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Database Engineer,"           The Cloud Database Engineer provides 24x7 operational support and designs and creates automation for the Hana, Hadoop, Redis, Kafka, InfluxDB, Postgres, Cassandra, and MySQL instances supporting the SAP Procurement and Business Network cloud platform in public cloud, SAP Business Technology Platform, and SAP Managed Data Centers                          Data Dynamo: Design, deploy, and maintain highly available, scalable, and secure cloud databases to support real-time data processing and fuel data-driven insights.                          Support Superhero: Provide 24x7 support, monitoring, and proactive maintenance for our critical databases, ensuring uptime and availability around the clock.                          Automation Alchemist: Leverage Terraform and Ansible to automate database provisioning, configuration, and scaling, reducing manual effort and boosting efficiency.                          Python Wizardry: Harness the power of Python to develop custom scripts, automation tools, and data pipelines that streamline database operations and enhance performance.                          Git Guardian: Collaborate with development teams to version control database changes effectively using Git, ensuring seamless integration and traceability.                          Performance Maestro: Dive deep into data performance analysis, fine-tuning queries, optimizing database configurations, and collaborating with developers to achieve peak efficiency.                          Security Sentinel: Implement robust access controls, encryption, and monitoring to safeguard our data assets, maintaining the highest levels of data security.                          Collaboration Virtuoso: Work hand-in-hand with cross-functional teams, offering database expertise, and driving successful integration with applications and analytics platforms.                          Qualifications                          Database Mastery: Extensive experience managing databases, including SAP HANA, Hadoop, Redis, Cassandra, MySQL, and InfluxDB, with a strong understanding of relational and NoSQL databases.                          Cloud Expertise: Proven proficiency in cloud database management on major platforms (AWS, Azure, or GCP), with the ability to optimize database performance in a cloud environment.                          Automation Wizardry: Hands-on experience with infrastructure automation tools like Terraform and configuration management tools like Ansible, coupled with Python proficiency for automation, and git for source code management.                          Innovation Mindset: A track record of innovation, staying abreast of industry trends, and a passion for exploring and integrating new technologies.                          Problem-Solving Ninja: Demonstrated ability to identify complex technical issues, devise creative solutions, and implement effective problem-solving strategies.                          Collaborative Spirit: Excellent communication skills and the ability to work collaboratively with cross-functional teams in an agile environment.                          Bachelor's Degree: A Bachelor's degree in Computer Science, Engineering, or a related field. Advanced degrees are a plus. 3+ years experience in database administration            ",71223500371,07-12-2023,06-03-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Procurement, Computer science, operational support, Configuration management, MySQL, Database administration, Agile, Data processing, Analytics, Monitoring",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Trainers and Freelancers Google Cloud Platform,           ?     We are looking for Trainers and Freelancers on Google Cloud Platform (GCP) for Machine Learning AI     Our Services are.       Instructor Led Live Training to One to One on Various new technologies.     On Job support     Corporate Trainings     Consulting Services for Full Time Contract Jobs             ,61223502381,06-12-2023,05-03-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Corporate training, GCP, Consulting, Machine learning, Cloud, Instructor, Manager Technology",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Infra Consultant (Platform - Integration) Kafka,"   A Multinational Technology consulting partner that designs and builds custom digital commerce platforms to power large and small enterprises is looking for suitable candidates         Mandatory skills   :       Experience with public cloud platforms like AWS, Azure, GCP, preferably Azure     Expert knowledge in cloud security (network, data and resource), auditing and transparency     Experience with Terraform or Cloud Formation or any other infrastructure provisioning frameworks     Experience with Chef, Puppet, Ansible or any other configuration management frameworks.     Experience with BigData platforms, like AWS EMR, Databricks, Cloudera, Hortonworks etc.     Experience with open-source technologies like Hadoop, Hive, Presto, Spark, Kafka, Airflow etc.         Good to Have Skills    :       Azure/GCP Solution Architect Certification (any level).       ",31023500030,03-10-2023,01-01-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Printing & Publishing,"cloudera, cloud security, Technology consulting, Networking, GCP, Configuration management, Open source, Solution Architect, Auditing",-,9am-6pm,"Full Time, Permanent",Beechi Vidya Kendra Trust,Organization,Beechi Vidya Kendra Trust,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Infra Consultant (Platform - Database),"   A Multinational Technology consulting partner that designs and builds custom digital commerce platforms to power large and small enterprises is looking for suitable candidates       Mandatory skills :       Experience with public cloud platforms like AWS, Azure, and GCP, preferably Azure     Expert knowledge in cloud security (network, data, and resource), auditing, and transparency     Experience with Terraform or Cloud Formation or any other infrastructure provisioning frameworks     Experience with Chef, Puppet, Ansible, or any other configuration management frameworks.     Experience with BigData platforms, like AWS EMR, Databricks, Cloudera, Hortonworks, etc.     Experience with open-source technologies like Hadoop, Hive, Presto, Spark, Kafka, Airflow etc.         Good to Have Skills    :        Azure/GCP Solution Architect Certification (any level).   ",31023500029,03-10-2023,01-01-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Printing & Publishing,"cloudera, cloud security, Technology consulting, GCP, Configuration management, Database, Open source, Solution Architect, Auditing",-,9am-6pm,"Full Time, Permanent",Beechi Vidya Kendra Trust,Organization,Beechi Vidya Kendra Trust,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Infra Consultant (Platform - Container),"   A Multinational Technology consulting partner that designs and builds custom digital commerce platforms to power large and small enterprises is looking for suitable candidates       Mandatory skills:       Experience with public cloud platforms like Azure, GCP, preferably Azure     Expert knowledge in cloud security (network, data and resource), auditing and transparency     Experience with Terraform or Cloud Formation or any other infrastructure provisioning frameworks     Experience with Chef, Puppet, Ansible or any other configuration management frameworks.     Experience with BigData platforms, like EMR, Databricks, Cloudera, Hortonworks etc.       ",31023500028,03-10-2023,01-01-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Printing & Publishing,"cloudera, cloud security, Technology consulting, Networking, GCP, Configuration management, Chef, Commerce, Auditing",-,9am-6pm,"Full Time, Permanent",Beechi Vidya Kendra Trust,Organization,Beechi Vidya Kendra Trust,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python,"   Responsible for designing, coding, testing, and debugging applications using Python      Write and maintain code for existing applications, ensuring that the code is efficient, maintainable, and scalable      Collaborate with other software developers and stakeholders to identify requirements and develop solutions      Deep understanding of computer science principles and be knowledgeable about software development best practices      Strong problem-solving, critical thinking, and communication skills, as well as the ability to work well in a team environment        ",20823502281,02-08-2023,31-10-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Coding, Debugging, Python, Testing",-,9am-6pm,"Full Time, Permanent",Diverse Lynx,Organization,Diverse Lynx,https://img.naukimg.com/logo_images/groups/v1/4554388.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
SecOps Engineer - Platform Engineering,"       As a member of the Platform Engineering team, the SecOps Engineer will enable the engineering teams to scale our infrastructure, platform and internal developer tooling of multiple global digital products being developed within Carrier Digital     This role will work on a range of software products critical to Carrier s continued business growth     As a SecOps Engineer, you will be working with a team of engineers to implement and support modern software delivery systems                 Key Responsibilities:               You will solve problems in a fast-paced, collaborative, and iterative delivery environment     You should be an engineer with software delivery experience, and a collaborative style who is able to work with developers and others to understand requirements and implement software delivery solutions that meet architectural guidelines           As a constant learner and early adopter, you are already embracing leading-edge technologies and methodologies. We embrace a culture of experimentation and constantly strive for improvement and learning. You will work in a collaborative, trusting, thought-provoking environment one that encourages diversity of thought and creative solutions that are in the best interests of our customers globally.           Strong analytical, strategical, and creative problem-solving skills.     Experience mentoring, coaching, developing and inspiring people and teams.     Team player with a positive, collaborative work ethic. Enjoys collaborating with design, product, etc.     Passionate about cloud native technologies.     Does not believe in reinventing the wheel, instead leveraging the best components that are readily available.               Primary Responsibilities           Work with other SecOps Engineers to implement and maintain scalable, reliable, performant, and efficient software delivery systems.     Help the team to continuously improve our operating processes.     Ensure we run a cost-effective service, monitor and make changes to hit spending targets.     Implement SecOps solutions based on the goals and requirements set forth by leadership.               Requirements:           At least 3 years of professional experience in Software Delivery     At least 2 years of experience programming in JavaScript, TypeScript, Node.js or Python     At least 2 years of experience delivering cloud native software.     At least 2 years of experience creating and maintaining CI/CD tooling.     At least 2 years of experience with traditional IT infrastructure management     Bachelor s degree or equivalent practical experience.                 Although not necessary, it would be nice if you have:         Experience delivering cloud native solutions.     Experience in -      SecOps best practices     Fully automated software delivery     Serverless and cloud native software delivery         Some experience in -      Application Testing     Application Monitoring     Full Stack Serverless Web Application Delivery     AWS Management, Security, Scalability, Reliability, Cost Optimization                   Certifications           AWS Certified Security - Specialty or equivalent practical experience     AWS Certified DevOps Engineer - Professional     AWS Certified Solutions Architect - Associate     Multiple IAC Methods, such as CDK, CloudFormation, Serverless Framework, SAM, Terraform     Hands on experience delivering cloud native software using modern SecOps practices.     Familiarity with Application Security concepts such as OWASP, CVE, SAST     ",20124500704,02-01-2024,01-04-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Electrical Equipment,"HVAC, Architecture, Analytical, Diversity and Inclusion, Cloud, Employee engagement, Ventilation, Refrigeration, AWS, Python",-,9am-6pm,"Full Time, Permanent",Rellio Services,Organization,Rellio Services,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Development Engineer,"Project Role : Software Development Engineer Project Role Description : Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.  Must have skills : Microsoft Azure Data Services Good to have skills : Talend ETL, Snowflake Data Warehouse Minimum  3  year(s) of experience is required Educational Qualification : Bachelors degree Summary :As a Software Development Engineer, you will be responsible for analyzing, designing, coding, and testing multiple components of application code across one or more clients. Your typical day will involve working with Microsoft Azure Data Services, performing maintenance, enhancements, and/or development work.  Roles & Responsibilities: Design, develop, and maintain Azure Data Services-based solutions for clients. Collaborate with cross-functional teams to identify and resolve technical issues. Perform maintenance, enhancements, and/or development work on application code. Analyze, design, code, and test multiple components of application code across one or more clients. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Experience with Talend ETL and Snowflake Data Warehouse. Strong understanding of cloud computing concepts and technologies. Experience with SQL and NoSQL databases. Experience with Agile development methodologies. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful solutions. This position is based at our Pune office. Qualification Bachelors degree",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"talend etl, data services, microsoft azure, nosql, agile, dbms, hibernate, sql, plsql, spring, coding, java, git, postgresql, j2ee, mysql, etl, cloud computing, mongodb, snowflake, python, software development, oracle, talend, warehouse, sql server",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Development Engineer,"Project Role : Software Development Engineer Project Role Description : Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.  Must have skills : Apache Kafka Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Full Time Graduates / Post Graduates in:BE / BTech / ME / MTech / MSc / MCA / MS Summary :As a Software Development Engineer, you will be responsible for analyzing, designing, coding, and testing multiple components of application code across one or more clients. Your typical day will involve working with Apache Kafka, performing maintenance, enhancements, and/or development work.  Roles & Responsibilities: Design, develop, and maintain Apache Kafka-based applications. Collaborate with cross-functional teams to identify and troubleshoot issues related to Apache Kafka. Perform maintenance, enhancements, and/or development work on Apache Kafka-based applications. Ensure the performance, quality, and responsiveness of Apache Kafka-based applications. Professional & Technical Skills: Must To Have Skills:Experience with Apache Kafka. Strong understanding of distributed systems and microservices architecture. Experience with Java, Spring Boot, and RESTful APIs. Experience with containerization technologies such as Docker and Kubernetes. Experience with cloud platforms such as AWS, Azure, or GCP. Qualification Full Time Graduates / Post Graduates in:BE / BTech / ME / MTech / MSc / MCA / MS",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, microservices, docker, distribution system, kafka, algorithms, hibernate, sql, spring, coding, java, apache, git, spring mvc, gcp, j2ee, json, mysql, html, data structures, mongodb, rest, python, software development, microsoft azure, javascript, spring boot, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Microsoft Azure Analytics Services-Software Development Engineer,"Project Role :Software Development Engineer  Project Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work. Must have skills :Microsoft Azure Analytics Services  Good to have skills :Accenture Delivery Methods (ADM) Minimum 3 year(s) of experience is required  Educational Qualification :15 years of full time qualification Key Reponsibilities :Do analysis and solve moderately complex data processing problems Do analysis and create pipelines with ADF to load data to target Deploy to higher environments using CICD yaml pipelines in Azure DevOps Create airflow jobs using python to execute ADF pipelines Work on Databricks notebooks to validate the data Create new solutions, leveraging and, where needed, adapting existing methods, standards and procedures The person would require understanding of the strategic direction set by seniors Technical Experience : Azure Data Factory Databricks Python / pyspark / scala Azure DevOps ADLS, AKV Apache Airflow Professional Attributes :Commitment to quality Strong analytical skills Good written and verbal communication Ability to meet deadlines Experience in banking Industry is added advantageExperience in banking Industry is added advantage Qualification NA",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"analytics services, scala, azure analytics, microsoft azure, azure data factory, c#, python, azure data lake, software development, strong analytical skills, data processing, pyspark, azure devops, sql server, sql, data bricks, coding, apache, java, devops, mysql",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Lead Software Engineer ??Python,"   As a part of our continued expansion in its technology organization, Targray is seeking to hire a Lead Software Engineer - Python, to join our Risk and Data Technology Team based at our India Office Gurgaon, Haryana      The role will be responsible for creating the Python frameworks, tools that will be used for developing back-end services, APIs, data science work (AI/ML)          Responsibilities          Reporting to the Senior Engineer Manager Risk and data Technology and working closely with CIO he/she will focus on designing and developing the key frameworks (service engines, API backed business functions) for business process execution as well as work in Data science projects (planning optimization, pricing/scheduling optimization) and driving the industrialization of the python-based software engineering /development processes.      The role will involve designing and developing risk and trading functionality covering MtM calculations, VaR and Shocking, PnL calculation, Position reporting, and Trade pricing.      Will be working with the rest of the data team to deliver data engineering related projects, designing re-usable solutions.      Also create the development and design standards for Python eco system, tools and library selection, packaging, build and deployment, CI/CD processes for Python based deliveries.      The role will also work with rest of technology organization to improve the SDLC rigor including automated testing, automated build and deployment, design and coding best practices.      Be proactive in active engagement to resolve issues and collaborate with rest of the development in the organization to deliver the technology strategy defined by CIO.      Be the leader for Python development in Targray and drive the industrializing of Python engineering processes.            Requirements          Bachelor s degree in computer science with good knowledge in data science topics (supervised/unsupervised ML, Deep learning topics)      8+ yrs. in software development and 5+ yrs. solid hands-on experience in Python development.      Knowledge in SQL and NoSQL database architectures, writing performant SQL and MongoDB queries and understanding and appreciation of how databases work and utilizing efficient ORM technologies /or alternate efficient data access tools is very important.      Good experience and knowledge in working in data engineering projects and deliverables including data integration, SQL and ETL tools.      Strong experience in python frameworks like numpy, pandas, scipy, machine learning and AI libraries, ML and data integration tools like Apache Spark, using Python REST and Web Socket APIs in actual real-life projects.      Worked in Agile delivery methodologies and familiar with agile ceremonies.      Detailed knowledge and experience in design patterns spanning all layers of an application architecture (mid-tier, business logic layer, integration and data layer).      Experience in using unit, component and integration testing with pytest scripting as part of development process, using continuous integration (Jenkins, Azure DevOps) and using build tools like setuptools and anaconda is must.      Experience in working in financial services, Trading and risk domain either in an enterprise or a software product company is definitely advantageous.      Ability to instrument code, monitor application execution, identify issues and solving them objectively and performance engineering are key needs.      Good experience in using Linux/ Windows OS and accessing/working with cloud IaaS services (hosted VMs) as well as containerized solutions like docker/ Kubernetes    ",80524501524,08-05-2024,06-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Water Treatment / Waste Management,"Linux, Coding, Machine learning, Agile, Packaging, SDLC, Financial services, SQL, Python",-,9am-6pm,"Full Time, Permanent",Targray,Organization,Targray,https://img.naukimg.com/logo_images/groups/v1/17544.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior SQL developers,Senior SQL developer Experience : 5 -7 Years Develop high-quality database/python/spark scripts/solutions. Use SQL to develop procedures/functions/views. Review and interpret ongoing Data requirements . Analyze existing SQL queries/function/stored proc for performance improvements. Develop procedures and scripts for data migration and validation Data Migration and validation. data pipeline creation/debugging. Understanding of delta lake/warehouse data security roles/policy and integrity. debugging performance of python/sql queries.,30524909298,03-05-2024,01-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SQL, Data Migration, SQL Development, SQL Queries, Data Lake, Spark, Python",-,9am-6pm,"Full Time, Permanent",ACI Global Business Services,Organization,ACI Global Business Services,https://www.naukri.com/hotjobs/images/v3/acitech_june19.gif,"Pune, Maharashtra","Pune, Maharashtra",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior SQL developers,  Senior SQL developer Experience : 5 -7 Years   Develop high-quality database/python/spark scripts/solutions. Use SQL to develop procedures/functions/views. Review and interpret ongoing Data requirements . Analyze existing SQL queries/function/stored proc for performance improvements. Develop procedures and scripts for data migration and validation Data Migration and validation. data pipeline creation/debugging. Understanding of delta lake/warehouse data security roles/policy and integrity. debugging performance of python/sql queries. Note : Interested professionals can share their resume to raghav.b@aciinfotech.com,30524908796,03-05-2024,01-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Data Migration, SQL Development, SQL Queries, Spark, Data Lake, Python",-,9am-6pm,"Full Time, Permanent",ACI Global Business Services,Organization,ACI Global Business Services,-,Pune,Pune,-,-,-,10-20 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Google Cloud Platform Architecture Good to have skills : Google BigQuery, Python (Programming Language), DevOps Minimum  7.5  year(s) of experience is required Educational Qualification : Btech. br/> Educational Qualification :Btech. Qualification Btech.",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"sap, sapui5, sap web ide, agile, sap abap, kubernetes, python, project management, vmware, enterprise architecture, microsoft azure, docker, ansible, infrastructure architecture, java, gcp, linux, paas, jenkins, cloud infrastructure, iaas, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Kolkata,Kolkata,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft 365 Good to have skills : Microsoft Exchange Online Minimum  3  year(s) of experience is required Educational Qualification : 15 years or above in education qualification is required  Summary :As a Data Platform Engineer, you will be responsible for assisting with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working with Microsoft 365 and Microsoft Exchange Online to ensure the smooth functioning of the data platform.  Roles & Responsibilities: Assist with the design and implementation of the data platform blueprint, ensuring cohesive integration between systems and data models. Collaborate with Integration Architects and Data Architects to ensure the smooth functioning of the data platform. Ensure the smooth functioning of Microsoft 365 and Microsoft Exchange Online. Troubleshoot and resolve any issues related to the data platform. Stay updated with the latest advancements in Microsoft 365 and Microsoft Exchange Online to ensure the smooth functioning of the data platform. Professional & Technical Skills: Must To Have Skills:Proficiency in Microsoft 365. Good To Have Skills:Experience with Microsoft Exchange Online. Solid understanding of data platform blueprint and design. Experience with troubleshooting and resolving issues related to the data platform. Strong communication and collaboration skills. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft 365. The ideal candidate will possess a strong educational background in Software Engineering or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Indore office.",2.70E+11,27-04-2024,26-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"blueprint, data modeling, exchange online, troubleshooting, software engineering, project management, dns, business analysis, microsoft azure, sharepoint, microsoft teams, active directory, exchange administration, ms exchange, agile, exchange server, adfs, jira",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Indore,Indore,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Microsoft Azure Data Services Good to have skills : Python (Programming Language), Microsoft Azure Analytics Services, Synapse Minimum  3  year(s) of experience is required Educational Qualification : Min 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working with Microsoft Azure Data Services and other relevant data platform components.  Roles & Responsibilities: Assist with the design and implementation of data platform components using Microsoft Azure Data Services. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines and ETL processes to support data integration and transformation. Implement data security and privacy measures to ensure compliance with regulatory requirements. Troubleshoot and resolve data platform issues, working with cross-functional teams as needed. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Data Services. Good To Have Skills:Proficiency in Python (Programming Language), Microsoft Azure Analytics Services, and Synapse. Strong understanding of data platform components and architecture. Experience with data integration and transformation using ETL processes. Knowledge of data security and privacy measures. Ability to troubleshoot and resolve data platform issues. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Data Services. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Gurugram office. Qualification Min 15 years of education",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, data services, analytics services, azure analytics, microsoft azure, hive, c#, azure data lake, ssas, power bi, data warehousing, data pipeline, azure data factory, machine learning, sql server, sql, sql azure, data modeling, spark, etl, ssis, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Ahmedabad,Ahmedabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Scientist,"Your Role and Responsibilities You will take on the role of Senior Data Scientist, tasked with the design, development, and deployment of Machine Learning models and capabilities for IBM Storage Insights, a SaaS Product at IBM India Systems Development Lab (ISDL). ISDL is engaged in the end-to-end design and development across IBM''s Power, Z, and Storage portfolios. You will join the WW Storage development organization, contributing to product delivery and strategy discussions. Your role will also encompass leadership responsibilities, guiding technical teams, and ensuring the delivery of comprehensive features. Responsibilities: Engage in hands-on development, focusing on Time Series Analysis, Forecasting, and GenAI technologies. Develop AIOps capabilities for IBM Cloud-based SaaS applications. Lead design efforts and actively participate in coding at all levels, working closely with Architects Operate within an Agile framework, ensuring continuous delivery. Define development standards, including technology selection and workflow processes. Collaborate with professionals to establish both functional and non-functional requirements. Take part in technical reviews, including the assessment of requirements, specifications, designs, and other project artifacts. Required Technical and Professional Expertise Minimum of 10 years of industry experience in a Data Scientist role, with hands-on development experience. Proficiency in Python programming. Expertise in Time Series Analysis and Forecasting. Demonstrated experience in AI projects, including familiarity with AI and machine learning frameworks (e.g., scikit-learn, TensorFlow, PyTorch, LLMs, NLP, GenAI). Skills in Anomaly Detection, Classification, Clustering, Optimization, and AI/AIOps. Knowledge in AI model deployment and integration. Experience with RESTful APIs and GitHub. Basic understanding of Java. Strong problem-solving capabilities, with the ability to adapt to a dynamic, fast-paced environment. Excellent communication and teamwork abilities. Preferred Technical and Professional Expertise Data Engineering background. Advanced experience with cloud technologies, including Kubernetes, microservices architecture, Kafka, Object Storage, Cassandra database, and Docker. Familiarity with IBM Cloud Technologies is a plus. Understanding of storage system observability. Experience in design and technical leadership.",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, scikit-learn, artificial intelligence, tensorflow, agile framework, time series analysis, kubernetes, rest, github, natural language processing, forecasting, docker, cassandra database, microservices, java, kafka, pytorch, machine learning algorithms",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills : Talend ETL Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 years of fulltime education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Talend ETL.  Roles & Responsibilities: Design and develop Talend ETL jobs to extract, transform, and load data from various sources into the data platform. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Assist with the development of the data platform blueprint and design, encompassing the relevant data platform components. Perform data profiling, data quality analysis, and data mapping to ensure data accuracy and completeness. Develop and maintain technical documentation related to the data platform components. Professional & Technical Skills: Must To Have Skills:Proficiency in Talend ETL. Strong understanding of data integration and data modeling concepts. Experience with data profiling, data quality analysis, and data mapping. Experience with SQL and relational databases. Experience with data warehousing concepts and technologies. Additional Information: The candidate should have a minimum of 3 years of experience in Talend ETL. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Pune office. Qualification 15 years of fulltime education",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"information technology, relational databases, data modeling, talend etl, data integration, python, oracle, data analysis, talend, data warehousing, data mapping, sql server, sql, plsql, quality analysis, data quality, java, mysql, etl, aws, unix, data profiling",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Neo4j Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and utilizing Neo4j to develop and maintain the data platform.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain the data platform utilizing Neo4j. Ensure the data platform is scalable, reliable, and secure. Troubleshoot and resolve any issues related to the data platform. Professional & Technical Skills: Must To Have Skills:Experience with Neo4j. Strong understanding of data platform components and architecture. Experience with data modeling and database design. Experience with ETL processes and tools. Experience with cloud-based data platforms such as AWS or Azure. Additional Information: The candidate should have a minimum of 3 years of experience with Neo4j. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Pune office. Qualification Minimum 15 years of full time education",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, neo4j, data modeling, etl, aws, visualforce, rest, python, css, web services, sfdc, machine learning, triggers, javascript, apex, sql, salesforce, spring, salesforce crm, spring boot, java, html, mysql, mongodb",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Infrastructure Engineer,"Project Role : Infrastructure Engineer Project Role Description : Assist in defining requirements, designing and building data center technology components and testing efforts.  Must have skills : Cloud Data Architecture Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : AZ900 Summary :As an Infrastructure Engineer, you will be responsible for assisting in defining requirements, designing and building data center technology components, and testing efforts related to Cloud Data Architecture. Your typical day will involve working with the team to ensure the smooth functioning of the data center technology components in Gurugram.  Roles & Responsibilities: Assist in defining requirements, designing and building data center technology components, and testing efforts related to Cloud Data Architecture. Collaborate with cross-functional teams to ensure the smooth functioning of the data center technology components. Provide technical guidance and support to the team in implementing Cloud Data Architecture solutions. Ensure compliance with industry standards and best practices in Cloud Data Architecture. Professional & Technical Skills: Must To Have Skills:Strong understanding of Cloud Data Architecture. Good To Have Skills:Experience with Cloud platforms such as AWS, Azure, or Google Cloud. Experience in designing and implementing Cloud Data Architecture solutions. Experience in data modeling, data integration, and data warehousing. Experience in working with Big Data technologies such as Hadoop, Spark, or Kafka. Additional Information: The candidate should have a minimum of 3 years of experience in Cloud Data Architecture. The ideal candidate will possess a strong educational background in computer science, information technology, or a related field, along with a proven track record of delivering impactful Cloud Data Architecture solutions. This position is based at our Gurugram office. Qualification AZ900",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"information technology, microsoft azure, data architecture, spark, gcp, vmware, data warehousing, infrastructure architecture, docker, ansible, data center, data modeling, devops, kafka, linux, hadoop, big data, aws, cloud computing, data integration, unix",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : SAP CPI for Data Services Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of fulltime education Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance using SAP CPI for Data Services.  Roles & Responsibilities: Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure using SAP CPI for Data Services. Deploy infrastructure and platform environments, create a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure the successful delivery of cloud application solutions. Stay updated with the latest advancements in cloud technology and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Proficiency in SAP CPI for Data Services. Experience in designing, building, testing, and deploying cloud application solutions. Strong understanding of cloud infrastructure and platform environments. Experience in creating a proof of architecture to test architecture viability, security, and performance. Experience in collaborating with cross-functional teams to ensure the successful delivery of cloud application solutions.Professional Attributes:Should have good communication skills.Educational Qualification:Minimum 15 years full time education Qualification Minimum 15 years of fulltime education",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, cloud platform, sap cpi, cloud infrastructure, aws, kubernetes, python, vmware, microsoft azure, networking, docker, ansible, sap pi, java, git, gcp, devops, linux, jenkins, shell scripting, cloud computing, sap hana",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : SAP CPI for Data Services Good to have skills : NA Minimum  2  year(s) of experience is required Educational Qualification : Minimum 15 years of fulltime education Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance using SAP CPI for Data Services.  Roles & Responsibilities: Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure using SAP CPI for Data Services. Deploy infrastructure and platform environments, create a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure the successful delivery of cloud application solutions. Stay updated with the latest advancements in cloud technology and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Proficiency in SAP CPI for Data Services. Experience in designing, building, testing, and deploying cloud application solutions. Strong understanding of cloud infrastructure and platform environments. Experience in creating a proof of architecture to test architecture viability, security, and performance. Experience in collaborating with cross-functional teams to ensure the successful delivery of cloud application solutions.Professional Attributes:Should have good communication skills.Educational Qualification:Minimum 15 years full time education Qualification Minimum 15 years of fulltime education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data services, cloud platform, sap cpi, infrastructure, cloud infrastructure, kubernetes, python, vmware, microsoft azure, networking, docker, ansible, sap pi, java, git, gcp, devops, linux, jenkins, shell scripting, aws, cloud computing, sap hana",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Technology Platform Engineer,"Project Role : Technology Platform Engineer Project Role Description : Creates production and non-production cloud environments using the proper software tools such as a platform for a project or product. Deploys the automation pipeline and automates environment creation and configuration.  Must have skills : SAP HCM On Premise ABAP Good to have skills : SAP HCM Organizational Management Minimum  3  year(s) of experience is required Educational Qualification : 15 Years Continuous education Summary :As a Technology Platform Engineer, you will be responsible for creating production and non-production cloud environments using the proper software tools such as a platform for a project or product. Your typical day will involve deploying the automation pipeline and automating environment creation and configuration using SAP HCM On Premise ABAP as the primary skill.  Roles & Responsibilities: Design and develop SAP HCM On Premise ABAP solutions for cloud environments. Create and maintain automation pipelines for environment creation and configuration. Collaborate with cross-functional teams to ensure successful deployment of cloud environments. Troubleshoot and resolve issues related to SAP HCM On Premise ABAP in cloud environments. Professional & Technical Skills: Must To Have Skills:Experience in SAP HCM On Premise ABAP. Good To Have Skills:Experience in SAP HCM Organizational Management. Experience in creating and maintaining automation pipelines. Strong understanding of cloud environments and deployment. Experience in troubleshooting and resolving issues related to SAP HCM On Premise ABAP. Additional Information: The candidate should have a minimum of 3 years of experience in SAP HCM On Premise ABAP. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful solutions. This position is based at our Gurugram office. Qualification 15 Years Continuous education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pipeline, java, sap hcm, troubleshooting, abap, python, sap, docker, sap hr, successfactors, piping, organizational management, devops, personnel administration, safety, linux, jenkins, alv reports, payroll, smartforms, sap abap, sap hana, oo abap",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Native Engineer,"Project Role : Cloud Native Engineer Project Role Description : Select and deploy appropriate cloud-native tools to accelerate application development. Knowledge of the target cloud-native tools is necessary, and this role can specialize in one specific native cloud, ex. Azure, AWS, GCP, etc.  Must have skills : Microsoft Intune Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : BE and Equivalent Summary :As a Cloud Native Engineer, you will be responsible for selecting and deploying appropriate cloud-native tools to accelerate application development. Your typical day will involve working with Microsoft Intune and specializing in one specific native cloud, ex. Azure, AWS, GCP, etc.  Roles & Responsibilities: Deploy and manage Microsoft Intune for mobile device management and application management. Provide technical support for Microsoft Intune and other cloud-native tools. Collaborate with cross-functional teams to identify and resolve issues related to cloud-native tools. Design and implement cloud-native solutions using Microsoft Intune and other cloud-native tools. Stay updated with the latest advancements in cloud-native tools and technologies. Professional & Technical Skills: Must To Have Skills:Experience in deploying and managing Microsoft Intune. Good To Have Skills:Knowledge of other cloud-native tools such as Azure, AWS, GCP, etc. Strong understanding of mobile device management and application management. Experience in designing and implementing cloud-native solutions. Experience in providing technical support for cloud-native tools. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Intune. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful cloud-native solutions. This position is based at our Gurugram office. Qualification BE and Equivalent",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, mobile device management, intune, application management, aws, kubernetes, python, cloud native, application development, docker, ansible, amazon ec2, gcp, devops, paas, linux, jenkins, terraform, iaas, cloud computing",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Engineer, Associate","Role Description This role is required to form part of the IT delivery team building Unauthorised Principal Trading Activities. The platform would be used predominantly by Compliance to monitor trader activity in order to detect potential unauthorised principal trading activity. Senior DevOps engineer, being able to work independently on medium to large sized projects with strict dead-lines. Should be able to work in a cross-application, mixed-technical environment. Must demonstrate solid hands-on development track record working on an agile methodology. The role demands working along side a geographically dispersed team consisting of development and QA vendor teams, FAs and BAs. Your key responsibilities  This individual will be responsible for the following: Operate as a team member of a Scrum Agile/Kanban team and release manager. Design, development and peer review new functionality in various code languages Work as a senior developer for the various data sourcing in GCP/Hadoop Work as a senior developer for developing analytics algorithm on top of ingested data Ensuring new code is tested, both at unit level and at system level Your skills and experience Must Have: Hands on experience on Google Cloud Platform or equivalent cloud platform Hands on experience on CloudBuild, ArtifactRegistry, CloudDNS, CloudLoadBalancing etc. Hands on experience on DataFlow, CloudComposer, Cloud Storage, Data Proc etc. Hands on experience on Jenkins and github actions. Proficient in Python, Ansible, Shell Scripting and Terraform . Basic understanding of data security on public cloud. Basic understanding of log monitoring tools Geneos, Splunk and Grafana. Basic understanding of Unix and Windows. Basic understanding of Docker, Openshift and Micro services architecture. Basic understanding of networking and network security. Has more than 8 years of coding experience in reputed organization. Good to have: Financial experience, cross product or regulatory knowledge. Data visualistaion experience Experience of RDBMS (Oracle) Hands on business and systems knowledge gained in a regulatory delivery environment Education and qualification: Degree in a numerate field required.",1.60E+11,16-04-2024,15-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Investment Banking / Venture Capital / Private Equity,"python, network security, Shell Scripting, CloudComposer, openshift, Data Proc, DataFlow, ansible, docker, Cloud Storage, grafana, devops, jenkins, terraform, hadoop",-,9am-6pm,"Full Time, Permanent",Deutsche Bank,Organization,Deutsche Bank,https://img.naukimg.com/logo_images/groups/v1/468918.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Associate,"   Strong knowledge of AWS or Azure cloud platform (both will be desired but not mandatory)   services along with DevOps knowledge.   Has designed, implemented and automated secure cloud infrastructure.   Experience in IaC.   Good experience in multiple scripting languages (Python, PowerShell) to automate tasks and   enhance cloud operations.   Collaborate with cross-functional teams to gather requirements and provide technical expertise   in designing cloud architectures that align with business needs.   Has experience transforming from monolithic to microservices architecture.   Build out monitoring, alerting and dashboard capabilities to monitor CICD pipelines and key   production metrics         AWS Skills:       Expertise in setting up and creating VPC, EC2, S3, IAM cloud Instances using Amazon Web   Services and configuration. Worked on AWS Storage services (S3, EBS) for content storage and   monitoring services like (CloudWatch, CloudTrail, SNS) for Network fault tolerance.   Expertise in creating Roles and AWS Security Groups. Be able to work on Multiple AWS   instances, setting up security groups, Elastic Load Balancer and AMIs, Auto scaling to design cost   effective.   Expertise in implementation and Maintenance of monitoring and alerting of production and   corporate servers/storage using AWS Cloud Watch.   Hands on Experience on serverless computing functionality using AWS LAMBDA function.   Experience in creating CloudWatch alerts for instances and using them in Auto-scaling launch   configurations.       Azure Skills:       Design and deploy scalable and reliable cloud infrastructure solutions on the Microsoft Azure   platform using Azure Resource Manager (ARM) templates.   Strong understanding on Azure Data Services including ADF, ADLS, Blob, Data Bricks, Polybase   with a background of Hive, Python, Spark.,   Strong working knowledge of SQL Server, SQL Azure Database, No SQL, Data Modeling, Azure   AD, ADFS, Identity & Access Management.,   Develop and maintain ARM templates to automate the provisioning and configuration of Azure   resources, ensuring consistency and efficiency in deployments.   Monitor and optimize cloud infrastructure performance, scalability, and cost-efficiency by   implementing best practices and making recommendations for improvements.   Troubleshoot and resolve issues related to cloud infrastructure, ensuring high availability and   reliability of services.       DevOps Skills (Nice to Have):         Strong experience in CI and CD solutions (Jenkins, GitLab, Harness etc)   Experience in setting up and monitoring APM tools (New Relic, AppDynamics etc)   Device log monitoring solutions   Experience in containerization and orchestration tools (Docker, Kubernetes)   Include continuous testing and DevSecOps principles across pipeline stages.   Configure and administer Git and GitHub source code repos   Configure and automate scalable infrastructure with focus on security and cost optimization as   well.   Researches and comes up with relevant solutions to scale DevOps.         Key Responsibilities:       Work closely with customer to understand the customer business and provide feedback on   business value addition.   Design, Test & Build solutions using IaC   Perform peer review of code/solutions   Enhance/improve existing solutions   Train juniors to build team.                                                                                       ",1.00E+11,10-04-2024,09-07-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Accounting / Auditing,"Solution architecture, orchestration, GIT, Architecture, Data management, Data modeling, Enterprise architecture, Infrastructure, Operations, Python",-,9am-6pm,"Full Time, Permanent",PwC Service Delivery Center,Organization,PwC Service Delivery Center,https://img.naukri.com/logo_images/v3/1428182.gif,Kolkata,Kolkata,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Specialist I - Data Science (Scala+Spark),"PrimarySkills:Spark+Scala+AWS/anycloud 6+yearsofexperienceinSpark+Scalaismandatory Strongdevelopmentexposuretohighlyscalablebackendandmiddletier. FluencyinwritingmultithreadedprogramsrunningonLinux.usingScala+Spark(PrimarySkill)|Java+Spark(SecondarySkill) Strongunderstandingofobject-orienteddesign,datastructures,algorithms,profiling,andoptimization. GoodinRDBMSoranyofthelargedatasystemssuchas Hadoop,Cassandra,etc. GoodexperienceinwritingPythonorshellscripts. KnowledgeofGarbageCollection,andexperienceinGCtuning. Knowledgeofalgorithmslikesorting,heap/stack,queue,search,etc. ExperiencewithGitandbuildtoolslike Gradle/ Maven/SBT. Skills 6+yearsofexperienceinScala&SparkwithAWS2+Years",80524907713,08-05-2024,06-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Data Science, Maven, Java, Scala, Hadoop, Cassandra, Spark, Gradle",-,9am-6pm,"Full Time, Permanent",UST,Organization,UST,https://img.naukimg.com/logo_images/groups/v1/216316.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Technology Platform Engineer - SAP HCM On Premise ABAP,"Project Role : Technology Platform Engineer Project Role Description : Creates production and non-production cloud environments using the proper software tools such as a platform for a project or product. Deploys the automation pipeline and automates environment creation and configuration.  Must have skills : SAP HCM On Premise ABAP Good to have skills : SAP HCM Organizational Management Minimum  3  year(s) of experience is required Educational Qualification : 15 Years Continuous education Summary :As a Technology Platform Engineer, you will be responsible for creating production and non-production cloud environments using the proper software tools such as a platform for a project or product. Your typical day will involve deploying the automation pipeline and automating environment creation and configuration using SAP HCM On Premise ABAP as the primary skill.  Roles & Responsibilities: Design and develop SAP HCM On Premise ABAP solutions for cloud environments. Create and maintain automation pipelines for environment creation and configuration. Collaborate with cross-functional teams to ensure successful deployment of cloud environments. Troubleshoot and resolve issues related to SAP HCM On Premise ABAP in cloud environments. Professional & Technical Skills: Must To Have Skills:Experience in SAP HCM On Premise ABAP. Good To Have Skills:Experience in SAP HCM Organizational Management. Experience in creating and maintaining automation pipelines. Strong understanding of cloud environments and deployment. Experience in troubleshooting and resolving issues related to SAP HCM On Premise ABAP. Additional Information: The candidate should have a minimum of 3 years of experience in SAP HCM On Premise ABAP. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful solutions. This position is based at our Gurugram office. Qualification 15 Years Continuous education",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pipeline, java, sap hcm, troubleshooting, abap, python, sap, docker, sap hr, successfactors, piping, organizational management, devops, personnel administration, safety, linux, jenkins, alv reports, payroll, smartforms, sap abap, sap hana, oo abap",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Associate,"   A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics      We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology      We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge      Creating business intelligence from data requires an understanding of the business, the data, and the technology used to store and analyse that data      Using our Rapid Business Intelligence Solutions, data visualisation and integrated reporting dashboards, we can deliver agile, highly interactive reporting and analytics that help our clients to more effectively run their business and understand what business questions can be answered and how to unlock the answers                Location: Gurugram    Experience: 3-8 years    Qualification: Any    Role: Mainframe Developer; Data Engineer; AWS Specialist; Business Analyst    (Insurance Background)    Skills:      Mainframe - COBOL, JCL, DB2, VSAM, IMS, COBOL, JCL,    DB2, VSAM, IMS, CICS      Main Frame    Dveloper Associate    3-6 Years              Mandatory skill sets- Captives (GCC) - Mainframe Developer/Data Engineer/Business Analyst/AWS Specialist    Preferred skill sets- Hadoop, HIVE, SQL, Spark, CDC Tool with AWS Services (DMS, Glue, Glue Catalog, Athena, S3, Lake Formation) ETL Concept, SQL; BA-from Insurance Domain; Mainframe - COBOL, JCL, DB2, VSAM, IMS, COBOL, JCL, DB2, VSAM, IMS, CICS; Python, AWS API Gateway, AWS Lambda, SNS, S3, SQS, Event Notification, Event Bridge, CloudWatch DynamoDB, AWS SAM, VPC, Security, CloudFormation/Teraform, IAM, AWS CLI etc    Year of experience required- 2 - 5 Yrs    Qualifications- Any  ",1.81E+11,18-09-2023,17-12-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Accounting / Auditing,"VSAM, JCL, Db2, Cics, Cobol, Agile, Business intelligence, IMS, SQL, Python",-,9am-6pm,"Full Time, Permanent",PwC Service Delivery Center,Organization,PwC Service Delivery Center,https://img.naukri.com/logo_images/v3/1428182.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer,"           We are seeking a highly skilled and motivated DevOps Engineer to join our Infrastructure Automation team       If you have extensive experience with Databricks, AWS, Infrastructure as Code (Terraform), CI/CD tools (Jenkins, Bitbucket), Python and Shell scripting, we    encourage you to apply for this exciting opportunity to contribute to the growth of our Data Lakehouse platform     You will have the fun of working in a very   dynamic and forward-thinking team         As a DevOps Engineer within our Infrastructure Automation (Platform Engineering) team in Data & Analytics department, you will play a crucial role in designing, implementing, and maintaining the infrastructure that underpins our data lakehouse platform     You will work closely with cross-functional teams to streamline design and delivery processes     Your expertise in Databricks, AWS, Terraform, CI/CD tools (Jenkins, Bitbucket), Python, Shell scripting and Grafana, Prometheus will be instrumental in achieving high levels of automation, scalability and performance               Must-to-have Tool expertise:                 AWS, Terraform, CI/CD tools (Jenkins, Bitbucket), Python, Databricks                  Good-to-have Tool expertise:                 Shell scripting, Grafana, Prometheus                 Infrastructure Automation:                Design, build, and maintain infrastructure automation solutions using Terraform to ensure the robustness and scalability of our data lakehouse platform.               CI/CD Implementation:                Develop and enhance CI/CD pipelines using Jenkins and Bitbucket, with a focus on automation and code quality.               Databricks Management:                Configure, manage and optimize Databricks clusters and workspaces to facilitate data processing and analytics.               AWS Cloud Management:                Leverage AWS services like Storage, IAM, Network configs, etc to enhance the performance and scalability of our infrastructure.               Scripting:                   Create and maintain Terraform, Python and Shell scripts for Infrastructure provisioning automations.               Monitoring and Logging:                Implement robust monitoring and logging solutions to detect & troubleshoot platform infra issues proactively which will help the relevant Operations Team               Security and Compliance:                Ensure the security and compliance of all the infrastructures by implementing best practices and adhering to company policies.               Documentation:                Maintain comprehensive documentation for all infrastructure and automation processes.               Collaboration:                Collaborate with cross-functional teams, including data engineers, data scientists, BI engineers and Data Lakehouse Architects to understand the requirements and engineer the infrastructures for the entire Data Lakehouse Platform               Qualifications:                     Bachelors degree in Computer Science, Information Technology, or a related field                         Minimum of 3+ years of hands-on experience with Databricks and AWS services                         Expertise in Infrastructure as Code (Terraform)                         Strong CI/CD experience with Jenkins and Bitbucket                         Proficient in shell scripting                         Understanding of DevOps best practices, including automation, continuous integration, and continuous delivery                         Excellent problem-solving and troubleshooting skills                         Familiarity with Agile methodologies                         Strong communication and teamwork skills                         AWS, Databricks, Terraform certifications are a plus                       COURAGE:    Speak up when you see an opportunity; step up when you see a need..           OWNERSHIP:    Pick up the ball. Be proactive, take responsibility and follow-through.           INNOVATION:    Elevate to win. Be curious, test and learn new and better ways of doing things.           TEAMPLAY:    Win together. Work collaboratively and cultivate a shared mindset.           INTEGRITY:    Play by the rules. Hold yourself and others accountable to our company s standards.           RESPECT:    Value all players. Display empathy, be inclusive and show dignity to all     ",31123500820,03-11-2023,01-02-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Fitness & Wellness,"Automation, Compliance, Shell scripting, Agile, Data processing, Information technology, Analytics, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Adidas,Organization,Adidas,https://img.naukimg.com/logo_images/groups/v1/154040.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Staff Software Engineer - Python,"We are looking for an experienced Staff Software Engineer to join our team and take a leadership role in shaping our software solutions. As a Staff Software Engineer, you will play a pivotal role in designing, developing, and maintaining software systems that leverage Python web frameworks, microservices, deployment, DevOps practices, and front-end technologies.  Key Responsibilities:  - Design, develop, and maintain software applications using Python web frameworks such as FastAPI or Django.  - Implement and manage microservices, utilizing containerization (Docker) and orchestration (Kubernetes) for scalability and reliability.  - Apply best practices in deployment and DevOps methodologies to enable continuous integration and delivery.  - Work on the front-end and back-end components of our applications, leveraging Angular, React, or other relevant frameworks.  - Mentor and guide junior developers, sharing your expertise and knowledge. - Collaborate with cross-functional teams to ensure the successful execution of projects. - Participate in code reviews to ensure code quality, performance, and maintainability.  - Stay updated on emerging technologies and industry trends, and apply them where relevant.  Qualifications:  - Bachelor's degree in Computer Science, Software Engineering, or a related field. - 6-9 years of software engineering experience.  - Proficiency in Python and experience with relevant web frameworks, such as FastAPI or Django. - Strong understanding of microservices architecture and expertise in containerization (Docker), orchestration (Kubernetes), and scaling microservices.  - Deep knowledge of deployment and DevOps practices.  - Proficiency in front-end frameworks, including Angular or React. - Excellent problem-solving and analytical skills.  - Strong communication and collaboration skills.  Additional Skills (Desirable):  - Experience with database systems (SQL and NoSQL).  - Knowledge of CI/CD pipelines.  - Familiarity with Agile and Scrum methodologies.  - Understanding of cloud platforms (e.g., AWS, Azure, Google Cloud).",10524904549,01-05-2024,30-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Python, Azure, Agile methodologies, Angular, Google Cloud, SQL, microservices, DevOps, NoSQL, Docker, Scrum methodologies, AWS, Kubernetes",-,9am-6pm,"Full Time, Permanent",Stupa Sports Analytics,Organization,Stupa Sports Analytics,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer - Python/MongoDB,"       Canonical is looking for an experienced Python developer with a background in MongoDB who will help us to build a fully managed MongoDB solution based on Juju        Were aiming to build the most reliable and secure way to deploy and operate MongoDB on machines, VMs, public cloud and Kubernetes      The MongoDB team is small, meaning you will be a key contributor and your work will have a significant impact on the product and the broader ecosystem         Who you are          You love technology and working with brilliant people.          You are an accomplished senior Python programmer that enjoys challenging projects in mission-critical environments.          You have experience operating and managing MongoDB clusters.          You are experienced with modern infrastructure deployment automations or with traditional Linux systems administration, operations, and package management.          You are passionate about quality and automatic testing.      ",10524501700,01-05-2024,30-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Architecture / Interior Design,"Ubuntu, Linux, Analog, Cloud, Infrastructure, MongoDB, Open source, VMS, Python, Testing",-,9am-6pm,"Full Time, Permanent",LaunchPD,Organization,LaunchPD,-,REMOTE,REMOTE,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Developer,"   Design, develop and deliver secure, scalable, and high-performing digital business solutions on the Azure cloud platform     Build application/database solutions on Azure cloud using Azure capabilities and native services     Use application development processes to modernize and transform legacy business applications that will drive next-generation business   outcomes     It is a critical role that requires strong technical abilities, collaboration skills, technology experience, and communication skills     Good understanding of DevSecOps, automated tools, devtest environments, POD-based delivery model, agile project management, and security best practices while working on the cloud     Need to be a team player to collaborate with business, technology, and project management stakeholders         Work Experience   : 4-6 Years       Work Location   : Pune       Must-Have Skills   :        Microsoft Azure solution development using IaaS and PaaS capabilities. Other required skills include   C#, .NET, .NET Core, modern Web technologies, and API management.       Good understanding of the distributed application development using microservices, Azure services, APIs, and cloud-native patterns and anti-patterns on the Azure cloud       Good understanding of design patterns, anti-patterns       Familiarity with modern architecture techniques event-driven architectures, and stream processing.       Experience developing Microsoft Azure asynchronous messaging solutions.       Experience with containerization (Kubernetes, Docker, Azure stack) and Azure PaaS approaches and architecture paradigms.   Experience with Visual Studio, Azure DevOps, ASP.Net, .Net      Framework, and relevant native and third-party development tools         Good To Have Skills:         Relational Databases and managed databases       Familiarity with Agile POD-based delivery models       Experience in Azure native Data Engineering, Data Analytics, and AI/ML capabilities       Experience with Terraform and native Azure automation       Understanding of AWS or GCP platforms Confidential #2         Key Responsibilities   :        The developer will participate in all development and performance engineering aspects of the projects. Major responsibilities for this role include       Design and build greenfield applications using Microsoft platform technologies (ASP.NET Web API) and Azure capabilities/services.       Demonstrate deep expertise in C# Asynchronous Programming, Azure development experience (Web jobs, Azure Functions, Key vault, SQL, Storage), C#, .Net CORE web API, REST APIs, and database development. Other skills would have HTML5, AngularJS, or ReactJS experience. Tools: Visual Studio, Visual Studio Code, SSMS, Azure Data Studio, Azure Storage Explorer.       Develop solutions using Azure Data Factory, Azure Synapse pipeline, Azure data lake, Azure SQL databases, and CICD deployment and Microsoft tools like Visual Studio, Azure DevOps, GIT, ASP.Net, SQL Server, .Net Framework       Carry out quick POCs/ build MVPs on Microsoft Azure to rapidly demonstrate solution possibilities/feasibilities across Infrastructure, Storage, Analytics, AI/ML, Security etc,       Work in POD-based Agile delivery model       Establish best practices and guidelines across build-test-deploy processes. Standardize tools, checklists, and best practices.       Contribute to Intellectual Property (IP) development initiatives. It would include the development of specific playbooks, reference implementations, utilities, reusable codebases, runbooks, and more.       Follow best practices, guidelines, and automated tools and test scripts for the accelerated development of the applications       Accelerate cloud infrastructure deployment by creating Infrastructure as a Code (PowerShell, Terraform, and ARM templates) for the azure cloud resources and automate the infrastructure creation process.       Focus on delivery excellence. Validate delivered solutions meet the technical and functional/non-functional requirements       Learn, learn, and learn by staying up-to-date on all technical developments related to Azure and other public cloud platforms         Educational Qualification   : Bachelor s Degree in Computer Science, IT or equivalent work experience and foundations certifications like Microsoft Azure AZ-100 or AZ-103. ",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Automation, GIT, Test scripts, Agile, Application development, Visual Studio, Business solutions, Analytics, SQL",-,9am-6pm,"Full Time, Permanent",Nimbusnext,Organization,Nimbusnext,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure IOT Developer,"   Design, develop and deliver secure, scalable, and high-performing IOT solutions on the Azure cloud platform     Build applications on Azure cloud using Azure capabilities and native services     It is a critical role that requires strong technical abilities, collaboration skills, technology experience, and communication skills     Good understanding of DevSecOps, automated tools, dev-test environments, POD-based delivery model, agile project management, and security best practices while working on the cloud     Need to be a team player to collaborate with business, technology, and project management stakeholders         Work Experience   : 5 7 Years       Work Location   : Pune       Must-Have Skills   :       ?Microsoft Azure solution development using IaaS and PaaS capabilities, Monitor, troubleshoot, and optimize IoT solutions. Other required skills include       Hands-on experience in Azure IoT Hub/ Central/ platform/ end-to-end solution design.       Good Understanding on Provision and manage devices       Good Understanding in Implementing IoT Edge       Implemented business integrations       Should have through understanding in Process and manage data       Good Understanding in Implementing security       Good understanding of the distributed application development using microservices, Azure services, APIs, and cloud-native patterns and anti-patterns on the Azure cloud       Experience with containerization (Kubernetes, Docker, Azure stack) and Azure PaaS approaches and architecture paradigms.       Experience with Visual Studio, Azure DevOps and modern Web technologies,         Good To Have Skills   :   MS. NET Framework, MVC, ASP. NET Web API and NET Core.   Experience in Azure digital twins, Stream Analytics, Function app, Logic app, EventGrid, and Azure AD   SQL, Blob storage, Cosmos DB, Azure data lake storage, Time Series Insights, Power BI.       Key Responsibilities   :        The developer will participate in all development and performance engineering aspects of the projects. Major responsibilities for this role include       Design and build greenfield applications using Microsoft IOT platform and Azure capabilities/services. Confidential #2       Carry out quick POCs/ build MVPs on IOT based solution on Microsoft Azure to rapidly demonstrate solution possibilities/feasibilities.       Work in POD-based Agile delivery model.       Establish best practices and guidelines across build-test-deploy processes. Standardize tools, checklists, and best practices.       Contribute to Intellectual Property (IP) development initiatives. It would include the development of specific playbooks, reference implementations, utilities, reusable codebases, runbooks, and more.       Follow best practices, guidelines, and automated tools and test scripts for the accelerated development of the applications       Focus on delivery excellence. Validate delivered solutions meet the technical and functional/non-functional requirements       Learn, learn, and learn by staying up-to-date on all technical developments related to Azure and other public cloud platforms     Educational Qualification: Bachelor s Engineering Degree in Computer Science, IT or equivalent work experience/ or MCA and foundations certifications like Microsoft Azure AZ-220   ",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, PDF, Test scripts, PAAS, Agile, Application development, MVC, Visual Studio, Analytics, SQL",-,9am-6pm,"Full Time, Permanent",Nimbusnext,Organization,Nimbusnext,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
ELK Developer (Splunk/ Wazuh / Arcsight /ELK Developer),"Job brief We are looking for a candidate who have experience in as Developer to creating systems software and analyzing data  to improve existing systems or New innovation , along with develop and maintain scalable applications Monitor, troubleshoot, and resolve issues including deployments in multiple environments.  Candidate must be well-versed in computer systems and network functions. They should be able to work diligently and  accurately and should have great problem-solving ability in order to fix issues and ensure clients business functionalities. Requirements: Must have:  1. ELK development experience 2. Dev or DevOps experience on AWS cloud, containers, serverless code 3. Development stack of Wazuh and ELK. 4. Implement best DevOps practice 5. Tool set knowledge required for parser/ use case development, plugin customisation Regex, python,  yaml, xml . 6. Researching and designing new software systems, websites, programs, and applications. 7. Writing and implementing, clean, scalable code. 8. Troubleshooting and debugging code. 9. Verifying and deploying software systems. 10. Evaluating user feedback. 11. Recommending and executing program improvements. 12. Maintaining software code and security systems. 13. Knowledge of cloud system (AWS, Azure). 14. Excellent communication skills  Good to have:  SOC, security domain experience is desirable. Knowledge of Docker, Machine Learning, BigData, Data Analysis, Web-Scrapping.ata Analysis, WebScrapping. Resourcefulness and problem-solving aptitude Good understanding of SIEM solutions like ELK, Splunk, ArcSight etc.  Understanding of cloud platforms like Amazon AWS, Microsoft Azure and Google Cloud.  Experience in managing firewall / UTM solutions from Sophos, Fortigate, Palo Alto, Cisco FirePower Professional certification (e.g. Linux Foundation Certified System Administrator, Linux+ CompTIA,  RHCSA Red Hat Certified System Administrator). Experience with Linux and monitoring, logging tools such as Splunk, Strong scripting skills Hands-on experience in DevOps.  Main Responsibilities: 1. Responsible for building and setting up new development tools and infrastructure utilizing knowledge in continuous integration, delivery, and deployment Cloud technologies, Container Orchestration and Security. Build and test end-to-end pipelines, ensuring that systems are safe against security threats. 2. Deploying front end / backend applications in different environments  3. Works hand-in-hand with the frontend and backend engineering teams in all technical operations  and works to reduce or eliminate any repetitive or manual tasks. 4. Also works with junior DevOps personnel in improving health and performance issues of the business sites/software systems. 5. Promotes, documents, and implements systems infrastructure best practices, building tools that allow the department to develop/deploy. 6. Work with developers to design algorithms and flowcharts. 7. Produce clean, efficient code based on specifications. 8. Integrate software components and third-party programs. 9. Verify and deploy programs and systems. 10. Troubleshoot, debug and upgrade existing software. 11. Gather and evaluate user feedback. 12. Recommend and execute improvements. 13. Create technical documentation for reference and reporting. 14. Develop an initial Implementation plan based on the clients requirements and available resources. 15. Advise clients on the configuration of the system to support their business practices. 16. Support clients in working through implementation issues and potential roadblocks\ Perform installation and configuration of software. 17. Working closely with the development teams to integrate the Platform Creation and Implementation of scripts as per the product/project customization. 18. Provide documentation and end-user training for applications. 19. Providing internal training to the team to ensure proper handshake of knowledge transfer. SIEM/SOC Platform Development & Management: Configure / Maintain Linux Server platforms Ubuntu, CentOS, Redhat and others. Configure / maintain Linux server security. Configure / maintain LVM, DNS, Network Settings of operating systems.  Configure / maintain SIEM Tools (ELK) Developments of Parsers Scheduled proactive activities and patching support. Configure / maintain backup solutions.",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Software Product,"Software Architectural Design, Open source Development, Cloud computing, Data Structures, Splunk Development, Elastic Stack, ELK Developer, Software Development, Elasticsearch, Wazuh, Front-End Development, Open source, Splunk, REST APIs",-,9am-6pm,"Full Time, Permanent",Soffit Infrastructure Services,Organization,Soffit Infrastructure Services,-,Kochi,Kochi,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google Cloud Platform Developer,"     NITYO INFOTECH is looking for Google Cloud Platform Developer to join our dynamic team and embark on a rewarding career journey.      Developing, testing, and deploying scalable and secure applications and services on Google Cloud Platform (GCP).     Designing and implementing cloud-native solutions using GCP services such as Compute Engine, Kubernetes Engine, and Cloud Storage.     Developing and implementing automation scripts using tools such as Google Cloud SDK, Google Cloud Deployment Manager, and Terraform.     Ensuring the security and compliance of cloud infrastructure and applications using GCP services such as Identity and Access Management (IAM), Security Command Center, and Cloud KMS.       ",2.81E+11,28-11-2023,26-02-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"Automation, Compliance, Access management, GCP, Cloud, cloud storage, Infrastructure, Deployment, SDK, Testing",-,9am-6pm,"Full Time, Permanent",Nityo Infotech,Organization,Nityo Infotech,https://img.naukimg.com/logo_images/groups/v1/76234.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google Cloud Platform Developer,"     NITYO INFOTECH is looking for Google Cloud Platform Developer to join our dynamic team and embark on a rewarding career journey.      Developing, testing, and deploying scalable and secure applications and services on Google Cloud Platform (GCP).     Designing and implementing cloud-native solutions using GCP services such as Compute Engine, Kubernetes Engine, and Cloud Storage.     Developing and implementing automation scripts using tools such as Google Cloud SDK, Google Cloud Deployment Manager, and Terraform.     Ensuring the security and compliance of cloud infrastructure and applications using GCP services such as Identity and Access Management (IAM), Security Command Center, and Cloud KMS.       ",2.81E+11,28-11-2023,26-02-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"Automation, Compliance, Access management, GCP, Cloud, cloud storage, Infrastructure, Deployment, SDK, Testing",-,9am-6pm,"Full Time, Permanent",Nityo Infotech,Organization,Nityo Infotech,https://img.naukimg.com/logo_images/groups/v1/76234.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Java Developer with AWS,"       Responsibility Must be hands-on who has built the solutions using Java before and now providing solutions using AWS / Java / Springboot / NodeJS / JavaScript / Microservices architecture. Responsibilities include leading a team and interacting with the client.      Essential Skills AWS Experience with Java is a must AWS Knowledge - Lambda, ECS, S3, AppSync, Dynamo DB, API Gateway, EC2, Step Function, Cloudwatch, Datadog, EKS. CloudFormation, VPC Java Springboot NodeJS/JavaScript     Desired Skills Language - Java, J2EE, JMS, Micro Service, Spring Cloud, Spring Integration. Other - JMS(Like MQ, Tibco), Kafka/Kinesis Database - Oracle, MySQL, Any No SQL DB like Couchbase, MongoDB Angular/css/html Good to have knowledge in tools: Harness [United is going to adopt in future project] / TeamCity/ Jira ID: STS or Eclipse/ IntelliJ/ VSCode Version Control : Git Team Collaboration Tool: Miro      Other Information Educational Qualifications Bachelor's degree is required Experience 5 yrs    ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"JMS, Version control, GIT, Eclipse, MySQL, Javascript, TIBCO, HTML, JIRA, SQL",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Scala Developer,"   # 2 Scala Functional Programmer - 7-14yrs - Remote     Level - Mid- Senior     Exclude - Big data, Spark Scala     Include - Akka, Hand on exp. with Scala Functional Programming, Extensive knowledge of Scala libraries such as cats, cats effect etc     6+ years of relevant experience working as a Scala Developer     Experience in Scala development and Functional Programming     Experience with FP libraries in Scala, particularly the Typelevel     stack, ie Cats, Cats Effect, Http4s, Circe, Doobie     Prior experience working with AWS services and pipelines     Knowledge of relational / NoSQL database systems.     Use CI tooling, one-button deployment to Cloud with a DevOps mentality.     Proficient in English     Results oriented     Proactive and displays initiative; seeks out information and detailed answers, etc without needing to be directed to do so     Successfully engages with limited supervision in a fast-paced environment         ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Usage, NoSQL, devops, SCALA, Cloud, Programming, Database, Deployment, big data, Supervision",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Kafkalead developer,"     Design and implement Kafka-based applications and solutions         Build and maintain message configuration and flows         Provide issue analysis on Kafka applications         Analyse issues with Kafka software and provide detailed solutions         In-depth knowledge of all the functionalities surrounding Kafka         Ability to install, maintain and troubleshoot Kafka         Extensive experience with messaging and stream processing on Kafka         Ability to set up and configure Kafka brokers         Experience with Kafka Connector and Apache Zookeeper         Experience with the Confluent version of Kafka and its many related         components         Design and implement APIs         Design and implement event-driven microservices         Maintain performance of applications and databases         Translate written requirements and specifications         Develop software on written requirements         Assist with MDM strategy and Process design     ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Process design, Apache, Troubleshooting, microservices",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Data Factory/Data Brick Developer,"     Develop and implement data transformations using Pyspark and Databricks      Work on Azure Data Bricks, Azure Data Lake, Azure SQL, Azure blob storage, Azure logic apps, Azure Functions, Azure Synapse, and Azure Purview      Utilize big data concepts such as Hive and Spark framework      Write complex SQL queries and have a strong understanding of DWH concepts      Hands-on experience with Python or Scala      Coordinate independently with business stakeholders to understand and fulfill requirements      Implement requirements using Azure Data Factory and Data Brick      Knowledge of DevOps and Agile methodologies-based projects      Version control using Git/Bitbucket      Basic understanding of Batch Account configuration and monitoring options            Candidate Qualifications:          Bachelors degree in Computer Science, Engineering, or related field      Minimum of 3 years of experience as a Data Engineer or related role      Sound knowledge of Pyspark and Databricks      Experience with Azure services such as Azure Data Lake, Azure SQL, Azure blob storage, Azure logic apps, Azure Functions, Azure Synapse, and Azure Purview      Strong understanding of big data concepts and SQL      Hands-on experience with Python or Scala      Prior experience with Azure Data Factory and Data Brick      Excellent communication and problem-solving skills      Ability to work independently and collaboratively in a team environment      ",2.71E+11,27-10-2023,25-01-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Version control, GIT, SCALA, Agile, big data, SQL Azure, Monitoring, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Genesys Cloud CX Developer,"   Solution Design: Collaborate with stakeholders to gather requirements and design custom solutions that leverage Genesys Cloud CX capabilities to improve customer experiences     Custom Development: Develop and customize applications, integrations, and workflows within the Genesys Cloud CX platform using its API, SDKs, and scripting capabilities     Integration: Integrate Genesys Cloud CX with other systems and data sources, such as CRM systems, databases, and third-party applications, to create a unified customer experience     Automation: Implement automation and workflows to streamline customer interactions and agent tasks, improving efficiency and consistency     Routing and Queuing: Configure call and interaction routing strategies to ensure customers are connected to the right agents or resources     Scripting: Create and maintain scripts for IVR (Interactive Voice Response) systems, chatbots, and other customer self-service channels     Reporting and Analytics: Implement reporting and analytics solutions to track key performance indicators, monitor customer interactions, and gain insights into customer behavior   ",2.61E+11,26-10-2023,24-01-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Solution design, Automation, IVR, Cloud, Routing, Customer experience, Analytics, Monitoring, CRM, Scripting",-,9am-6pm,"Full Time, Permanent",Nbits,Organization,Nbits,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Engineer/DevOps Developer," ?         Experience With Cloud (Gcp Preferrable)     Proficiency with Git and GitHub workflows     Experience working on Linux based infrastructure     Excellent understanding of Node and JavaScript     Configuration and managing databases such as MySQL, Mongo     Awareness of critical concepts in DevOps and Agile principles     Familiar with algorithms, data structures, complexity analysis and software design             Preferred qualifications:           Experience With Cloud (Gcp Preferrable)     Experience as a DevOps engineer or in a similar software engineering role     Proficiency with Git and GitHub workflows     Experience working on Linux based infrastructure     Excellent understanding of Node and JavaScript     Configuration and managing databases such as MySQL, Mongo     Awareness of critical concepts in DevOps and Agile principles     Familiar with algorithms, data structures, complexity analysis and software design                   About the Job           We are looking for passionate and talented individuals to build awesome products using Google Cloud Platform.                   Responsibilities           Implementing various development, testing, automation tools, and IT infrastructure     Setting up tools and required infrastructure     Encouraging and building automated processes wherever possible     Develop software to integrate with internal back-end systems      Perform root cause analysis of production errors and resolve technical issues     Develop scripts to automate visualization     Design procedures for system troubleshooting and maintenance       ",2.41E+11,24-07-2023,22-10-2023,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Real Estate,"Software design, github, GIT, Linux, GCP, MySQL, Javascript, Agile, Infrastructure, Data structures",-,9am-6pm,"Full Time, Permanent",Agprop,Organization,Agprop,-,New Delhi,New Delhi,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Developer Analyst," UL Technology Solutions is looking for Developer Analyst to join our dynamic team and embark on a rewarding career journey        As a Developer Analyst specializing in Python, you will play a key role in developing and maintaining software solutions to meet the needs of our organization      You will work closely with cross-functional teams to analyze requirements, design solutions, and implement Python-based applications and tools        Responsibilities:        Collaborate with business stakeholders to understand requirements and translate them into technical specifications      Design, develop, and maintain Python-based applications and tools to support various business functions      Write clean, efficient, and maintainable code following best practices and coding standards      Perform code reviews and provide constructive feedback to team members      Troubleshoot and debug issues in existing Python codebase      Work with data analysts to integrate data sources and develop data processing pipelines using Python libraries such as Pandas and NumPy      Collaborate with DevOps engineers to deploy and maintain Python applications in cloud environments (eg, AWS, Azure, GCP)      Stay up-to-date with the latest trends and technologies in Python development and propose improvements to existing processes and tools    ",1.40E+11,14-02-2024,14-05-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"c#, rest, python, css, software development, oracle, web services, owasp, dbms, hibernate, vulnerability assessment, javascript, sql server, jquery, sql, spring, coding, spring boot, java, j2ee, code review, html, mysql",-,9am-6pm,"Full Time, Permanent",ULTS,Organization,ULTS,https://img.naukimg.com/logo_images/groups/v1/552766.gif,Kozhikode,Kozhikode,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Engineer/DevOps Developer,"       ?                   Experience With Cloud (Gcp Preferrable)     Proficiency with Git and GitHub workflows     Experience working on Linux based infrastructure     Excellent understanding of Node and JavaScript     Configuration and managing databases such as MySQL, Mongo     Awareness of critical concepts in DevOps and Agile principles     Familiar with algorithms, data structures, complexity analysis and software design             Preferred qualifications:           Experience With Cloud (Gcp Preferrable)     Experience as a DevOps engineer or in a similar software engineering role     Proficiency with Git and GitHub workflows     Experience working on Linux based infrastructure     Excellent understanding of Node and JavaScript     Configuration and managing databases such as MySQL, Mongo     Awareness of critical concepts in DevOps and Agile principles     Familiar with algorithms, data structures, complexity analysis and software design                   About the Job           We are looking for passionate and talented individuals to build awesome products using Google Cloud Platform.                   Responsibilities           Implementing various development, testing, automation tools, and IT infrastructure     Setting up tools and required infrastructure     Encouraging and building automated processes wherever possible     Develop software to integrate with internal back-end systems      Perform root cause analysis of production errors and resolve technical issues     Develop scripts to automate visualization     Design procedures for system troubleshooting and maintenance                 ",1.31E+11,13-07-2023,11-10-2023,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Real Estate,"Software design, github, GIT, Linux, GCP, devops, MySQL, Javascript, Agile, Data structures",-,9am-6pm,"Full Time, Permanent",Agprop,Organization,Agprop,-,New Delhi,New Delhi,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Mendix Developer,"   We are seeking a skilled and experienced Mendix Developer to join our dynamic team     The ideal candidate should have a strong background in software development, an excellent understanding of the Mendix low-code platform, and the ability to design and develop innovative applications to meet business needs         Responsibilities:         Collaborate with business analysts and stakeholders to understand application requirements and objectives     Design, develop, and maintain Mendix applications using low-code development principles     Configure and customize Mendix modules and widgets to create responsive and user-friendly web and mobile applications     Ensure the applications meet high standards of quality, performance, and security     Perform regular testing, debugging, and troubleshooting to identify and resolve issues promptly     Work on integrating Mendix applications with other systems and platforms     Provide technical support and assistance to end-users and address their queries and concerns     Stay up-to-date with Mendix platform updates and industry best practices     Collaborate with cross-functional teams, including UI/UX designers, QA engineers, and project managers     Document application development processes, code, and configurations   ",71223500243,07-12-2023,06-03-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"QA, Web technologies, Business Analyst, Debugging, Application development, Mobile applications, Troubleshooting, Technical support, Testing",-,9am-6pm,"Full Time, Permanent",Global Pharma Tek,Organization,Global Pharma Tek,https://img.naukimg.com/logo_images/groups/v1/1012164.gif,Ahmedabad,Ahmedabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Analyst - Data Engineer - IPA - DAP,"   We are in search of strong Developer who is technically passionate,  solution-focused,  and able to contribute to design,  development,  test automation and hardening (security,  stability,  deployment) of our in-house-developed pricing & risk components.     We live by our Orange Code that is built upon our values (We are honest,  we are prudent,  and We are responsible) and is reflected in our behaviors (We take it on and make it happen,  we help others to be successful and We are always a step ahead).  We expect to get a buy-in on them from successful candidates.           As a Software Engineer you will:        Design and implement complex new functionality with a strong focus on   reliability  ,    performance,    and large data sets.       Influence technology choices   and   architecture   of newly built components and services.     Use of   Java or Python,  BigQuery,  dBT,  Looker      Good with   Docker,  Kubernetes and Terraform        Good with GCP   or at least AWS or Azure      Good with SQL      Good to have decent knowledge on Dataproc.     Through our One   Agile   Way of Working process interpret business requirements with the Product Owner and Feature Engineer during the refinement sessions,  break down stories into the tasks,  estimate the Story Points to develop,  test and harden a feature with your colleagues,  and deliver and support it on the production environment to our global users.     ",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Banking,", Automation testing, GCP, Senior Analyst, Agile, Design development, AWS, SQL, Python, Testing",-,9am-6pm,"Full Time, Permanent",Crisil,Organization,Crisil,https://img.naukimg.com/logo_images/groups/v1/340662.gif,"Mumbai, Pune","Mumbai, Pune",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Artificial Intelligence Consultant,"Team: - Strategy & Consulting Practice: - Marketing Analytics - Conversational AI Job location: - Hyderabad Should have hands on experience with at least one Machine learning algorithm, Logistic Regression, Random Forest, K-Means Clustering.  Fallout Analytics, NLP, Journey Analytics, Audio Personification, Customer analytics  Concepts of Intent, Entity, Fallback Management and Context Management  AI bot using Dialogflow / Rasa / EX / ES.  Webhooks and Ngrok, programming language like Python, SQL - Advanced Level.  Omni Channel Integration  Regression Testing  Node.js  Understanding of CI/CD pipelines  Advanced Level.  Experience with proposing solution architectures.",60524901285,06-05-2024,04-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"Artificial Intelligence, Logistic regression, NLP, Machine learning, Node.js, Customer analytics, Python, SQL, Random Forest",-,9am-6pm,"Full Time, Permanent",Mount Talent Consulting Private Limited,Organization,Mount Talent Consulting Private Limited,https://img.naukri.com/logo_images/v3/259646.gif,"Mumbai, Gurugram, Bengaluru","Mumbai, Gurugram, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Urgent Job Opening For Perl & shell Scripting Profile .,"    : Perl Script, XML with XSLT/ XQuery, Shell Script : Python    Shift: 5 PM to 2:30 AM IST",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"shell script, XML, Perl Script, XSLT",-,9am-6pm,"Full Time, Permanent",Techbliss Digital Solution,Organization,Techbliss Digital Solution,-,"Noida, Pune, Bangalore Rural","Noida, Pune, Bangalore Rural",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer (Python | MLOps | Platform Processing),"       ?     You will join our team of world-class experts developing the AlphaSense platform. The team is right at the very core of what we do and responsible for implementing cutting-edge technology for scalable, distributed processing of millions of documents.     We are seeking a highly skilled Senior Software Engineer to join our dynamic team responsible for building and maintaining data ingestion systems at scale. As a key member of our team, you will play a crucial role in designing, implementing and optimizing robust solutions for ingesting millions of documents per month.           What You ll Do:         Design, develop, and maintain scalable data ingestion pipelines to process large volumes of documents efficiently and reliably.     Collaborate with cross-functional teams to understand requirements and translate them into technical specifications and system designs.     Implement best practices for data ingestion, storage, and processing to ensure high performance, scalability, and reliability.     Integrate ML/LLM models into existing data pipelines, leveraging appropriate technologies     Optimize data ingestion workflows to improve throughput, reduce latency, and minimize resource utilization.     Monitor system performance, troubleshoot issues, and implement solutions to address bottlenecks and ensure smooth operation.     Stay up-to-date with emerging technologies and industry trends related to data ingestion and multimedia processing, and propose innovative solutions to enhance our capabilities.     Work closely with Product Management to translate product requirements into software architectures     Follow the engineering processes, DevOps practices, and trends inside the company (monitoring, alerting, performance optimization, integration testing, design documentation) and make sure the teams improve related knowledge as well             Candidate Requirements:         Must-Have         Bachelors or Masters degree in Computer Science, Engineering, or a related field.     Minimum 4 years of experience in Software Development with 3+ years of proficiency in Python      Working knowledge of any cloud service provider(preferably AWS)     Experience with working on Dockers, K8s     Experience in python based web frameworks like Django, Fast API or Flask     Excellent verbal and written communication skills with the ability to share thoughts and ideas, concisely and persuasively.       Nice to have       Proficiency in programming languages such as Java, or GO, and experience with related frameworks and libraries for data processing and manipulation.      Knowledge of document scraping techniques specially with PDFs     Exposure NoSQL databases like MongoDB, DynamoDB, etc     Experience in MLOps/LLMOps building and/or deploying ML/LLM platforms         ",1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Product management, Computer science, NoSQL, Django, Integration testing, Market intelligence, Programming, Data processing, MongoDB, Python",-,9am-6pm,"Full Time, Permanent",Alphasense,Organization,Alphasense,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer (Python | MLOps | Platform Processing),"     About AlphaSense:       AlphaSense provides an AI-based search engine for market intelligence, used by the largest and fastest-growing firms globally. Our mission is to curate and semantically index the world s market and company information, including the vast high-value content sets that traditional web search engines cannot reach. With 3500+ enterprise clients, AlphaSense helps knowledge professionals become dramatically more productive, and gain an information edge by discovering critical data points and trends that others miss.      Check out what we ve built so far:         1. The decision that matters          2. India Office -             The Role:        You will join our team of world-class experts developing the AlphaSense platform. The team is right at the very core of what we do and responsible for implementing cutting-edge technology for scalable, distributed processing of millions of documents.     We are seeking a highly skilled Senior Software Engineer to join our dynamic team responsible for building and maintaining data ingestion systems at scale. As a key member of our team, you will play a crucial role in designing, implementing and optimizing robust solutions for ingesting millions of documents per month.           What You ll Do:         Design, develop, and maintain scalable data ingestion pipelines to process large volumes of documents efficiently and reliably.     Collaborate with cross-functional teams to understand requirements and translate them into technical specifications and system designs.     Implement best practices for data ingestion, storage, and processing to ensure high performance, scalability, and reliability.     Integrate ML/LLM models into existing data pipelines, leveraging appropriate technologies     Optimize data ingestion workflows to improve throughput, reduce latency, and minimize resource utilization.     Monitor system performance, troubleshoot issues, and implement solutions to address bottlenecks and ensure smooth operation.     Stay up-to-date with emerging technologies and industry trends related to data ingestion and multimedia processing, and propose innovative solutions to enhance our capabilities.     Work closely with Product Management to translate product requirements into software architectures     Follow the engineering processes, DevOps practices, and trends inside the company (monitoring, alerting, performance optimization, integration testing, design documentation) and make sure the teams improve related knowledge as well             Candidate Requirements:         Must-Have         Bachelors or Masters degree in Computer Science, Engineering, or a related field.     Minimum 4 years of experience in Software Development with 3+ years of proficiency in Python      Working knowledge of any cloud service provider(preferably AWS)     Experience with working on Dockers, K8s     Experience in python based web frameworks like Django, Fast API or Flask     Excellent verbal and written communication skills with the ability to share thoughts and ideas, concisely and persuasively.       Nice to have       Proficiency in programming languages such as Java, or GO, and experience with related frameworks and libraries for data processing and manipulation.      Knowledge of document scraping techniques specially with PDFs     Exposure NoSQL databases like MongoDB, DynamoDB, etc     Experience in MLOps/LLMOps building and/or deploying ML/LLM platforms             Want to hear more       You can apply by sending your LinkedIn or resume through the application form.     For more information, please contact Kailash Roy on LinkedIn or by email: kroy@alpha-sense.com .   ",1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Product management, Computer science, NoSQL, Django, Integration testing, Market intelligence, Programming, Data processing, MongoDB, Python",-,9am-6pm,"Full Time, Permanent",Alphasense,Organization,Alphasense,-,New Delhi,New Delhi,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Lead Software Engineer,"         Lead for Cloud Applications, you will be responsible for the Design / Implementation / Maintenance of Cloud-Based Applications for SCS.                  This role also requires strong programming skills in AWS, Java, Scala and Data Streaming technologies        You will be required to triage issues closely working with development and support teams for the resolution      This role require excellent Analytical problem solving skills, be able to understand application architecture and dataflow and be able to adapt quickly to new and different problems day to day      Be able to discover the team training needs and provide the technical trainings to the team; be able to oversee the day-to-day tasks/operations                       Role Description:                            Experience in Designing, Creating and maintaining applications in Scala and Java.                      Experience of designing AWS cloud-native Application Architecture.                      Experience of developing and deploying cloud-native applications including serverless environments like Lambda or CaaS platforms like ECS, EKS, etc.                      System Integration Experience with integrating multiple applications with different types of interfaces.                      Application development skills on AWS Platform lambda, API Gateway, Cloud watch, RDS, DynamoDB etc.                      Experience with Docker and Kubernetes.                      Experience with monitoring solutions such as CloudWatch, Dashboards etc.                      Should have Knowledge on MuleSoft Integration Platform, JIRA, Confluence, Cherwell Ticketing Tool, Azure Cloud, Splunk, Newrelic, Grafana.                      Expert Reactive Architectural , functional programming expertise Architectural skills Reactive, Event-Driven                      Experience in coordination of onshore/offshore team with shifts bases to support 24 7 live for all applications to make sure they are working as expected within the company.                      Experience in Troubleshooting of runtime issues associated with Middleware Integration Applications and provide the detailed RCA to fix the issues permanently.                      Exposure on Data Streaming technologies e.g; Kafka, Akka Streams, AWS Kinesis.                      Should identify technical dependencies across several product and application teams to support the implementation of new or enhanced functionality.                      Collaborates with Solution Architects, Product management, and delivery teams to bridge missing links on solution architectures.                      Solidifies high-level requirements to support the definition and refinement of a robust testing strategy, and supports the planning and execution of functional and non-functional testing activities                      Defines remediation tactics for large and complex defects affecting several features and/or capabilities within a given product or application.                      Participates in release planning, deployment and knowledge transfer activities.                      Ability to lead, triage prioritize critical incidents Collaborate within an agile dynamic development environment eager to learn technologies.                      Continually improve personal knowledge and skills relevant to current enterprise application architecture and integrations.                      Experience in following the standard ITIL ticketing procedures include Incident, Change, Problem, Task etc. Should maintain the SLA for each ticket with sufficient documentation provided. Should be able to represent the team s change requests in CAB calls.                      Should lead and train the New Joiners, Monitor team performance and report on metrics, Discover training needs and provide required training for the team, Oversee day-to-day operation, Listen to team members feedback and resolve any issues or conflicts.                      Should set clear team goals, Delegate tasks and set deadlines, Motivate team members, Create an inspiring team environment with an open communication culture, Encourage creativity and risk-taking, Suggest and organize team-building activities, Motivate team members.                                            Key Qualifications:                                            Bachelor s Degree with 5 to 10+ years of experience in computer science / Information Technology.                      Retail experience is required with experience in one or more of the following Scala, Data Streaming Technologies (Kafka, Akka, Spark)                      Expert development knowledge with containerized workloads (Docker Kubernetes)                      Experience with AWS Compute Loads (e.g; AWS Lambda)                      Knowledge on IBM MQ, Amazon SQS etc,                      Experience with Reactive Architectures                      Working knowledge on Relational Databases.                      Strong verbal and written communications skills, with an ability to express complex technical concepts                      Excellent analytical, problem-solving, and conceptual skills                    Preferred Qualifications (nice to haves):                                  Ready to work in shifts (First general and second)                                Knowledge in AWS, Nagios                                          Logical approach and process to problem solving                                          Thrive in a busy, high-pace environment and can easily maintain your composure under pressure                                          Understand how problems affect other areas of the company outside of the IT space                                  Strive for excellence and lead with a business focus mindset                                  You are a quick learner with the ability to learn new technologies                                          Knowledge in Docket, Kubernetes, Kafka, AWS Step Functions, AWS Glue etc            ",90524501779,09-05-2024,07-08-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,Retail,"Product management, RCA, Analytical, Application development, Troubleshooting, Information technology, Analytics, Monitoring, Team building",-,9am-6pm,"Full Time, Permanent",Hudsons bay Company (HBC),Organization,Hudsons bay Company (HBC),https://img.naukimg.com/logo_images/groups/v1/4111248.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer (Azure databricks),?     Role: ?Senior Software Engineer     Experience:  4 to 8 yrs     Location: ?Bangalore    ?     Responsibilities:     Data Pipeline Design and Development Collaborate with data scientists analysts and other stakeholders to understand data requirements Design develop and implement efficient data pipelines integrating various data sources to transform raw data into usable formats   Cloud Data Platform Utilize your expertise in working with cloud based data platforms especially Azure to architect deploy and maintain scalable and reliable data infrastructure   Databricks Expertise Demonstrate a strong understanding of Databricks and its capabilities using it as a primary platform for big data processing analytics and machine learning?   ETL Extract Transform Load Processes Implement ETL processes to extract data from diverse sources transform it into suitable formats and load it into the data warehouse or analytical systems?   Data Security and Compliance Ensure data security and compliance with data privacy regulations throughout the data engineering process?   Collaboration Work closely with cross functional teams including data scientists analysts and business stakeholders to understand data requirements share insights and provide technical support?   Troubleshooting and Issue Resolution Proactively identify data related issues and provide timely resolutions to maintain smooth data operations?   Documentation Maintain comprehensive documentation for data pipelines architecture and processes making it easier for team members to understand and contribute to the data engineering initiatives      ?    Roles and Responsibilities   Senior Software Engineer RR/527/2024,60524001846,06-05-2024,04-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure databricks, azure data lake, data, analytical, data processing, microsoft azure, azure data factory, data pipeline, engineering, warehouse, machine learning, data engineering, sql, data bricks, cloud, analytics, troubleshooting, software engineering, etl, big data, architecture",-,9am-6pm,"Full Time, Permanent",Emids,Organization,Emids,https://img.naukimg.com/logo_images/groups/v1/4593535.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer - Python / Django,"     We are looking for a dynamic Python / Django Senior Software Engineer to join our product development center at Kochi, India.      Your responsibilities will include developing and implementing a SaaS based product in a cloud environment.      As a Software Engineer you will collaborate with our Customers, Product Management, Development, Quality Assurance team over Dubai/Kochi/Bangalore in day to day basis.      Your insightful contribution will help develop, expand our product portfolio of Beinex.              Responsibilities              Should be familiar with scrum methodology of the software development process.                  Understand the product requirements and convert to design and deliverable units as part of sprints.                  Follow the latest industry developments and stay up to date on corporate competitors.                  Key Skills Requirements              At least 3+ years of experience as a software engineer with specific experience building back-end systems.                  Experience with Django, Python, MySQL, RDS, Docker, and AWS are all desirable.                  Experience developing REST APIs specifically for native mobile applications is also desirable.                  Software development experience as an individual contributor.                  Strong development experience in Python/Django/Flask frameworks.                  Strong problem-solving and creative skills.                  Excellent team player, with emphasis on priorities and goal setting                  Superior presentation and communication skills, both written and verbal                  Sharp Technical skills. Adaptive to new technologies and frameworks.                  Minimum Bachelors Engineering Degree / MCA in related field              ",1.20E+11,12-03-2024,10-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Management Consulting,"Product management, Manager Quality Assurance, Django, Product portfolio, SAAS, Cloud, Python",-,9am-6pm,"Full Time, Permanent",Beinex Consulting,Organization,Beinex Consulting,-,Kochi,Kochi,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer,"As a Platform Engineer, you will be responsible for developing, deploying, monitoring, and managing containerized applications using our IaaS model. You will be responsible for developing groundbreaking technologies to drive our CI/CD process. You will work closely with cross-functional teams to ensure the scalability, reliability, and performance of our platform. The ideal candidate should have a strong background in software engineering, infrastructure design, and deployment automation. Responsibilities : Design, develop, and maintain our platform infrastructure to ensure scalability, reliability, and performance. Collaborate with cross-functional teams to understand requirements and translate them into technical solutions. Implement and maintain deployment automation processes to streamline the platform deployment and configuration. Monitor and troubleshoot platform issues, ensuring timely resolution and minimal downtime. Conduct performance analysis and optimization to improve the overall platform efficiency. Stay up to date with the latest industry trends and technologies and recommend improvements to enhance the platform. Develop, test, and maintain builds and deployments for CI/CD pipelines. Develop tools to support frequent deployments of containerized applications on major cloud platforms including AWS, GCP, Microsoft Azure Automate existing manual processes and write clear design documents. Skill Requirements: 5+ years of experience building, automating and deploying using tools like GitHub, Jenkins, Ansible, Groovy, Terraform, and Helm charts. 5+ years of experience with CICD build systems and Automated Testing 3+ years of programming experience in Python (or other high-level languages) 3+ years of experience with Kubernetes (AKS/EKS/GKE/OpenShift) 3+ years of experience with major cloud platforms (AWS/GCP/Microsoft Azure) Familiarity with Linux or other Unix-like Operating Systems with the ability to troubleshoot network issues. Experience with the Agile software development process Excellence in technical communication with peers and non-technical people alike Education Typically requires a Bachelors degree and 5+ years of relevant industry experience. Demonstrated leadership abilities in an engineering environment in driving operational excellence, big picture thinking and best practices.",80524907885,08-05-2024,06-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Software Product,"Python, kubernetes, automation testing, GitHub, linux, jenkins, software engineering, terraform, aws, Groovy, ansible",-,9am-6pm,"Full Time, Permanent",Netapp,Organization,Netapp,https://www.naukri.com/hotjobs/images/v3/netapp_nov13.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer - SRE,"     Design, implement, deliver and operate an internal development platform for the wider Engineering organisation     Work with product development teams to understand the needs for building, testing and deploying releases, and translate those needs to standardized platform components     Build self-service tooling and capabilities for development and operations teams to provision, manage and operate cloud infrastructure     Build CI/CD tooling and processes that accelerate the entire product release cycle (from delivering cloud infrastructure, all the way to customer facing features)     Participate in all stages of software development and collaborate with different stakeholders like Architecture, Security, Operations and Product Management     Help the team maintain high code quality standards and improve best practices      Work with the agile iterative/incremental mindset     Be open-minded to learn new technologies and programming languages when needed               Bachelor s degree in Computer Science, Computer Engineering or related field         Minimum 6-11     years of experience in software development     , ideally on a platform team         Ex",70524500237,07-05-2024,05-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Product management, Computer science, Automation, Networking, Configuration management, Agile, HTTP, Security operations, SSL, Monitoring",-,9am-6pm,"Full Time, Permanent",Nexthink,Organization,Nexthink,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Solutions Engineer (India),"   Collaborate with and support both open source developers and Onehouse customers by triaging/troubleshooting technical issues to resolution.          Triage, escalate, and resolve issues with a sense of urgency and commitment to keeping our customers happy.     Develop solutions and contribute to open source and core product development by building run-books, processes, and procedures to scale support programs.     Analyze trends of customer pain and directly partner with our Product Management and Engineering teams to influence the roadmap for the product.       What You Bring to the Table:        1+ years of experience working directly with customers on technical engagements.     1+ years of experience with major cloud providers like AWS, GCP or Azure.     Strong verbal and writing communication skills with a track record of simplifying technical explanations.     Hand-on technical experience building, operating, and troubleshooting data engineering pipelines.     Working knowledge of a few modern data tools and frameworks like Apache Spark, Kafka, Flink, Presto/Trino, Hive, DBT, Airflow, Parquet, Avro, ORC.     Familiarity with some cloud data services like EMR, Redshift, Kinesis, Glue, Dataproc, BigQuery, Databricks, HDInsight, Synapse.     Ability to collaborate with multiple stakeholders internally and externally.         Nice to haves (but not required):       Experience with modern data lakehouse technologies like Apache Hudi, Iceberg or Delta Lake.     Experience in contributing to an open source project.   ",60524501808,06-05-2024,04-08-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Product management, Provident fund, GCP, Focus, Cloud, Troubleshooting, Open source, Apache, Analytics, Downstream",-,9am-6pm,"Full Time, Permanent",Onehouse,Organization,Onehouse,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Sr. Engineer(Python)," ?     Tech Lead / Sr. Engineer with Python, Strong SQL, Snowflake, AWS, Lambda, Glue and API development skills.     ?         This is a new team we are tapping and they need someone best in tech, communication, face to stakeholder for meetings and team support / management etc.         This is an urgent position, if the person is class apart, we will be able to onboard him in no time ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Networking, Manager Technology, Technical Lead, Management, Business solutions, AWS, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Backend - AWS Serverless + Python,"   Minimum 3 years of Experience in Python, AWS         Must Include-       - AWS Lambda     - AWS ECS container or Docker(locally) - Working knowledge of Docker and container based deployment is needed. EC2 knowledge is not mandatory.     - AWS S3 for Storage     -    AWS DynamoDB       - API Gateway     - AWS CDK - Typescript     - AWS Cloud formation(or Serverless-framework or Terraform or any other similar tools)      - Experience in creation and configuring of cloud resources using cloud formation/terraform is required.     - AWS SNS/SQS - Knowledge of Message Queue system and related AWS tools.       Must have certification in AWS     Must have worked on Python and must have strong experience     NodeJS knowledge is added advantage   ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Backend, ECS, Cloud, Deployment, AWS, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Lead - Django / Flask Maestro,"     Join our dynamic team as a Python Lead and become the orchestrator of innovative web applications        If you are a Python virtuoso with 5+ years of experience, specializing in Django and Flask, we invite you to take the lead in our ensemble      Embrace the opportunity to shape the future of our applications and mentor a team of passionate developers      Python Maestro: Design, develop, and maintain cutting-edge web applications with expertise in Python, Django, and Flask      Collaborative Composer: Work seamlessly with cross-functional teams to create innovative features that enhance our applications      Code Virtuosity: Craft code that not only functions flawlessly but is also a masterpiece, adhering to the best practices in the Python ecosystem      Testing Conductor: Lead the implementation of comprehensive testing strategies to ensure the performance of our applications is harmonious      Performance Virtuosity: Fine-tune applications for speed and scalability, ensuring a seamless performance without any discordant notes      Mentorship Maven: Share your knowledge and mentor junior developers, fostering a culture of continuous learning and growth      Trendsetter: Stay ahead of the curve by incorporating the latest trends and technologies into our Pythonic repertoire    ",2.20E+11,22-01-2024,21-04-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Maven, Web technologies, Scalability, Django, Mentor, Virtuoso, Python, Testing",-,9am-6pm,"Full Time, Permanent",Kodehash Technologies,Organization,Kodehash Technologies,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Lead customer Success Engineer III,"   We are looking for a Lead Customer Success Engineer to join our team and provide support to our customers     You will be responsible for troubleshooting and resolving cloud-related issues, as well as deploying and optimizing cloud-based solutions     You will also work closely with other IT professionals and teams to ensure the smooth functioning of our cloud infrastructure and services     To be successful in this role, you should have at least 5 years of relevant working experience in cloud operation support and deployment     You should also have proven problem-solving skills and a strong knowledge of cloud platforms and technologies     You should be able to communicate effectively with clients and stakeholders, and provide cloud support and recommendations based on their needs           Responsibilities :       Lead and facility troubleshooting and resolve complex cloud-related issues     Provide guidance to customer on deploy and migrate applications and data to the cloud     Provide best practice for monitor and optimize cloud infrastructure     Ensure data security and compliance in the cloud     Interact with clients and provide cloud support and guidance     Collaborate with teams to design and implement cloud-based solutions     Ensure customers realize the value of Rackspace and Fanatical Support through pro-active architecture reviews and consultancy work     Attend and participate in monthly/quarterly service review meetings     Identify opportunities for growth and pass leads to a Business Development Consultant     Mentor and assist in the development of other technical staff       Requirements:       Bachelor s degree in Computer Science, Engineering, or equivalent working experience     At least 5 years of experience in cloud operation support and deployment     Azure, AWS, and GCP certifications preferred     Proficient in cloud platforms, tools, and services     Experience in cloud architecture, development, and administration     Proven Troubleshooting and analytical skills     Tenacious problem solver, will own issues until full resolution     Excellent communication skills, both written and verbal with great attention to detail     Strong rapport and relationship building skills with both internal departments and external customers     A good level of business awareness and commercial acumen     Understanding of IT industry working practices / methodologies - ITIL foundation certification desirable but not essential     Due to 24x7x365 operation, must be willing to be available for occasional out of hours work     Responsible for adhering to company security policies and procedure as directed   ",90124500349,09-01-2024,08-04-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"operational support, data security, GCP, Relationship building, Cloud, Manager Technology, Troubleshooting, rackspace",-,9am-6pm,"Full Time, Permanent",Rackspace Technology,Organization,Rackspace Technology,https://img.naukimg.com/logo_images/groups/v1/3080116.gif,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Workplace Strategy Intern,"   Support WPS pitches and develop reports needed.      Collaborate on projects with our regional design studios, project consultants as required.      Attend and support in ongoing project meetings and Client presentations as needed.      Work closely with the Workplace Strategy team within the allocated tenure      Coordinate with various studios to gather relevant documents and layouts for the purpose of analytics      Support with manual historic data collection of identified projects to form a part of the company database      Support with transferring gathered data to the company proprietary tool      You will perform any additional responsibilities as may be requested or assigned from time to time                What we expect:          You are a team player with a positive attitude to go the extra mile      You have multi-tasking ability, with time management and organizational skills.      You have knowledge of the design tool - AutoCAD.      You will subscribe to the company core values of Teamwork, Integrity and Excellence.      You are interested in design, strategic approaches to design, research, data collection and analytics.    ",10923501371,01-09-2023,30-11-2023,EducationalOccupationalCredential,1,Engineering - Software & QA,Data Platform Engineer,Design,"Intern, wps",-,9am-6pm,"Full Time, Permanent",Space Matrix Design Consultants,Organization,Space Matrix Design Consultants,https://img.naukimg.com/logo_images/groups/v1/4043268.gif,Pune,Pune,-,-,-,Unpaid P.M ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AI Lead Software Engineer,"Who We Are: The Cimpress Technology Data Platform team is dedicated to helping our worldwide network of companies unlock the value of what they know. Our self-service data platform provides tools to store, manage, and visualize data and a powerful, distributed framework for creating data products, from data transformation pipelines to machine learning models. ? We are seeking a Principal Software Engineer for our Data Science Platform team. Working with a passionate global team, you will be responsible for hands-on solution architecture, technology strategy, technology vision, team leadership and building capabilities that enables Cimpress businesses to turn facts into competitive advantage at scale. ? ? What you will do:   Design and build solutions guided by the ML Lifecycle to enable diagnostic and predictive analytics capabilities   Collaborate with our Data Scientists and Data Engineers to increase, improve and extend model management and prediction capabilities   Work with teams across the business in a cooperative environment to understand what customers need and iterate on solutions adapting to feedback   Be curious about trends and emerging technologies in data science, participate in user communities, and share what you learn with your teammates   Bring new insights to the table; take your idea and turn it into a full-fledged product with the support of the broader Data Platform teams   Contribute to a roadmap making data science products a core capability at Cimpress   Working with AGILE principles ? Your Qualification:   Bachelor's degree or higher in a computer science-related field   Minimum of 8+ years as a ML, Software Engineer or similar role   Experience building software that supports ML applications   Experience building complex ETL data pipelines with scheduling and monitoring (e.g., with DBT, Spark and Airflow, or other tools) for batch processing and real-time data   Strong programming skills in?SQL and Python or R   Experience with big data technologies/platforms such as Snowflake, ETL tools   You have a strong interest in development using cloud technologies. Experience with AWS services including Lambda, Sagemaker, RDS, DynamoDB or CloudFormation or Azure/Google Cloud equivalents ? Nice to have   Entrepreneurial Engineering chops to deliver end to end proofs of concept   Have built REST APIs   Containerization like Docker ? Why You'll Love Working Here: Being at Cimpress means that you don?? see work as just a building, a desk or a manufacturing floor. You see it as a chance to take a step forward in your career journey ??and your life. We strive to give you everything you need to learn, grow, and succeed. Through innovation, collaboration, and perpetual exposure to what?? next, we??e always pushing boundaries and broadening our horizons. We embrace the chance to operate outside of our comfort zone to discover what we??e capable of. Some might call that a challenge; we just call it another great day at work. ? We're Remote-First: In 2020, Cimpress adopted a Remote-First operating model and culture. We heard from our team members that having the freedom, autonomy and trust in each other to work from home and, the ability to operate when they are most productive, empowers them to be their best. Cimpress also provides collaboration spaces for team members to work physically together when it's safe to do so and when in-person collaboration will deliver the best results. Currently we are enabled to hire remote team members in over 30 US States as well as several countries in Europe, including Spain, Germany, UK, Czech Republic, the Netherlands and Switzerland. About Us: Led by founder and CEO Robert Keane, Cimpress invests in and helps build customer-focused, entrepreneurial mass customization businesses. Through the personalized physical (and digital) products these companies create, we empower over 17 million global customers to make an impression. Last year, Cimpress generated $2.88B in revenue through customized print products, signage, apparel, packaging and more. The Cimpress family includes a dynamic, international group of businesses and central teams, all working to solve problems, build businesses, innovate and improve. Equal Opportunity Employer: Cimpress Technology, a Cimpress company, is an Equal Employment Opportunity Employer. All qualified candidates will receive consideration for employment without regard to race, color, sex, national or ethnic origin, nationality, age, religion, citizenship, disability, medical condition, sexual orientation, gender identity, gender presentation, legal or preferred name, marital status, pregnancy, family structure, veteran status or any other basis protected by human rights laws or regulations. This list is not exhaustive and, in fact, in many cases, we strive to do more than the law requires. ? ?",20524003022,02-05-2024,31-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,Internet,"data, customization, cloud technologies, sql, docker, cloud, spark, gcp, oops, data structures, software engineering, etl, programming, ml, snowflake, rest, python, dynamo db, microsoft azure, aws sagemaker, amazon rds, aws cloudformation, r, framework, lambda expressions, mean, agile, aws, db",-,9am-6pm,"Full Time, Permanent",Cimpress,Organization,Cimpress,https://img.naukimg.com/logo_images/groups/v1/1410430.gif,Bengaluru,Bengaluru,-,-,-,4-6 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
ML Ops Engineer,"Key responsibilities: Design and implement cloud solutions, build MLOps on cloud (AWS, Azure, or GCP) Build CI/CD pipelines orchestration by GitLab CI, GitHub Actions, Circle CI, Airflow or similar tools Data science model review, run the code refactoring and optimization, containerization, deployment, versioning, and monitoring of its quality Data science models testing, validation and tests automation Communicate with a team of data scientists, data engineers and architect, document the processes Required Qualifications: Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure or GCP) Experience with MLOps Frameworks like Kubeflow, MLFlow, DataRobot, Airflow etc., experience with Docker and Kubernetes, OpenShift Programming languages like Python, Go, Ruby or Bash, good understanding of Linux, knowledge of frameworks such as scikit-learn, Keras, PyTorch, Tensorflow, etc. Ability to understand tools used by data scientist and experience with software development and test automation Fluent in English, good communication skills and ability to work in a team",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Azure Databricks, Tensorflow, Pytorch, GCP, AWS, Gitlab, Scikit-Learn",-,9am-6pm,"Full Time, Permanent",Ltimindtree,Organization,Ltimindtree,https://img.naukimg.com/logo_images/groups/v1/7519247.gif,"Mumbai, Pune, Bengaluru","Mumbai, Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Engineer," As an Analytics Technology Engineer, you will work on the Technology team that helps deliver our Data Engineering offerings at large scale to our Fortune clients worldwide. The role is responsible for innovating, building and maintaining technology services.       Responsibilities:         Be an integral part of large scale client business development and delivery engagements     Develop the software and systems needed for end-to-end execution on large projects     Work across all phases of SDLC, and use Software Engineering principles to build scaled solutions     Build the knowledge base required to deliver increasingly complex technology projects         Qualifications Experience:     A bachelor s degree in Computer Science or related field with 5 to 10 years of technology experience       Desired Technical Skills:         Data Engineering and Analytics on Google Cloud Platform:      Basic Cloud Computing Concepts     Bigquery, Google Cloud Storage, Cloud SQL,     PubSub, Dataflow, Cloud Composer, GCP Data Transfer,     gcloud CLI         Python, Google Cloud Python SDK, SQL     Experience in working with Any NoSQL/Columnar / MPP Database     Experience in working with Any ETL Tool (Informatica / DataStage / Talend / Pentaho etc.)     Strong Knowledge of database concepts, Data Modeling in RDBMS Vs NoSQL, OLTP Vs OLAP, MPP Architecture         Other Desired Skills:         Excellent communication and co-ordination skills     Problem understanding, articulation and solutioning     Quick learner adaptable with regards to new technologies     Ability to research solve technical issues         Responsibilities:         Developing Data Pipelines (Batch/Streaming)     Developing Complex data transformations     ETL Orchestration     Data Migration     Develop and Maintain Datawarehouse / Data Lakes         Good to have:         Experience in working with Apache Spark / Kafka     Machine Learning concepts     Google Cloud Professional Data Engineer Certification           ",2.91E+11,29-09-2023,28-12-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Cloud computing, Data modeling, RDBMS, Datastage, Informatica, Data warehousing, SDLC, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Neal Analytics,Organization,Neal Analytics,https://img.naukimg.com/logo_images/groups/v1/2694638.gif,"Mumbai, Pune, Chennai, Gurugram, Bengaluru","Mumbai, Pune, Chennai, Gurugram, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Cloud Data Engineer - Data Integration & Warehousing,"We are seeking a talented Cloud Data Engineer to join our team on a remote basis. The ideal candidate will have a strong background in cloud migration, building ETL pipelines, data integrations, and the development of Operational Data Stores (ODS) and Data Warehouses (DW). If you are passionate about leveraging cutting-edge technologies to solve complex data challenges, we want to hear from you. Responsibilities : - Cloud Migration: Lead the migration of on-premises data systems to cloud-based solutions, ensuring scalability, reliability, and efficiency. - ETL Pipeline Development: Design, develop, and maintain robust Extract, Transform, Load (ETL) pipelines to process large volumes of data from various sources into our data ecosystem. - Data Integration: Implement seamless integration between different data sources and platforms, enabling unified access to critical business data. - Operational Data Store (ODS) and Data Warehouse (DW) Development: Architect and build ODS and DW solutions to support analytics, reporting, and decision-making processes. - Big Data Technologies: Utilize advanced technologies such as BigQuery, Kafka, Google Cloud Storage (GCS), and REST APIs to drive data engineering initiatives forward. - Performance Optimization: Continuously optimize data pipelines and storage solutions for improved performance, reliability, and cost-effectiveness. - Collaboration: Work closely with cross-functional teams including data scientists, analysts, and software engineers to understand data requirements and deliver solutions that meet business needs. Requirements : - Bachelor's degree or higher in Computer Science, Engineering, or a related field. - Proven experience in cloud data engineering, with a focus on cloud migration, ETL pipeline development, and data integration. - Strong proficiency in cloud platforms such as Google Cloud Platform (GCP), particularly BigQuery, Google Cloud Storage (GCS), and related services. - Hands-on experience with streaming data technologies like Kafka for real-time data processing and analysis. - Familiarity with RESTful APIs for integrating data from external sources into internal systems. - Solid understanding of data modeling concepts and experience with relational and non-relational databases. - Excellent problem-solving skills and the ability to thrive in a fast-paced, dynamic environment. - Strong communication skills with the ability to collaborate effectively with team members remotely. Preferred Qualifications : - Experience with containerization technologies such as Docker and orchestration tools like Kubernetes. - Certification in Google Cloud Platform or relevant cloud technologies. - Knowledge of machine learning concepts and experience with ML pipeline development is a plus. - Familiarity with agile methodologies and DevOps practices for continuous integration and deployment (CI/CD). - Previous experience in a remote work environment or distributed team setup. Location  : Remote,Delhi NCR,Bangalore,Chennai,Pune,Kolkata,Ahmedabad,Mumbai,Hyderabad",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"GCP Cloud, BigQuery, Data Pipeline, Google Cloud Platform, Cloud, Big Data, Kafka, Data Warehousing, Data Modeling, ETL, Machine Learning, Data Integration",-,9am-6pm,"Full Time, Permanent",Victrix Systems And Labs,Organization,Victrix Systems And Labs,-,"Mumbai, Delhi / NCR, Bengaluru","Mumbai, Delhi / NCR, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Engineer,"   Be an integral part of large-scale client business development and delivery engagements      Develop the software and systems needed for end-to-end execution on large projects      Work across all phases of SDLC, and use Software Engineering principles to build scaled solutions      Build the knowledge base required to deliver increasingly complex technology projects.            Technical skill family (hard skills):        Azure, Azure data bricks, Azure data lake, Azure data factory, Pyspark, sql.            Mandatory technical skills:        Azure data bricks & Pyspark      A bachelor s degree in computer science or related field with 3- 6 years of technology experience      Strong experience in System Integration, Application Development or Data-Warehouse projects, across technologies used in the enterprise space      Software development experience using: Object-oriented languages (e.g. Python, PySpark,) and frameworks      Database programming using any flavours of SQL      Expertise in relational and dimensional modelling, including big data technologies      Exposure across all the SDLC process, including testing and deployment      Expertise in Microsoft Azure is mandatory including components like Azure Data Factory, Azure Data Lake Storage, Azure SQL, Azure DataBricks, HD Insights, ML Service etc.      Good knowledge of Python and Spark are required      Good understanding of how to enable analytics using cloud technology and ML Ops      Experience in Azure Infrastructure and Azure Dev Ops will be a strong plus      Proven track record in keeping existing technical skills and developing new ones, so that you can make strong contributions to deep architecture discussions around systems and applications in the cloud (Azure)      Characteristics of a forward thinker and self-starter      Ability to work with a global team of consulting professionals across multiple projects      Knack for helping an organization to understand application architectures and integration approaches, to architect advanced cloud-based solutions, and to help launch the build-out of those systems      Passion for educating, training, designing, and building end-to-end systems for a diverse and challenging set of customers to success.              EDUCATION:          B.E/B.Tech/M.Tech in Computer Science or related technical degree OR Equivalent    ",1.70E+11,17-01-2024,16-04-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Consulting, System integration, Cloud, Manager Technology, Application development, SQL Azure, SDLC, Analytics, Python",-,9am-6pm,"Full Time, Permanent",Neal Analytics,Organization,Neal Analytics,https://img.naukimg.com/logo_images/groups/v1/2694638.gif,"Mumbai, Pune, Chennai, Gurugram, Bengaluru","Mumbai, Pune, Chennai, Gurugram, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
MLOps Engineer,"  Experience in designing, building, and maintaining ML data infrastructure, data pipelines, and data storage systems.   Proficiency in programming languages such as Python, Java, or Scala.   Experience in working with messaging systems such as Kafka and Confluent Kafka.   Experience in working with cloud platforms such as Azure and GCP.   Experience in working with databases such as Postgres and BigQuery.   Experience in implementing infrastructure as code using tools such as Terraform.   Experience in setting up data within Databricks to help data science teams to build and deploy AI models.      Roles and Responsibilities     Assist in managing the build of AI data infrastructure, including databases, data warehouses, and data lakes.   Manage maintaining our AI data pipelines to ensure that our data is accurately collected, processed, and stored.   Work with our data scientists, AI engineers, and analysts to ensure that our data is accurate, reliable, and accessible for analysis and reporting.    Monitor ML data quality and performance to ensure that our data meets our business needs.   Collaborate with our IT and software development teams to integrate ML data systems and applications.   Deploy and manage ML workloads on cloud platforms such as Azure and GCP   Configure and manage Postgres and Big Query databases.   Implement infrastructure as code using tools such as Terraform.   Enable data science teams with AI infra like Databricks to build and deploy ML models.   Configure and manage networking and graph databases.   Assist AI projects and engage with stakeholders to ensure the success of ML initiatives.  ",3.00E+11,30-04-2024,29-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"hive, scala, amazon redshift, data warehousing, pyspark, networking, data pipeline, emr, sql, cloud, java, postgresql, gcp, spark, linux, mysql, bigquery, hadoop, big data, etl, programming, deployment, python, microsoft azure, engineering, network management, nosql, amazon ec2, kafka, hdfs, terraform, sqoop, aws",-,9am-6pm,"Full Time, Permanent",PureSoftware Pvt Ltd,Organization,PureSoftware Pvt Ltd,https://img.naukimg.com/logo_images/groups/v1/2020226.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Engineer- Data Platform Optimization,"             Act as a data engineering expert and partner to Global Technology and data consumers in controlling the complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate.                          Understand current and future data consumption patterns, and architecture (granular level), and partner with Architects to make sure optimal design of data layers.                          Apply best practices in Data architecture. For example, the balance between materialization and virtualization, the optimal level of de-normalization, caching, and partitioning strategies, choice of storage and querying technology, and performance tuning.                          Led and hands-on execution of research into new technologies such as Delta tables, graph databases, and observability answers. Formulating frameworks for the assessment of new technology vs business benefit, implications for data consumers.                          Act as a best practice expert, and blueprint creator of ways of working such as testing, logging, CI/CD, observability, and release, enabling rapid growth in data inventory and utilization of Data Science Platform.                          Design Business Object, Logical, Physical, and Semantic Models at the enterprise and/or individual line of business and/or product level                          Design prototypes and work in a fast-paced iterative solution delivery model.                                    Required Skills and Abilities:                          Bachelors degree in computer science      , Mathematics, Statistics, Finance, related technical field, or equivalent work experience.                      Experience in extensive work in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc.), application development, and advanced data querying skills.                            Experience in SQL. Experience with Databricks is a plus.                            Solid experience writing, optimizing, and analyzing SQL and data pipelines, with a robust capability to interpret complex data requirements and architect solutions.                          Robust familiarity with Software Development Life Cycle (SDLC) processes and workflow.                                Desired Skills and Abilities:                                  Ability to interpret complex data requirements and architect solutions.                            Distinctive problem-solving and analytical skills combined with robust business acumen.                            Understanding of Microservice Architecture, System Integration, and Agile development methodology.                            Knowledge of Azure cloud computing platform with Azure Synapse, ADLS, and ADF.                            Familiarity with Python or Scala is a plus.                            Familiarity with Reporting software MicroStrategy and Power BI are a plus.                            Passion for data and experience working within a data-driven organization.                            You care about what you do, and what we do            .        ",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data science, Agile development, System integration, SCALA, Application development, Data warehousing, SQL, Data architecture",-,9am-6pm,"Full Time, Permanent",AXA Global Business Services,Organization,AXA Global Business Services,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Systems Engineer - Data,"     To ensure the Group achieves sustainable competitive advantage through data engineering, you will need to play a key role in supporting and executing the Groups data strategy.      We are looking for an experienced Senior Data Systems Engineer to join our Group Security Team, which is part of the wider Cyber Security Engineering practice.      In your role as Data Engineer, you will be required to setup Group Security Data Platform to ingests data (structured and unstructured) from organisations security telemetry data with additional data assets to provide security controls and services that are leveraged across the Group              Roles & Responsibilities :-        You will be expected to perform the following tasks in a manner consistent with CBA s Values and People Capabilities.          CORE RESPONSIBILITIES:      As a Senior Data Systems Engineer,        Contribute to thought leadership enabling business and technology teams to deliver world class data centric solutions and analytics by championing sustainable and re-useable data products.      Participate in the full lifecycle of agile projects (ideation through implementation)      Design and build group data products by integrating diverse data from hundreds of internal and external sources.      Help to promote good coding standard and practices to ensure high quality and minimum risks.      Utilizing Big Data technologies to solve our customers data centric problems.      Working closely with the Solution Designers and Business Stakeholders to perform Data Ingestion, Enrichment and Egression      Designing and building group data products by integrating diverse data from hundreds of internal and external sources      Adopting tools, programming languages and templates to improve our data quality and efficiency.      Building and optimizing Big Data pipelines, architecture, and data sets      Working with and extracting data from large disconnected / unstructured datasets      Required to maintain and build excellent relationships with CDAO peers and stakeholders as well as actively contribute to business outcomes and team culture.            SKILLS:        Stakeholder Management - Contribute to meeting Team financial plan and simplifying how we work. Effective use of practice time (simplifying, optimising, automating, tools standardization, integration, decommissioning, innovation). Productive and effective participation of team members across Crews and Squads (contribution to achieving quarterly plans).      Customers - Contribute to team being a trusted provider of data to the Group to achieve great outcomes for our customers and community.      Risk Mindset -All CommBank employees are expected to proactively identify and understand, openly discuss, and act on current and future risks. Effectiveness with agreed risk reduction actions.      Communication - expert oral and written communication skills with ability to lead discussions with a varied stakeholder across levels.      Problem solving - advanced conceptual, analytical, and critical thinking skills to analyse complex information for key insights and present as meaningful information to senior management.      Undertake any other tasks assigned by your manager that you have the capability to perform safely in line with relevant internal Bank policies and external regulatory requirements.              Essential Skills :-          Minimum 8-11 years of experience with expert level knowledge.      Hands-on experience in Software Engineering or development using Scala or Java to build solution in Hadoop, Hive and Spark technologies.      Demonstrated experience in SQL.      A good high-level understanding of any source control tools like GitHub/ BitBucket etc.      Strong experience in Shell Scripting      Should be well aware of AGILE processes and methodologies.      A natural ability to effectively communicate, educate and influence the different stakeholders/ peers, to solve challenging problems, while working in collaboration with other cross-functional engineering and solution design teams      A positive desire to contribute towards initiatives, along with a strong can- do attitude in juggling between constantly changing priorities.      Hands on Experience with AWS (Amazon Web Services) and or public cloud infrastructure.      Demonstrated experience in delivering data solutions and querying on Redshift or other data warehouses is highly desirable.      Practical knowledge on any programming language like Java/Python      Experience with automation, orchestration working across; Ansible, GitHub, Teamcity, Artifactory and or Octopus Deploy desirable              Qualifications :        Essential:        Proven Experience working with Agile methodologies.      A degree in Engineering / Computer Science or a related discipline        Desirable:        Prior experience on Banking Domain / Financial Services related projects would be an immense value add.      Hands on experience on Teradata or Ab Initio is highly desirable.      Good to have experience in Python.      Exposure to JIRA/ Confluence would be beneficial.      Additional industry or product certification Tertiary qualifications in an IT/computer-related discipline      Experience working in a security domain.      ",30524501456,03-05-2024,01-08-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Financial Services,"Automation, Coding, Shell scripting, Agile, Customer service, Teradata, JIRA, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Commbank,Organization,Commbank,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AI / ML Engineer,"     We are building worlds best copilot for contracts, inside Microsoft Word.      Our current NLP pipeline consists of an ensemble of powerful, proprietary ML models to drive various use cases in contract drafting and review. On the top of that, we have built a robust RAG pipeline to leverage LLMs (both private as well as open source).      We are looking to hire a full time ML engineer [open to consulting contracts too] who can accelerate experimentation, development and deployment of latest developments in Generative AI into our RAG pipeline. Candidates are expected to work (as an IC) with software engineers to build complex, end-to-end AI driven products.      Selected candidate would be working with the founding team and contribute to building a best-in-class Document AI for contracts.            Job Requirements        Read:        Has strong interest and understanding of how LLM space is evolving. Especially Open Source vs Proprietary models (capabilities, cost, sandbox testing, etc.)          Demonstrated experience in Fine Tuning an LLM for business use case will be highly preferred          Demonstrated hands-on experience and understanding in deploying applications using libraries like langchain, llamaindex / gpt-index, etc. for RAG pipeline          Demonstrated experience in ingestion of source data using APIs, SQL, etc. and efficient use of NoSQL, vector databases          Hands-on experience in building and deploying powerful retrieval engines for knowledge-augmented output or multi-message Q&A/Chat, etc.          Strong skills in python and familiarity with cloud platforms like AWS, Azure and containerization tech like Docker, etc.      ",20524501141,02-05-2024,31-07-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Contract drafting, Usage, NoSQL, Consulting, Cloud, microsoft, Open source, AWS, SQL, Python",-,9am-6pm,"Full Time, Permanent",Contractken Inc.,Organization,Contractken Inc.,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Technical Solutions Engineer- Apache Spark,"     As a Technical Solutions Engineer you will help our customers to be successful with the Databricks Data Intelligence platform by resolving important technical customer escalations and the support team      You will be the technical bridge between support and engineering and the first line of defense for engineering      You will ensure that all issues are vetted by you before it reaches the engineering team      You will report to the Senior Backline Manager of the Backline Escalations Team        Outcomes        Troubleshoot, resolve and suggest deep code-level analysis of Apache Spark to address complex customer issues related to Apache Spark core internals, Apache Spark SQL, Structured Streaming and Databricks Delta.      Provide best practices guidance around Apache Spark runtime performance and usage of Apache Spark core libraries and APIs for custom-built solutions developed by Databricks customers.      Help the support team with detailed troubleshooting guides and runbooks.      Contribute to automation and tooling programs to make daily troubleshooting efficient.      Work with the Apache Spark Engineering Team and spread awareness of upcoming features and releases.      Identify Apache Spark bugs and suggest possible workarounds.      Demonstrate ownership and coordinate with engineering and escalation teams to achieve resolution of customer issues and requests      Participate in weekend and weekday on call rotation.      Competencies        Minimum 5 years    experience    developing    ,    testing    , and    sustaining Python    or    Java    or    Scala    -based applications.      Comfortable with compiling, building and navigating the Apache Spark source code.      Comfortable with identifying and applying patches/bug fixes to the Apache Spark source code.      Experience in Big Data/Hadoop/Apache Spark /Kafka/Elasticsearch data pipelines.      Hands-on experience with SQL-based database systems.      Experience in JVM, GC, Thread dump-based troubleshooting is required.      Experience with AWS or Azure related services.      Bachelors degree in Computer Science or a related field is required.      ",10524501376,01-05-2024,30-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, SAN, Automation, Usage, spark, Delta, Diversity and Inclusion, Data analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Databricks,Organization,Databricks,https://img.naukimg.com/logo_images/groups/v1/4128698.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Engineer - Cloud Data Services,"Software Engineer II - Cloud Data Services (Remote)   Experience:  3+ years Salary:  INR 4,05,714 / month Expected Notice Period : 2 to 4 Weeks Shift : 11:00AM to 7:00PM IST Opportunity Type:  Remote Placement    Type:  Permanent   (*Note: This is a requirement for one of Uplers' clients)   What do you need for this opportunity? Primary Skills : AWS Athena, AWS EKS, AWS Glue, AWS Lambda, AWS S3, ETL Orchestration, JVM, Kotlin, React, Snowflake, Spark, Java, SQL Assessment:  Role based AI Screening covering Communication and Technical Skills   Our Hiring Partner is Looking for: Software Engineer II - Cloud Data Services (Remote) who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.   Roles & Responsibilities About TripAdvisor: Who We Are:  We believe that we are better together, and at Tripadvisor we welcome you for who you are. Our workplace is for everyone, as is our people powered platform. At Tripadvisor, we want you to bring your unique perspective and experiences, so we can collectively revolutionize travel and together find the good out there.   What we do in Cloud Data Services:  Tripadvisor is the worlds largest travel website, and we have a LOT of data! With over 1 billion reviews, opinions, photos, and videos, reaching an audience of hundreds of millions worldwide each month. We are a data driven company, and our data infrastructure forms the foundation. Our team is responsible for building the tools that unlock that data across Tripadvisor. We build orchestration tools, scheduling tools, and ETL generation tools to enable cloud migrations and support petabyte scale operations.   What You Will Do:  Across Tripadvisor, weve worked hard to create a great environment for engineers - minimizing process, shipping products quickly, and doing everything to avoid big company paralysis. In this role, within the data platform engineering group, you will help us design, build, and operate our data services infrastructure which is at the core of our data-driven culture. Take responsibility for the quality of the code produced. Take responsibility for all aspects of software engineering, from design to implementation, QA, and maintenance. Operate across our evolving technology stack - Java, Kotlin, React, SQL, Spark, Snowflake and more. Implement new features in a powerful and widely used ETL orchestration tool. Investigate and use Cloud Technologies to deploy our software. Integrate with production Data Sources and Peta-byte scale data lake. Collaborate closely with Product, Data Engineering, Machine Learning, Analytics as well as other functional teams to define feature specifications and develop high-quality deliverables for our customers. Work with technical leadership to make strategic technology decisions. Work alongside other engineering groups located around the world (US, Lisbon, UK, India)   Engagement Type: TripAdvisor is hiring Payroll and Compliance to be managed by: KaamWork Location: Remote Requirements:  Education BS or MS degree in Computer Science or equivalent. Work Experience 3+ years' Experience as a Professional Engineer. Tech Experience: A strong history of development with Java or a JVM based language. Proven record of successful software development. Nice to have:  Knowledge of the modern AWS Data Ecosystem, including AWS Glue, AWS Athena, AWS EKS, AWS S3, and AWS Lambda Experience developing ETL processes and streaming data pipelines; including defining SLAs and performance monitoring. Experience with Kotlin. Experience with Sping. Experience with frontend technologies (React, GraphQL, etc.) Experience developing for large-scale, full life cycle, software applications. Strong interpersonal skills, intense curiosity, and enthusiasm for solving difficult. problems. Familiarity with big data modeling and tools (Spark, Snowflake, Big Query, Presto, etc. How to apply for this opportunity? Register or login on our portal & fill out the application Clear the given AI Screening (30 min) and Click on Apply??to get shortlisted Our team of talent matchmakers will matchmake you with the client Crack a quick interview with our client Land your global dream job and get your exciting career started!   About our Hiring Partner: Tripadvisor, the world's largest travel guidance platform*, helps hundreds of millions of people each month** become better travelers, from planning to booking to taking a trip. Travelers across the globe use the Tripadvisor site and app to discover where to stay, what to do and where to eat based on guidance from those who have been there before. With more than 1 billion reviews and opinions of nearly 8 million businesses, travelers turn to Tripadvisor to find deals on accommodations, book experiences, reserve tables at delicious restaurants and discover great places nearby. As a travel guidance company available in 43 markets and 22 languages, Tripadvisor makes planning easy no matter the trip type. About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. You will also be assigned to a dedicated Talent Success Coach during the engagement.    ( Note:  There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).    So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!  ",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Java, Cloud Technologies, Software Engineering, Cloud Data Migration, Aws Cloud, Aws Lambda, Etl Process, Aws Glue, Spring, AWS, Kotlin",-,9am-6pm,"Full Time, Permanent",Uplers,Organization,Uplers,https://www.naukri.com/hotjobs/images/v3/uplers_jan20.gif,Bengaluru,Bengaluru,-,-,-,30-45 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Engineer II - Tracking & Experimentation,"Software Engineer II - TREX   Experience:  5+ years Salary:  INR 4,05,714 / month Expected Notice Period : 2 to 4 Weeks Shift : 11:00AM to 7:00PM IST Opportunity Type:  Remote Placement    Type:  Permanent   (*Note: This is a requirement for one of Uplers' clients)   What do you need for this opportunity? Primary Skills : Hive, NoSql, Snowflake, Apache Tomcat, BERT, CSS, Gradle, GraphQL, HTML5, Linux, Postgres, React, Spark, AWS, Java, JavaScript, Python, SQL Assessment:  Role based AI Screening covering Communication and Technical Skills   Our Hiring Partner is Looking for: Software Engineer II - TREX who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.   Roles & Responsibilities About TripAdvisor: Who We Are:  We believe that we are better together, and at Tripadvisor we welcome you for who you are. Our workplace is for everyone, as is our people powered platform. At Tripadvisor, we want you to bring your unique perspective and experiences, so we can collectively revolutionize travel and together find the good out there.   What we do in the Tracking & Experimentation Team: At Tripadvisor, we host 400 million monthly active visitors, and assist them to explore the world, and up-level their travel. Tripadvisor also has a culture of ""test and learn"" to encourage finding new ways to better serve travelers.   We are responsible for providing software that enables Tripadvisor to better understand visitor preferences in order to enable tailoring our site, app and product offerings to assist them in having more delightful travel experiences. We are also responsible for the software system that empowers Product Managers, General Managers, and Engineering to experiment and test out new innovative ideas across all Tripadvisor surfaces including the website, mobile app, and customer communications in email. These experiments in turn drive millions of dollars of revenue annually.   What You Will Do:  Tripadvisor is looking for a Software Engineer II to take this exciting opportunity to join our fast-moving tracking and experimentation group. In this role, you will help us build, upgrade, and sustain successful tracking and experimentation infrastructure to serve the worlds largest and most trusted travel site, visited by over 500 million travelers each month, and the worlds leading travel brands, from large OTAs to independent boutique chains.   Do you like building features end to end? Do you like working with a large number of technologies? Do you like moving quickly, releasing features daily, and working with other smart and talented engineers?  If this sounds like you, wed love to talk to you. Take on projects with independence and a mandate to leave things better than you found them. Participate in the planning and initial steps for key changes on the site. Be pragmatic when solving problems with a deep understanding of the purpose and goal of your work. Touch code at all levels, from client ingestion to data storage, data analysis whatever is required to complete your project. Take responsibility for all aspects of software engineering, from design to implementation, QA, and maintenance. Have a CI/CD mindset. Most of our engineer's release code to production every few days and we have a daily release cycle. Be integral for the code quality on your team through leadership in design and code review. Take responsibility for the quality of the code produced by you and the team. Operate across our evolving technology stack - were developing in Java, React, SQL, and more. Collaborate closely with Product and design teams to define feature specifications and develop high quality deliverables for our stakeholders. Work alongside other engineering groups located around the world.     Enagement Type: Payroll and Compliance to be managed by: KaamWork Client / Talent Agreement - Full-time Permanent position Kaamwork is the on-the-ground HR partner managing services like talent sourcing, employment contracts, payroll, and compliance. Kaamwork serves as the Employer of Record while Trip manages all work-related decisions and operations.   Requirements:  Education Bachelor of Science in Computer Science, Engineering or equivalent. Work Experience 5+ years' Experience in commercial software development.   Tech Experience: A strong history of development with Java. Some exposure to the following technologies a plus: HTML5, JavaScript, React, GraphQL, CSS, SQL, Postgres, Linux, Python, Gradle, Apache Tomcat, BERT, Hive, Spark Familiarity with Linux. Familiarity with designing infrastructure on AWS or other cloud providers. Solid foundation in data structures, algorithms, and OO design. Willingness and ability to take on new technologies. Ability to break down complex problems into simple solutions. Strong analytical skills and desire to write clean, correct, and efficient code. Sense of ownership, urgency, and pride in your work. Exposure to developing scalable code for high-volume systems is a plus.     Nice to have:  Having a data mindset along with Software Engineering expertise. Also, have worked on designing infrastructures that deal with the processing of large data sets. Experience working with and processing large quantities of data - Hive, Snowflake, NoSQL databases.   How to apply for this opportunity? Register or login on our portal & fill out the application Clear the given AI Screening (30 min) and Click on Apply to get shortlisted Our team of talent matchmakers will matchmake you with the client Crack a quick interview with our client Land your global dream job and get your exciting career started!   About our Hiring Partner: Tripadvisor, the world's largest travel guidance platform*, helps hundreds of millions of people each month** become better travelers, from planning to booking to taking a trip. Travelers across the globe use the Tripadvisor site and app to discover where to stay, what to do and where to eat based on guidance from those who have been there before. With more than 1 billion reviews and opinions of nearly 8 million businesses, travelers turn to Tripadvisor to find deals on accommodations, book experiences, reserve tables at delicious restaurants and discover great places nearby. As a travel guidance company available in 43 markets and 22 languages, Tripadvisor makes planning easy no matter the trip type. About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. You will also be assigned to a dedicated Talent Success Coach during the engagement.    ( Note:  There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).    So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!  ",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Java, Software Engineering, Hive, Linux, Snowflake, Javascript, Bert, Gradle, AWS, Python, SQL",-,9am-6pm,"Full Time, Permanent",Uplers,Organization,Uplers,https://www.naukri.com/hotjobs/images/v3/uplers_jan20.gif,Bengaluru,Bengaluru,-,-,-,40-50 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer,"Your role will focus on leading the development efforts of the Kafka-as-a-Service platform on top of Confluent Cloud.  You will help drive technical decision-making, particularly regarding operations and the platform's architectural direction. Youll help solve problems related to building a global Kafka architecture considering scalability, replication, schema registries and self-service IaC workflows to solve for different use cases such as high traffic telemetry (logs & metrics), business critical events and data processing. Youll emphasize modern, rigorous software development practices that emphasize testability, repeatability, and self-service automation.  Youll openly collaborate with other teams leads and help raise the bar of engineering excellence across the entire organization. What to Bring: Bachelors degree with 5 - 8 years of experience as a software developer You  bring passion and enthusiasm to your engineering team You have a track record of shipping quality code to production on a frequent and consistent basis You thrive under minimal supervision with the ability to self-motivate and self-organize within the organizational structure You have experience running production infrastructure that supports multiple systems in a scalable, stable, and performant manner You measure everything, make decisions based on data, are consumer-obsessed, and take immense pride in your work The ideal candidate for this role will have a wide breadth of experience across the entire software stack, as well as  deep subject matter expertise in at least one technology from each of the following groups: Operating  Kafka  at scale (we use Confluent Cloud) Cloud Infrastructure Automation (AWS strongly preferred with Terraform) Database design (e.g. Postgres, NoSQL, etc) Data Processing (e.g. Hadoop, Big Table, Spark, Redshift) Software Development: Distributed Systems Development (e.g. asynchronous communication patterns, consensus algorithms, distributed transactions) Programming (e.g. Go-lang, Java, Kotlin, Scala, Python, Ruby) Event Driven Architecture & Streaming Frameworks (e.g. Kafka Streams) Your Technical Know-How In addition, your technical expertise should match well to the following: Experience leading the development of large-scale projects, e.g. breaking down tasks, delegating work, assisting in the creation of roadmaps and work back plans Deep understanding of distributed systems in Kubernetes Hands-on experience with multiple IaC tools Experience with some of the frameworks and tools our development teams use day to day (i.e Spring Boot, Kafka Streams) Experience with the development and operation of high throughput, low-latency systems Hands-on experience with automating development workflow pipelines (CI/CD) Operational experience (i.e. on-call rotation, incident response) Ability to collaborate effectively with remote peers across disparate geographies and time zones Excellent written and verbal communication skills with particular emphasis on technical documentation (including diagramming) Strong CS fundamentals",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,TV / Radio,"Software Engineer, Java, Go-lang, Scala, Spring Boot, asynchronous communication patterns, on-call rotation, incident response, Kotlin, CS, CI/CD, consensus algorithms, distributed transactions, Kafka Streams, Ruby, Python",-,9am-6pm,"Full Time, Permanent",Discovery Communications,Organization,Discovery Communications,https://www.naukri.com/hotjobs/images/v3/WARNER_Oct22.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer - DevSecOps,"       DevOps Integration: Collaborate with software development and IT operations teams to integrate security into the DevOps pipeline, automating security controls, and promoting a culture of security awareness.      Security Automation: Develop and maintain automation scripts and tools for security testing, scanning, and monitoring of applications, infrastructure, and code repositories.      Continuous Monitoring: Implement continuous monitoring solutions to detect and respond to security threats and vulnerabilities in real-time.      Security Testing: Conduct regular security testing, including static analysis, dynamic analysis, and penetration testing, to identify and remediate vulnerabilities.      Incident Response: Assist in developing and maintaining incident response plans, and actively participate in security incident response activities when necessary.      Compliance: Ensure compliance with industry standards and regulations (e.g., GDPR, HIPAA, ISO 27001) by implementing necessary security controls      Security Education: Provide training and guidance to development and operations teams on security best practices and tools.      Toolchain Management: Evaluate, select, and maintain security tools and technologies that enhance the security posture of the organization.          Requirements    :        6+ years experience in software development or DevSecOps role      Bachelor s degree in computer science, Information Security, or related field (or equivalent work experience).      Proficiency in scripting and programming languages (e.g., Python, Ruby, Bash).      Familiarity with containerization and orchestration technologies (e.g., Docker, Kubernetes).      Proven experience in DevSecOps, including experience in automating security practices within a DevOps environment.      Proficiency in DevOps practices, toolchain and processes - source code, build pipeline, deployment, observability, AWS      Strong knowledge of security tools and technologies, such as vulnerability scanners, WAFs, SIEM, and IDS/IPS.      Experience with cloud security best practices, preferably AWS, Azure, or GCP.      Understanding of security exposure during product development and deployment - scanning, prioritizing and validating fixes.      Knowledge of relevant tools - Snyk, Crowdstrike, SonarQube or similar      Ability to communicate and collaborate with development teams, Devops team and security teams      Experience with Incident Response, Security Audits, Monitoring and Analysis          Nice to Have:          Relevant security certifications (e.g., CISSP, Certified Ethical Hacker, CompTIA Security+) are a plus.      ",80324501840,08-03-2024,06-06-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Software Product,"Computer science, Automation, Information security, Consulting, ISO 27001, Ruby, IPS, Analytics, Monitoring, Python",-,9am-6pm,"Full Time, Permanent",Poppulo,Organization,Poppulo,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Engineer Software Engineering - AI/ML,"     The AI/ML Solutions team in GLOBALFOUNDRIES has a primary mission of delivering data engineering, advanced analytics, and machine learning systems and applications to improve business operations in any of our business units     We are involved in the entire application lifecycle from concept to deployment with a primary focus of enabling the distributed data science teams to develop and deploy their workflows     We also lead or help with building prototype solutions for new project areas     The successful candidate will help architect and build data flows for advanced analytics and machine learning applications, as well as build custom tools to facilitate the speedy development and release of the applications                 Essential Responsibilities:         Build tools to automate and improve development and release processes, and deploy Machine Learning (ML) to large production environments     Utilize Versioning Repository tools, Continuous Integration Frameworks, Application Containerization, Automation Deployment, Code Quality and Code Security Scanning tools     Design, build, and maintain efficient, reusable, and tested code in Python and other applicable languages and library tools, utilize off-the-shelf solutions where warranted     Design modern data pipeline architectures and build tooling to efficiently tackle Big Data projects in a multi-cloud environment     Able to handle multiple projects     Present project status to peers and the leadership team as needed, collaborate across organizational boundaries         Required Qualifications:         B.S. in Computer Science, Software Engineering, or equivalent field with 4-6 years industrial experience, or M.S. with 3-5 years experience, or Ph.D. with 1-2 years experience.     Experience with full stack development     Understanding of REST APIs, JSON data format     Experience with ML services in AWS ecosystem     Experience with the deployment of big data ETL pipelines in the cloud, eg PySpark     Experience with of SQL and NoSQL databases, Python, Docker containers     Exposure to and/or understanding of ML tools/libraries such as: TensorFlow/Keras, PyTorch, Pandas     Understanding of advanced data analytics and machine learning     Prior experience architecting end-to-end pipeline for ML and Analytics using AWS or Azure services     Understanding of MLOps tools and flow             Preferred Qualifications:         Experience deploying advanced data analytics and machine learning applications     Experience in managing projects     Understanding of AWS Well Architected Framework (6 pillars)     Experience mentoring junior engineers / interns     AWS certifications (eg Cloud Practitioner, Machine Learning, Associate Solution Architect)   ",90124500690,09-01-2024,08-04-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Electronic Components / Semiconductors,"Automation, Semiconductor, Prototype, Software Engineering Manager, Machine learning, Software Engineer, JSON, SQL, Python",-,9am-6pm,"Full Time, Permanent",Globalfoundries,Organization,Globalfoundries,https://img.naukimg.com/logo_images/groups/v1/4663071.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Software Engineer, Back End","   You want to be part of the founding engineering team that takes humit to the moon (and beyond).      You can easily pick up new technologies and frameworks on the go. For you, engineering is about principles and practices; technology is about tools and tradeoffs.      You care about the implications of your features and development as much as the code itself.      You're passionate about music and relish problems at the intersection of music and technology.        If your answer to these questions is a resounding yes, we already like you!        Minimum qualifications:          Bachelor's degree in Computer Science or equivalent practical experience.      3 years of relevant work experience.      Experience with software development with one or more general programming languages (e.g. Python, Java, or Go).      Demonstrated ability to share knowledge via formal mentoring, reviewing code, reviewing design documents, providing technical talks, teaching classes, or as a consultant on projects.      What you'll be doing      You will help scale humit's backend for the next million users and take us from 0 to 1.      You will work with a passionate cross-functional team of Designers, Frontend Engineers, and Product Managers to push out new product features, focusing on the backend. Think - building an API for public comments on hums.      You'll help build improvements to the performance and reliability of our backend and infrastructure. Think creating a caching layer to the backend using Redis or creating an async task queue to offload work.      Hacking your way around Spotify, Apple Music, YouTube APIs to create a seamless playback experience for our users.        As a Back End Engineer, youll be a key member of the product team, and will work with the Founder / CTO and Engineering Lead.      Some of the technology stack you will be working with:        Python 3.7      Django      Celery      Docker      AWS Beanstalk (EC2)      AWS RDS Postgres (persistent storage)      AWS DocumentDB (mongo)      AWS S3      AWS EKS (K8s)      AWS CloudWatch (logging)      AWS ElastiCache (cache and message bus)    ",40823501442,04-08-2023,02-11-2023,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Events / Live Entertainment,"Backend, Front end, Django, Manager Technology, Programming, AWS, Engineering Lead, Python",-,9am-6pm,"Full Time, Permanent",Humit,Organization,Humit,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Engineer,"       Be an integral part of large-scale client business development and delivery engagements by    understanding the business requirements.        Hands-on with    Dataflow/Apache beam and    Realtime data streaming        Engineer ingestion and processing pipelines on GCP using python libraries, Java, BigQuery and composer.          Automate the repeatable tasks into a framework that can be reused in other parts of the projects.          Handle the data quality, governance, and reconciliation during the development phases.        Being able to communicate with internal/external customers, desire to develop communication and client-facing skills.      Understand and contribute in all the agile ceremonies to ensure the efficiency in delivery.          Qualification & Experience:          A bachelor s degree in Computer Science or related field.      Minimum 5 years of experience in software development.      Minimum 3 years of technology experience in Data Engineering projects        Minimum 3 years of experience in GCP.          Minimum 3 years of experience in python programming.          Minimum 3 years of experience in SQL/PL SQL Scripting.          Minimum 3 years of experience in Data Warehouse / ETL      .          Ability to build streaming/batching solutions.          Exposure to project management tools like JIRA, Confluence and GIT      .          Ability to define, create, test, and execute operations procedures.            Must have skills:            Strong understanding of real time streaming concepts          Strong problem solving and analytical skills.          Good communication skills.          Understanding of message queues like Kafka, Rabbit MQ, PubSub          Understanding of fast data caching systems like Redis/Memory Store          GCP experience - ~3+ years          Dataflow/Apache beam hands of experience - Custom templates          Understanding of Composer          Good experience with Big Query and PubSub          Good hands-on experience with Python          Hands on experience with modular java code development involving design patterns - Factory, Reflection, etc.            Good to have skills:            GCP Professional Data Engineer certification is an added advantage.          Understanding of Terraform script.          Understanding of Devops Pipeline          Identity and Access Management, Authentication protocols        Google drive APIs, One drive APIs    ",30823500822,03-08-2023,01-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Access management, GCP, Project management, Reconciliation, Agile, PLSQL, Data quality, Apache, Python",-,9am-6pm,"Full Time, Permanent",Neal Analytics,Organization,Neal Analytics,https://img.naukimg.com/logo_images/groups/v1/2694638.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Machine Learning Engineer,"Experience with SQL and NoSQL, MLflow, GitHub, docker, Kubernetes, ELK, or similar stack. And should be comfortable with libraries like Pandas, NumPy, OpenCV, PIL, Spacy, transformers etc. Experience with cloud computing platforms like GCP and AWS",20524007906,02-05-2024,31-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Miscellaneous,"Deep Learning Frameworks, Image Processing, Machine Learning",-,9am-6pm,"Full Time, Permanent",Neutrinos Solutions,Organization,Neutrinos Solutions,-,Bangalore Rural,Bangalore Rural,-,-,-,10-20 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer - DevSecOps,"     The Reltio TechOps team is committed to delivering best in class DevSecOps capabilities and developer experience     You will be responsible for helping the company scale its cloud-based services through refining our internal platform, heavy focus on automation, and continuous delivery     Help TechOps to set time bound, clear and concise maturity and strategic goals and execute against the strategy to improve developer experience and level up DevSecOps maturity     Platform Engineer/SME in infrastructure Security management and services for Reltio cloud services - MDM, RIQ, RDM, and other services or components that create the foundation of the core Reltio Product line     Platform Engineering is the first line of investigation and troubleshooting of the service reliability issues pertaining to DevSecOps             Dynamic pace and opportunity to work with the hottest technology K8S CI/CD and infrastructure-as-code tools in AWS, GCP, Azure.     Be a key participant in implementing tools and strategies used to revolutionize Reltio s internal infrastructure platform.     Exposure to challenging problems in blending MDM and Big Data worlds with a focus on cost efficiency, reliability, and availability.     Have the opportunity to bring new automated approaches to scalability, capacity management, cost optimization, and elasticity.         Job Duties and Responsibilities:         Implement and manage infrastructure as code (IaC) solutions using tools like Terraform and Helm Charts to provision and manage cloud resources.     Collaborate with development, operations, and quality assurance teams to streamline the software delivery process and ensure high-quality releases.     Monitor, troubleshoot, and optimize the performance and cost of our DevOps tools and platforms to ensure reliability, availability, and scalability.     Execute zero downtime maintenance strategies for fleets of kubernetes clusters and cloud resources.     Act as a force multiplier to engineers and development teams by providing guidance on integrating with the internal platform, best practices, tools, and methodologies.     Collaborate with security teams to integrate security controls and best practices into our DevOps processes and infrastructure.     Document technical designs, procedures, and configurations to ensure knowledge sharing and maintain system integrity.     Contribute to a culture of innovation, collaboration, and continuous improvement within the TechOps team and across the organization.         Skills You Must Have:         Bachelors degree in Computer Science, Engineering, or related field; or equivalent work experience.     3+ years of experience working as a Cloud Engineer, or similar role, with a strong background in software development and infrastructure operations.     Experience with containerization technologies (Docker, Kubernetes) and orchestration tools.     Experience with declarative management tools such as Terraform or Cloud Formation.     Experience with cloud platforms such as AWS, Azure, or Google Cloud Platform.     Experience in scripting and programming languages such as Python or Bash.     Hands-on experience with CI/CD tools such as Jenkins or ArgoCD.     Strong troubleshooting and problem-solving skills, with the ability to quickly diagnose and resolve technical issues.     Excellent communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.     Proven track record of driving process improvements and implementing best practices in DevOps methodologies.         Skills That Are Nice to Have:         Experience with monitoring and logging tools such as Prometheus, Grafana, or OpenTelemetry.     Knowledge of security best practices and compliance requirements in cloud environments.     Experience with agile development methodologies and DevOps practices in a fast-paced, dynamic environment   ",2.90E+11,29-02-2024,29-05-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Software Product,"Capacity management, Automation, Manager Quality Assurance, orchestration, Security management, Continuous improvement, Analytics, infrastructure security, Python",-,9am-6pm,"Full Time, Permanent",Reltio,Organization,Reltio,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"ML Engineer ( AWS, Sagemaker )","1.Understanding of Natural Language Processing (NLP), Natural Language 2.Understanding (NLU) and Natural Language Generation (NLG) 3.Develop and implement technical efforts to design, build, and deploy AWS ML models at the direction of lead architects, including large-scale data processing, computationally intensive statistical modelling, and advanced analytics 4.Familiarity with deep learning, machine learning and NLP/NLG frameworks (like Keras, TensorFlow or PyTorch etc.), HuggingFace Transformers and libraries (like scikit-learn, spacy, gensim, CoreNLP etc.) 5.Should have experience in AWS services such as SageMaker, Elasticsearch, and general knowledge of AWS architecture & other services  6.Ability to write robust code in Python, Java and R 7.Experience optimizing model hyperparameter tuning for speed and cost 8.Experience in building MLOps pipeline using MLOps frameworks like Kubeflow, MLFlow, and DataRobot 9.Experience with Docker and Kubernetes 10. Familiarity with Eventbridge and AWS step function for workload orchestration 11.Experience with Agile Development.",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"AWS, Kubeflow, Java, Sagemaker, MLOps, Docker, NLU, MLFlow, Agile Development, Kubernetes, DataRobot, Python",-,9am-6pm,"Full Time, Permanent",Winning Edge,Organization,Winning Edge,-,"Bengaluru, Karnataka","Bengaluru, Karnataka",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Engineer - GCP certified added advantage,"       3+ years of experience in designing and building IaaS, PaaS and Serverless cloud solutions on Google Cloud         3+ years of experience as a Cloud Engineer with GCP certification         3+ years of experience in GCP Troubleshooting and analytical skills         3+ years of experience with infrastructure as code development using Terraform         3+ year of experience in provisioning of GCP Resources like Cloud Run,Cloud SQL,Secret Manager,Cloud Storage,Pub Sub etc through Terraform         Exposure to continuous delivery toolsets like Tekton and Jenkins tools         Collaborating with development teams,Troubleshoot GCP Deployment issues, identify root causes, fix and document problems, and implement preventive measures         Help resolving Connectivity and Access issues between the Application and other resources in GCP         Strong Knowledge on GCP Service Account management and GCP IAM Services         Other Details          Work shift:12-9 PM IST | 2-10 PM IST     Role: Cloud System Administration     Industry Type: IT Services & Consulting     Department: IT & Information Security                ",2.60E+11,26-03-2024,24-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"IT services, GCP, Information security, Consulting, Cloud, cloud storage, Account management, Troubleshooting, SQL, System administration",-,9am-6pm,"Full Time, Permanent",Pradeepit Consulting Services,Organization,Pradeepit Consulting Services,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
SAP BTP,"         Implement workable code on the SAP Business Technology Platform for user stories that have been groomed:           Active involvement in meetings for the system requirements and technical specifications     Analyze the client systems to understand program needs, based on user stories that have been groomed and list deliverables based on this       Development of program code for the development of the SAP Neo and Cloud foundry applications using Java, Node.js, CAPM/CDS         Development and configuration of the SAP Identity and Access Services (IAS)       Development on the Adobe AJO / Experience platform, AWS (SNS/SQS, Lambda, Terraform, and security)     Development on the SAP Hana database     Experience with Docker / Docker registry     Estimate and commit to sprint deliverables     Stick to the Signify development process     Code quality according to the Signify standards     First time right code delivery above 95% during peer reviews     Pro-active guidance towards standard or configurable solutions     Demo of provided solution to product owner more than 95% first time right     Document the solution in Confluence, reviewed and approved     Report impediments when they occur, not after the facts     Report risks when identified, not when they have become an impediment     Step in when a P1 event is raised     Active involvement in meetings for the system requirements and technical specifications     Proficiency in the implementation of SAP projects in On-Premise and On-Cloud                     ",2.60E+11,26-03-2024,24-06-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SAP, CAPM, Cloud, Manager Technology, Silicon, Open source, Adobe, sap hana, cloud foundry, Salesforce",-,9am-6pm,"Full Time, Permanent",Pradeepit Consulting Services,Organization,Pradeepit Consulting Services,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Windows Platform Engineer,GCP Windows Platform Engineer,2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"GCP, Jenkins, python, java, Linux, Microsoft azure, aws, docker",-,9am-6pm,"Full Time, Permanent",Ascendion Engineering Private Limited,Organization,Ascendion Engineering Private Limited,https://www.naukri.com/hotjobs/images/v3/ASE_oct22.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
GCP Big Query Engineer,"     We are looking for an analytical, big-picture thinker who is driven to enhance and further the mission of Tredence by delivering technology to internal business and functional stakeholders.      You will serve as a leader to drive the IT strategy to create value across the organization.      This Data Engineer will be empowered to lead the engagement to focus on implementing both low-level, innovative solutions, as well as the day-to-day tactics that drive efficiency, effectiveness, and value     You will play a critical role in creating and analyzing deliverables to provide critical content to enable fact-based decision-making, facilitation, and achievement of successful collaboration with the business stakeholders.      You will analyze, design, and develop best practices for business changes through technology solutions.         Technical Requirements         Have Implemented and Architected solutions on the Google Cloud Platform using the components of GCP     Experience with Apache Beam/Google Dataflow/Apache Spark in creating end-to-end data pipelines.     Experience in some of the following: Python, Hadoop, Spark, SQL, Big Query, Big Table Cloud Storage, Datastore, Spanner, Cloud SQL, and Machine Learning.     Experience programming in Java, Python, etc.     Expertise in at least two of these technologies: Relational Databases, Analytical Databases, and NoSQL databases.     Certified in Google Professional Data Engineer/ Solution Architect is a major Advantage         Roles Responsibilities         Experience         6-8 years experience in IT or professional services experience in IT delivery or large-scale IT analytics projects     Candidates must have expertise and knowledge of the Google Cloud Platform; the other cloud platforms are nice to have.     Expert knowledge in SQL development.     Expertise in building data integration and preparation tools using cloud technologies (like Snaplogic, Google Dataflow, Cloud Dataprep, Python, etc).     Experience with Apache Beam/Google Dataflow/Apache Spark in creating end-to-end data pipelines.     Experience in some of the following: Python, Hadoop, Spark, SQL, Big Query, Big Table Cloud Storage, Datastore, Spanner, Cloud SQL, and Machine Learning.     Experience programming in Java, Python, etc.     Identify downstream implications of data loads/migration (e.g., data quality, regulatory, etc.)     Implement data pipelines to automate the ingestion, transformation, and augmentation of data sources, and provide best practices for pipeline operations.     ",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"NoSQL, IT strategy, GCP, Analytical, Machine learning, Data quality, Analytics, Downstream, SQL, Python",-,9am-6pm,"Full Time, Permanent",Pradeepit Consulting Services,Organization,Pradeepit Consulting Services,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Cloud Data Services Engineer,"Senior Software Engineer - Cloud Data Services (Remote)   Experience:  5+ years Salary:  INR 5,62,000 / month Expected Notice Period : 2 to 4 Weeks Shift : 11:00AM to 7:00PM IST Opportunity Type:  Remote Placement    Type:  Permanent   (*Note: This is a requirement for one of Uplers' clients)   What do you need for this opportunity? Primary Skills : AWS Data Ecosystem, Big Query, GraphQL, Hive, Presto, AWS Athena, AWS Glue, AWS Kinesis, AWS Lambda, AWS S3, ETL Orchestration, JVM, Kotlin, React, Snowflake, Spark, Java, SQL Assessment:  Role based AI Screening covering Communication and Technical Skills   Our Hiring Partner is Looking for: Senior Software Engineer - Cloud Data Services (Remote) who is passionate about their work, eager to learn and grow, and who is committed to delivering exceptional results. If you are a team player, with a positive attitude and a desire to make a difference, then we want to hear from you.   Roles & Responsibilities About TripAdvisor: Who We Are:  We believe that we are better together, and at Tripadvisor we welcome you for who you are. Our workplace is for everyone, as is our people powered platform. At Tripadvisor, we want you to bring your unique perspective and experiences, so we can collectively revolutionize travel and together find the good out there.   What we do in Cloud Data Services:  Tripadvisor is the worlds largest travel website, and we have a LOT of data! With over 1 billion reviews, opinions, photos, and videos, reaching an audience of hundreds of millions worldwide each month. We are a data driven company, and our data infrastructure forms the foundation. Our team is responsible for building the tools that unlock that data across Tripadvisor. We build orchestration tools, scheduling tools, and ETL generation tools to enable cloud migrations and support petabyte scale operations.   What You Will Do:  Across Tripadvisor, weve worked hard to create a great environment for engineers - minimizing process, shipping products quickly, and doing everything to avoid big company paralysis. In this role, within the data platform engineering group, you will help us design, build, and operate our data services infrastructure which is at the core of our data-driven culture. Take responsibility for the quality of the code produced. Take responsibility for all aspects of software engineering, from design to implementation, QA, and maintenance. Operate across our evolving technology stack - Java, Kotlin, React, SQL, Spark, Snowflake and more. Implement new features in a powerful and widely used ETL orchestration tool. Investigate and use Cloud Technologies to deploy our software. Integrate with production Data Sources and Peta-byte scale data lake. Collaborate closely with Product, Data Engineering, Machine Learning, Analytics as well as other functional teams to define feature specifications and develop high-quality deliverables for our customers. Work with technical leadership to make strategic technology decisions. Work alongside other engineering groups located around the world (US, Lisbon, UK, India)   Engagement Type: TripAdvisor is hiring Payroll and Compliance to be managed by: KaamWork Location: Remote Requirements:  Education BS or MS degree in Computer Science or equivalent. Work Experience 5+ years' Experience as a Professional Engineer. Tech Experience: A strong history of development with Java or a JVM-based Language. Proven record of innovation via non-trivial solutions to day-to-day problems Experience developing for large-scale, full life cycle, software applications. Experience developing complete JVM-based (or equivalent) applications, preferably for large distributed systems.   Nice to have:  Hands-on knowledge of the modern AWS Data Ecosystem, including AWS Glue, AWS Athena, AWS Kinesis, AWS S3, and AWS Lambda. Experience developing ETL processes and streaming data pipelines; including defining SLAs and performance monitoring??Experience with frontend technologies (React, GraphQL, etc.) Strong interpersonal skills, intense curiosity, and enthusiasm for solving difficult problems. Familiarity with big data modeling and tools (Spark, Hive, Snowflake, Big Query, Presto, etc.   How to apply for this opportunity? Register or login on our portal & fill out the application Clear the given AI Screening (30 min) and Click on Apply to get shortlisted Our team of talent matchmakers will matchmake you with the client Crack a quick interview with our client Land your global dream job and get your exciting career started!   About our Hiring Partner: Tripadvisor, the world's largest travel guidance platform*, helps hundreds of millions of people each month** become better travelers, from planning to booking to taking a trip. Travelers across the globe use the Tripadvisor site and app to discover where to stay, what to do and where to eat based on guidance from those who have been there before. With more than 1 billion reviews and opinions of nearly 8 million businesses, travelers turn to Tripadvisor to find deals on accommodations, book experiences, reserve tables at delicious restaurants and discover great places nearby. As a travel guidance company available in 43 markets and 22 languages, Tripadvisor makes planning easy no matter the trip type. About Uplers: Our goal is to make hiring reliable, simple, and fast. Our role will be to help all our talents find and apply for relevant opportunities and progress in their career. We will support any grievances or challenges you may face during the engagement. You will also be assigned to a dedicated Talent Success Coach during the engagement.    ( Note:  There are many more opportunities apart from this on the portal. Depending on the assessments you clear, you can apply for them as well).    So, if you are ready for a new challenge, a great work environment, and an opportunity to take your career to the next level, don't hesitate to apply today. We are waiting for you!  ",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Cloud Technologies, Software Application Development, Java, Aws Cloud, Aws Lambda, Cloud Data Flow, Etl Process, Big Data Technologies, Aws Glue, Aws Stack",-,9am-6pm,"Full Time, Permanent",Uplers,Organization,Uplers,https://www.naukri.com/hotjobs/images/v3/uplers_jan20.gif,Bengaluru,Bengaluru,-,-,-,45-65 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"ML Engineer (AWS, Sagemaker)","1.Understanding of Natural Language Processing (NLP), Natural Language 2.Understanding (NLU) and Natural Language Generation (NLG) 3.Develop and implement technical efforts to design, build, and deploy AWS ML models at the direction of lead architects, including large-scale data processing, computationally intensive statistical modelling, and advanced analytics 4.Familiarity with deep learning, machine learning and NLP/NLG frameworks (like Keras, TensorFlow or PyTorch etc.), HuggingFace Transformers and libraries (like scikit-learn, spacy, gensim, CoreNLP etc.) 5.Should have experience in AWS services such as SageMaker, Elasticsearch, and general knowledge of AWS architecture & other services  6.Ability to write robust code in Python, Java and R 7.Experience optimizing model hyperparameter tuning for speed and cost 8.Experience in building MLOps pipeline using MLOps frameworks like Kubeflow, MLFlow, and DataRobot 9.Experience with Docker and Kubernetes 10. Familiarity with Eventbridge and AWS step function for workload orchestration 11.Experience with Agile Development.",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"NLP, AWS, R, Sagemaker, MLOps, PyTorch, Docker, NLU, NLG, Python, Kubernetes, TensorFlow",-,9am-6pm,"Full Time, Permanent",Winning Edge,Organization,Winning Edge,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Intern - Cloud Engineer / Cloud Consultant,"Experience: Freshers Work Timings: Rotational Shift Internship Scope: Intern to work on Cloud Computing: To learn & develop, implement, optimize, and maintain cloud-based solutions in collaboration with our team. Automation on Cloud Upskill on Cloud, DevOps, Big Data, AI and ML technologies Researching and designing Cloud POCs Working on deploying and debugging cloud solutions, Understanding & work on the concept of the security of the cloud infrastructure. Pre-requisites: Understanding of cloud technologies such as AWS, Azure, GCP, etc. Good networking Skills Basic Programming and scripting Knowledge like Python, Power shell and Bash. Understanding of Software Development Lifecycle and Development Practices. Degree in computer science or a similar field. Azure certifications preferred. Troubleshooting and analytical skills. Good communication and collaboration skills. Flexible to work in different shifts. Stipend: Month 1 No Stipend Month 2 and 3 - INR 10,000/- per month, purely subject to your performance and contribution. This decision of stipend eligibility lies with the concerned Department Head Month 4 Full time offer or termination of Internship (based on performance and evaluation during the Internship period)",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,1,Engineering - Software & QA,Data Platform Engineer,Software Product,"Cloud Computing, Cloud Consultant, scripting, DevOps, Azure, Cloud Engineer, GCP, Big Data, debugging, Troubleshooting, AWS",-,9am-6pm,"Full Time, Permanent",Spektra,Organization,Spektra,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"ML Engineer (AWS, Sagemaker)","1.Understanding of Natural Language Processing (NLP), Natural Language 2.Understanding (NLU) and Natural Language Generation (NLG) 3.Develop and implement technical efforts to design, build, and deploy AWS ML models at the direction of lead architects, including large-scale data processing, computationally intensive statistical modelling, and advanced analytics 4.Familiarity with deep learning, machine learning and NLP/NLG frameworks (like Keras, TensorFlow or PyTorch etc.), HuggingFace Transformers and libraries (like scikit-learn, spacy, gensim, CoreNLP etc.) 5.Should have experience in AWS services such as SageMaker, Elasticsearch, and general knowledge of AWS architecture & other services  6.Ability to write robust code in Python, Java and R 7.Experience optimizing model hyperparameter tuning for speed and cost 8.Experience in building MLOps pipeline using MLOps frameworks like Kubeflow, MLFlow, and DataRobot 9.Experience with Docker and Kubernetes 10. Familiarity with Eventbridge and AWS step function for workload orchestration 11.Experience with Agile Development.",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"NLP, AWS, R, Sagemaker, MLOps, NLU, NLG, Python",-,9am-6pm,"Full Time, Permanent",Winning Edge,Organization,Winning Edge,-,"Bengaluru, Karnataka","Bengaluru, Karnataka",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Fivetran Platform Engineer," Role & responsibilities     1. Experience in installation, deployment, configuration and customization of Fivetran/HVR and its connectors; having experience SAP as data source is a plus 2. Experience in optimization of pipelines for efficiency, scalability and performance tuning 3. Experience with cloud platforms such as Google Cloud Platform, Azure or AWS 4. Experience in Fivetran troubleshooting 5. Strong communication and collaboration skills, and working in a global environment Position is on Contract Preferred candidate profile  Skills:  Fivetran setup, HVR [now Fivetran LDP], GCP Perks and benefits   Remote",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"hvr, GCP, fivetran",-,9am-6pm,"Full Time, Permanent",Iitjobs Inc.,Organization,Iitjobs Inc.,https://img.naukimg.com/logo_images/groups/v1/3303272.gif,Bengaluru,Bengaluru,-,-,-,6-12 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Strategy Intern,"     This role is a high visibility role providing an all encompassing understanding of business and opportunity to work on strategy.      We have some interesting projects in Strategy, Consulting,Finance, Business Process Improvement.        What are we looking for:              Effective collaboration and coordination across various internal stakeholders.          Comfortable dealing with huge data and spreadsheet modeling          Good understanding of financial terms          Extremely detail-oriented and organized, and able to meet deadlines.          Quick learning ability, interest and intent in Exotel business lines.            The following are the important eligibility requirements for this Job:          1. Bachelor of Engineering with 2-3 years of work experience in roles such as CEO Office      2. MBA internship/ MBA Graduates    ",1.60E+11,16-04-2024,15-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Telecom / ISP,"Strategy consulting, Telecom, Business process improvement, Finance, Cloud, Banking, Management, Customer engagement, Internship",-,9am-6pm,"Full Time, Permanent",Exotel,Organization,Exotel,https://img.naukimg.com/logo_images/groups/v1/514742.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Sr . Data Platform Engineer,"     The Data Platform team in Avis is a level 3 technology specialist team to roll out new capabilities in to cloud to support data engineers, analysts, and business teams. We help to unlock the full value of data available with in enterprise systems to our partners and internal teams to achieve our business goals. Team is geographically dispersed team and responsible for administration of critical production systems across multi cloud environment. This role does    NOT    require night shifts but need to have overlapping hours between IST and US east morning hours for proper handholding with stake holders.     We are looking for a talented and passionate senior Cloud engineer with experience in data platform components to join this team. They are expected to provide technical leadership to a group of cloud engineers in the team, evaluate and roll out new solutions quickly with minimal guidance in accordance with recommended best practices and security policies. They play a key role in identifying opportunities for improvement in the current system practices, automate deployments practices and venture into new technical stacks with ease.     This role requires a resourceful individual, a persistent problem solver, and a strong hands-on engineer who can move around various technology stacks with ease. This is a great opportunity to have a big impact as part of a growing team during the ongoing technology and product transformation.         Responsibilities          Carry out POCs and Roll out new multi cloud solutions with minimal guidance.         Review Platform Infrastructure components, data warehousing environments and identify opportunities for improvement in terms of operational excellence, security, and cost optimization.         Provide technical leadership and responsible for team building by quick assimilation of knowledge about new systems being rolled out, documenting, and sharing the same with team members, assist them as required. You need to be an excellent team player.         Administration of critical data platform components and provide L3 support as needed. Job Requirements         Overall 8+ years experience in IT industry including 4+ years experience with AWS platform components.         Good understanding on the AWS cloud platform capabilities is mandatory.         Experience rolling out infrastructure as a code, using Terraform/Ansible stacks.         Very good understanding of Cloud databases and technologies including AWS RDS Redshift.         Good knowledge of relational database concepts and SQL skills are needed. Minimum 2 years of Team lead experience.         AWS Network IAM services experience. Understand network topologies and common network protocols and services with knowledge on trouble shooting.         Operating Systems: 3+ years of experience in Linux/Unix environment.         Experience with scripting languages: Python/Shell.         Experience with AWS data migration services.         Experience with CI/CD tools and scripted pipelines with focus on Jenkins.         Experience integrating applications with third party encryption tools like voltage.         Interpersonal skills to include effective verbal and written communication within team, key leaders, and other departments.         Ability to understand the big picture and clarity of thoughts to actively contribute to solutioning calls involving multiple teams.         Good to have skills:          AWS associate level certification or professional certifications.         Experience with GCP/OCI is preferred.         Experience with Confluent Kafka cluster administration.         Distributed system architecture awareness.         Experience with Tableau and Apache Airflow.         Experience setting up monitoring and alerting tools like Dynatrace and cloud native logging and monitoring frameworks     ",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Travel & Tourism,"Unix, System architecture, Data migration, Linux, GCP, Apache, Monitoring, Team building, SQL, Python",-,9am-6pm,"Full Time, Permanent",Avis Budget,Organization,Avis Budget,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Microsoft Azure Analytics Services
","Must have: -Microsoft Azure Analytics Services A Strong experience in Azure is preferred with hands-on experience in two or more of these skills : Azure Synapse Analytics, Azure HDInsight, Azure Databricks with PySpark / Scala / SparkSQL, Azure Analysis Services B Experience in handling medium to large Big Data implementations C Candidate must have 5-6 years of IT experience and around 3 years of Big data experience design  build",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Pyspark, Azure Databricks, Scala, Microsoft Azure Analytics Services, SparkSQL, Big Data, Azure Analysis Services, Data Lake",-,9am-6pm,"Full Time, Permanent",Net2Source LLP  ,Organization,Net2Source LLP  ,-,Kolkata,Kolkata,-,-,-,8-18 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Alloy DB Engineer,"     Design, implement, and maintain AlloyDB solutions on the Google Cloud Platform.          Collaborate with development teams to optimize data models, queries, and database performance.          Perform AlloyDB installations, upgrades, and patch management following best practices.          Monitor and ensure the availability, integrity, and security of AlloyDB databases.          Implement and maintain backup and recovery strategies to safeguard data and ensure business continuity.          Collaborate with DevOps teams to automate routine AlloyDB administration tasks and deployment processes.          Troubleshoot and resolve database-related issues, ensuring minimal downtime.          Stay updated on the latest trends and features in AlloyDB and GCP database services.          Implement and enforce database security measures and access controls.        Skill Strong SQL, Python (AlloyDB for Python library),AlloyDB JDBC driver,        Tools Cloud dataflow, Apache Spark, Apache Hadoop (Optional)          Experience:    8 10 years (5 years hands-on )    ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Patch management, GCP, spark, database security, Cloud, Hadoop, JDBC, Business continuity, SQL, Python",-,9am-6pm,"Full Time, Permanent",Sbcs India,Organization,Sbcs India,-,REMOTE,REMOTE,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
MLOps Engineer,"     We are seeking a skilled MLOps Engineer with 4-5 years of experience to design, implement, and manage machine learning operations that align with our companys strategic goals.     ?The ideal candidate will have a strong background in machine learning, software engineering, and data system architectures.      You will play a critical role in enhancing our machine learning lifecycle from model development to production, ensuring scalability, efficiency, and reliability.             What you will be expected to do      Design and implement robust MLOps solutions that integrate with existing CI/CD pipelines and data platforms.     Develop automated tools and frameworks for model training, testing, deployment, and monitoring.     Partnering with data scientists and software engineers to ensure operational and architectural alignment.     Maintain and optimize machine learning infrastructure for performance and scalability.     Implement best practices for data governance, security, and compliance in machine learning operations.     Continuously research and integrate new MLOps tools and methodologies to enhance workflow efficiencies.     You might be a strong candidate if you have/are      Bachelor s Degree in Computer Science or a similar quantitative field. Master s degree or PhD preferred     4+ years of experience in Machine Learning and related fields     2+ years of experience in an MLOps or related role, with a proven track record of deploying and managing ML systems in production.     Strong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch) and programming languages (e.g., Python, Scala).     Experience with containerization technologies (e.g., Docker, Kubernetes) and cloud services (e.g., AWS, Azure, Google Cloud).     Familiarity with data orchestration tools (e.g., Apache Airflow) and model monitoring solutions.     Excellent problem-solving, teamwork, and communication skills.     Certifications in relevant technologies (e.g., AWS Machine Learning, Kubernetes) is good to have.     ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Power,"Computer science, Backend, Architecture, Machine learning, Workflow, Apache, Analytics, Monitoring, Android, Python",-,9am-6pm,"Full Time, Permanent",Sun King,Organization,Sun King,https://img.naukimg.com/logo_images/groups/v1/6899095.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Snowflake Engineer,"         Collaborate with cross-functional teams to gather business requirements and translate them into scalable Snowflake-based data solutions.     Design, implement, and maintain end-to-end data pipelines using Snowflake Data Warehouse and related technologies.     Develop ETL processes to transform and load data from diverse sources into Snowflake.     Design and optimize data models and schemas for performance and query efficiency.     Implement best practices for data governance, quality, and security in Snowflake.      Work closely with Data Scientists and Analysts to provide them with accurate and accessible data for analysis and reporting.     Troubleshoot and resolve issues related to data pipelines, performance, and data integrity.     Stay updated with the latest Snowflake features and advancements in data engineering to ensure continuous improvement.     Collaborate with team members on code reviews and contribute to maintaining code quality and standards.       Required Skills and Qualifications:        Bachelors degree in Computer Science, Engineering, or related field (or equivalent experience).      Proven track record of 4-7 years in data engineering, with a focus on Snowflake Data Warehouse.     Strong expertise in designing and implementing data solutions using Snowflake, including data modeling, ETL processes, and data integration.     Proficiency in SQL queries and scripting for data transformation and integration.     Experience with Snowflakes features and capabilities, such as virtual warehouses, data sharing, and Snowflake functions.     Familiarity with cloud platforms (e.g., AWS, Azure) and their integration with Snowflake.     Strong problem-solving skills and the ability to diagnose and troubleshoot complex data engineering challenges.     Excellent communication skills to collaborate effectively with a multidisciplinary team.     Experience with data governance, data quality, and data security practices.      Prior experience mentoring and guiding junior data engineers is advantageous.     Familiarity with Big Data technologies (Hadoop, Spark) and familiarity with containerization (Docker) is a plus.     Snowflake certification is a desirable asset.      ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Data modeling, data security, data governance, Data quality, data integrity, Continuous improvement, big data, Troubleshooting, Data warehousing",-,9am-6pm,"Full Time, Permanent",Sasvatinfotech,Organization,Sasvatinfotech,-,Vadodara,Vadodara,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
MLOps Engineer - D&A Platform,"             As an engineer in this team you will design, develop, and maintain robust end-to-end capabilities for our Machine Learning Platform that enable data and analytics teams across the organisation to deliver value from data, whilst creating a culture of collaboration and data excellence        You will advocate the practice of applying software engineering principles to ensure the reproducible development of reliable and innovative ML products                          Do you fit the profile?                              As a Kubernetes Platform Engineer, you will be an empowered member in a committed, cross-functional team that develops Volvo Cars ML platform in collaboration with stakeholders across the company.                      In this role, team contributors thrive best.                      We envisage that you embrace agile working principles, enjoy solving problems and are committed to learning and development.                      You should have experience in Software Engineering, the Public Cloud and Kubernetes and be fluent in spoken and written English.                      An awareness and interest in Machine Learning is a distinct advantage.                            Key competencies for the position include:                        Significant experience of deploying and operating applications, in production, on Kubernetes.                      Strong knowledge of at least one programming language, ideally Python or Go.                      Thorough understanding of the Software Engineering discipline with a passion for automation and security.                      Comfortable in the use of docker and containerisation.                      Knowledge of public cloud services; AWS or Azure are preferred.                      Familiarity with relevant technologies from the cloud-native ecosystem, such as Kubeflow, Istio, TektonCD and KNative.                              Who you are and what you believe in                            Embraces We Culture                    You are customer focused and have a solution-oriented mindset                  You excel in collaboration, both with internal and external stakeholders, and are passionate about building a work environment based on trust                        Curious                    You are passionate about software development and you have an interest in staying up to date with new technologies                  You are self-propelled and have an innovative and creative mind                        Agile Mindset                    You have a strong belief in agile ways of working                  You are flexible, open to new ideas and have a strong focus on continuous improvement                        Communication                      You are an avid communicator, and are fluent in both spoken and written English                  You can build compelling and interesting presentations supporting your ideas                  You manage stakeholder expectations at various levels with ease                  You navigate confidently in a multi-cultural environment            ",1.30E+11,13-03-2024,11-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Automobile,"Automation, Cloud Services, Machine learning, Cloud, Agile, Data analytics, Technology solutions, Continuous improvement, Analytics, Python",-,9am-6pm,"Full Time, Permanent",Volvo Auto,Organization,Volvo Auto,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer - D&A Platform,"           As a Data Platform Engineer in Data Platforms you will design, develop, and maintain robust end-to-end platform capabilities that enable data and analytics teams across the organization to deliver value from data, whilst building a culture of collaboration and data excellence.                      We are mandated to define and build a data platform based on modern technology to host our company-wide data mesh for the rapidly growing data community at Volvo Cars.                            Main responsibilities                            As a Data Platform Engineer you will be responsible for DevOps of business critical data platform capabilities in collaboration with the team, product managers and the data engineering community.                  By contributing with expertise and teamwork you will be instrumental in our mission to transform the data ecosystem to empower our company with trusted data.                  We are in an early phase of our transformation journey so you will have a key role in contributing to our technical direction and forming a great team culture.                            Do you fit the profile?                              As a Data Platform Engineer, you will be an empowered member in one of our committed teams that develops our Data & Analytics platform in collaboration with other teams. In this role, team contributors thrive best.                      You want to contribute to team commitments and are committed to learning and development.                      We envisage that you embrace agile working principles and enjoy solving problems.                      You should also have experience in software engineering, modern data stacks, cloud infrastructure and be fluent in spoken and written English.                                        Other key competencies for the position include:                        Experience in Python and in at least one other programing language (C, C++, Java, Javascript, Go )                      Experience with infrastructure as code and development frameworks                      Experience in building self-service platform and tools from scratch                      Practical experience with Data Engineering and the accompanying DevOps & DataOps workflows.                      Experience with designing and delivering data platforms with built-in lineage, federation, governance, compliance, security, and privacy.                      Experience with containerisation technologies, such as Docker and Kubernetes                              Who you are and what you believe in                            Embraces We Culture                    You are customer focused and have a solution-oriented mindset                  You excel in collaboration, both with internal and external stakeholders, and are passionate about building a work environment based on trust                        Curious                    You are passionate about software development and you have an interest in staying up to date with new technologies                  You are self-propelled and have an innovative and creative mind                        Agile Mindset                    You have a strong belief in agile ways of working                  You are flexible, open to new ideas and have a strong focus on continuous improvement                        Communication                      You are an avid communicator, and are fluent in both spoken and written English                  You can build compelling and interesting presentations supporting your ideas                  You manage stakeholder expectations at various levels with ease                  You navigate confidently in a multi-cultural environment              ",1.30E+11,13-03-2024,11-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Automobile,"C++, C, devops, Javascript, Agile, Infrastructure, Data analytics, Technology solutions, Continuous improvement, Python",-,9am-6pm,"Full Time, Permanent",Volvo Auto,Organization,Volvo Auto,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer - Golden Path,"         As a Data Platform Engineer in Data Platforms you will design, develop, and maintain robust end-to-end platform capabilities that enable data and analytics teams across the organization to deliver value from data, whilst building a culture of collaboration and data excellence.                  We are mandated to define and build a data platform based on modern technology to host our company-wide data mesh for the rapidly growing data community at Volvo Cars.                             Main responsibilities                          As a Data Platform Engineer you will be responsible for DevOps of business critical data platform capabilities in collaboration with the team, product managers and the data engineering community.                  By contributing with expertise and teamwork you will be instrumental in our mission to transform the data ecosystem to empower our company with trusted data.                  We are in an early phase of our transformation journey so you will have a key role in contributing to our technical direction and forming a great team culture.                          Do you fit the profile                         As a Data Platform Engineer, you will be an empowered member in one of our committed teams that develops our Data Analytics platform in collaboration with other teams     In this role, team contributors thrive best     You want to contribute to team commitments and are committed to learning and development     We envisage that you embrace agile working principles and enjoy solving problems     You should also have experience in software engineering, modern data stacks, cloud infrastructure and be fluent in spoken and written English                              Other key competencies for the position include:                     Experience in Python and in at least one other programing language (C, C++, Java, Javascript, Go )                 Experience with infrastructure as code and development frameworks                 Experience in building self-service platform and tools from scratch                 Practical experience with Data Engineering and the accompanying DevOps DataOps workflows.                 Experience with designing and delivering data platforms with built-in lineage, federation, governance, compliance, security, and privacy.                 Experience with containerisation technologies, such as Docker and Kubernetes                         Who you are and what you believe in                       Embraces We Culture                      You are customer focused and have a solution-oriented mindset                 You excel in collaboration, both with internal and external stakeholders, and are passionate about building a work environment based on trust                     Curious                     You are passionate about software development and you have an interest in staying up to date with new technologies                 You are self-propelled and have an innovative and creative mind                     Agile Mindset                      You have a strong belief in agile ways of working                 You are flexible, open to new ideas and have a strong focus on continuous improvement                     Communication                     You are an avid communicator, and are fluent in both spoken and written English                 You can build compelling and interesting presentations supporting your ideas                 You manage stakeholder expectations at various levels with ease                 You navigate confidently in a multi-cultural environment           ",1.30E+11,13-03-2024,11-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Automobile,"C++, devops, Javascript, Agile, Data analytics, Technology solutions, Continuous improvement, Python",-,9am-6pm,"Full Time, Permanent",Volvo Auto,Organization,Volvo Auto,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer - Data Infrastructure,"           As a Data Platform Engineer in Data Platforms you will design, develop, and maintain robust end-to-end platform capabilities that enable data and analytics teams across the organization to deliver value from data, whilst building a culture of collaboration and data excellence.                      We are mandated to define and build a data platform based on modern technology to host our company-wide data mesh for the rapidly growing data community at Volvo Cars.                                     Main responsibilities                            As a Data Platform Engineer you will be responsible for DevOps of business critical data platform capabilities in collaboration with the team, product managers and the data engineering community.                  By contributing with expertise and teamwork you will be instrumental in our mission to transform the data ecosystem to empower our company with trusted data.                  We are in an early phase of our transformation journey so you will have a key role in contributing to our technical direction and forming a great team culture.                            Do you fit the profile                               As a Data Platform Engineer, you will be an empowered member in one of our committed teams that develops our Data Analytics platform in collaboration with other teams       In this role, team contributors thrive best     You want to contribute to team commitments and are committed to learning and development     We envisage that you embrace agile working principles and enjoy solving problems     You should also have experience in software engineering, modern data stacks, cloud infrastructure and be fluent in spoken and written English                                     Other key competencies for the position include:                         Experience in Python and in at least one other programing language (C, C++, Java, Javascript, Go )                     Experience with infrastructure as code and development frameworks                     Experience in building self-service platform and tools from scratch                     Practical experience with Data Engineering and the accompanying DevOps DataOps workflows.                     Experience with designing and delivering data platforms with built-in lineage, federation, governance, compliance, security, and privacy.                     Experience with containerisation technologies, such as Docker and Kubernetes                             Who you are and what you believe in                         Embraces We Culture                      You are customer focused and have a solution-oriented mindset                 You excel in collaboration, both with internal and external stakeholders, and are passionate about building a work environment based on trust                     Curious                     You are passionate about software development and you have an interest in staying up to date with new technologies                 You are self-propelled and have an innovative and creative mind                     Agile Mindset                      You have a strong belief in agile ways of working                 You are flexible, open to new ideas and have a strong focus on continuous improvement                     Communication                     You are an avid communicator, and are fluent in both spoken and written English                 You can build compelling and interesting presentations supporting your ideas                 You manage stakeholder expectations at various levels with ease                 You navigate confidently in a multi-cultural environment           ",1.30E+11,13-03-2024,11-06-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Automobile,"C++, devops, Javascript, Agile, Data analytics, Technology solutions, Continuous improvement, Python",-,9am-6pm,"Full Time, Permanent",Volvo Auto,Organization,Volvo Auto,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Engineer,"What You Will Be Doing Cloud Cost monitoring and reporting: Monitor cloud spend day to day and prepare daily, weekly & monthly reports for review of Cloud spend. Cloud Cost Optimization: Perform infra cost optimization operational work such as infra rightsizing, decommission of unused cloud resources such as servers, databases and other cloud resources and cleanups as needed AWS Cost Management Tools: Utilize and configure cloud cost management tools, such as AWS Cost Explorer, Azure cost explorer, Cloudhealth etc, to monitor, track, and manage costs effectively. Cost Optimization Strategies: Detect and report deviations from baseline, find areas of improvement and provide recommendation for optimum usage of various AWS/Azure Services. Stay Updated on AWS Cost Management: Stay abreast of the latest AWS cost management tools, features, and best practices. Assis team in evaluating and recommend new cost optimization strategies and technologies Automation: Ability to write automation scripts/utilities to automate cloud-environment asset reporting, de-provisioning or resources. Collaboration: Collaborate with Engineering and cloud teams to implement cost optimization recommendations and best practices What You Bring 1-2+ years supporting AWS/Azure environments and native services. Understanding of Azure and AWS cloud platforms Cloud pricing schemes and various savings programs such as enterprise discount plans, reserved instances, compute saving plans, on-demand spend. Experience with native and 3rd party Public Cloud cost management & advisory tools (AWS Cost Explorer, AWS Trusted Advisor, Azure Cost management+billing, cloudhealth or any other cloud cost management tool) Experience automating querying of Cost and related REST API is a plus",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Software Product,"AWS, cloud, REST API, Azure, AWS Trusted Advisor, Public Cloud, cloud cost management tool, AWS Cost Explorer, cloud computing, Cloud pricing schemes, Azure Cost management",-,9am-6pm,"Full Time, Permanent",AVE-Promagne Business Solutions,Organization,AVE-Promagne Business Solutions,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Hiring Senior Databricks Engineer Immediate Joiner,"About Us: Hvantage Technologies Inc. is at the forefront of innovation in finance, healthcare, technology etc. We leverage the latest in cloud and data analytics technologies to drive our mission forward, solving complex problems and delivering value to our customers. We are looking for talented individuals who are passionate about making a difference using big data and advanced analytics. Position: Senior Databricks Engineer  Location: Indore, India (Work-from-Office) Immediate Joiner Preferred Position Overview: As a Senior Databricks Engineer, you will play a pivotal role in our data team, focusing on leveraging Databricks to drive insights, optimize operations, and power predictive models that inform strategic decisions. Your expertise will support the development, deployment, and management of scalable data analytics and machine learning solutions that meet our business objectives. Key Responsibilities: Design and implement scalable and reliable data pipelines within the Databricks platform, ensuring data quality and accessibility for analytics and machine learning applications. Led the integration of Databricks with various data sources and platforms (e.g., SQL, NoSQL, Data Lakes, and Cloud Storage) to support data ingestion, processing, and analytics. Develop and optimize Spark jobs for data transformation and aggregation. Work closely with data scientists and analysts to deploy machine learning models at scale, ensuring performance and reliability. Implement best practices for data governance, security, and compliance within the Databricks environment. Mentor junior team members and contribute to the development of internal knowledge bases and documentation. Stay up-to-date with advancements in Databricks, Spark, and related cloud data technologies to continually enhance our capabilities. Qualifications: Bachelors or Masters degree in Computer Science, Engineering, Mathematics, or a related field. Minimum of 5 years of experience in data engineering with at least 2 years focused on Databricks and Apache Spark. Strong proficiency in languages such as Python, Scala, or SQL. Demonstrated experience designing and implementing data pipelines and architectures within cloud environments (AWS, Azure, GCP). In-depth understanding of big data technologies, data modeling, ETL processes, and data warehousing principles. Experience with machine learning model deployment and lifecycle management within Databricks is highly desirable. Knowledge of data governance, security practices, and compliance regulations relevant to data and cloud platforms. Excellent problem-solving skills, along with the ability to work independently and as part of a cross-functional team. Strong communication skills, capable of conveying complex technical concepts to non-technical stakeholders. Benefits: Competitive salary and benefits package. Opportunities for professional growth and career advancement. Dynamic and collaborative work environment with a focus on innovation and continuous learning. If you meet the above requirements and are excited about the opportunity to work as a Senior Databricks Developer in a dynamic and innovative environment, Join our team and be part of an organization that values innovation, collaboration, and personal growth. We offer competitive compensation, comprehensive benefits, and a stimulating work environment that encourages learning and professional development. To apply, please submit your resume and a cover letter highlighting your relevant experience and skills to neelesh@hvantage.com. Or you can share your CV over WhatsApp at +919755299999 Note: Only shortlisted candidates will be contacted for further evaluation. Hvantage Technologies is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.",1.20E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Data Bricks, Aws Data Lake, Pyspark, Data Engineering, Sql Dw, Azure Databricks, SQL Azure, Azure Data Factory, Datafactory, Data Pipeline, Azure Cloud, GCP, SCALA, Azure Data Lake, Data Lake, Ms Azure, Spark, AWS, Python",-,9am-6pm,"Full Time, Permanent",Hvantage Tech Solutions Pvt. Ltd.,Organization,Hvantage Tech Solutions Pvt. Ltd.,https://img.naukimg.com/logo_images/groups/v1/2847282.gif,Indore,Indore,-,-,-,8-18 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Backline Engineer, Platform","   As a Backline Technical Solutions Engineer you will help our customers to be successful with the Databricks platform by resolving important technical customer escalations and working closely with the frontline support team. You will be the technical bridge between support and engineering and the first line of defense for engineering. You will ensure that all issues are vetted by you before it reaches the engineering team. You will report to the Manager of the Backline Engineering Team.         ?       The impact you will have:         Troubleshoot and resolve complex customer issues related to the Databricks Platform by analyzing the core component metrics and logs.     Provide suggestions and best practice guidance for improving performance in customer-specific environments and providing product improvement feedback.     Help the support team with detailed troubleshooting guides and runbooks.     Contribute to automation and tooling programs to make daily troubleshooting efficient.     Partner with the engineering team and spread awareness of upcoming features and releases.     Identify and contribute supportability features back into the product.     Demonstrate ownership and coordinate with engineering and escalation teams to achieve resolution of customer issues and requests.     Participate in weekend and weekday on-call rotation.         What we look for:         Minimum 6 years experience developing, testing, and sustaining Python or Java or Scala-based applications.     Expert in scripting using Python or Shell.     Comfortable with black box troubleshooting.     Experience troubleshooting issues related to ""Distributed Big Data Computing"" environment.     Experience with SQL-based database systems.     Experience in Linux troubleshooting is required.     Experience with AWS, Azure or GCP related services.     Bachelors degree in Computer Science or a related field is required     ",90424500039,09-04-2024,08-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SAN, Automation, GCP, spark, Linux troubleshooting, Diversity and Inclusion, Data analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Databricks,Organization,Databricks,https://img.naukimg.com/logo_images/groups/v1/4128698.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer (K8s),"     We are seeking a Platform Engineer with a specialization in Kubernetes to join our innovative technology team     This role focuses on the development, implementation, and optimization of Kubernetes-based solutions     The ideal candidate will have a strong background in software engineering, intermediate experience with Kubernetes, and a commitment to developing high-quality, scalable, and resilient cloud-native applications     This position offers a unique opportunity to play a leading role in shaping and advancing our Kubernetes-based platforms     If you are passionate about cloud-native technologies and eager to work in a dynamic, innovative environment, we would love to hear from you     This position will be ideally hybrid from our     Bengaluru Office (   HSR     Layout)       as part of our expanding site location       EarnIn provides excellent benefits for our employees including healthcare, internet/cell phone reimbursement, a learning and development stipend, and potential opportunities to travel to our Palo Alto HQ     Our salary ranges are determined by role, level, and location          WHAT you'll Do         Design and develop robust Kubernetes-based applications and infrastructure.     Implement automation tools and frameworks (CI/CD pipelines) for the rapid deployment and scalability of Kubernetes clusters.     Collaborate with development teams to ensure seamless integration of Kubernetes into the overall software development lifecycle.     Optimize Kubernetes clusters for performance, scalability, and high availability.     Contribute to the design and improvement of microservices architecture.     Adhere to software development and Kubernetes best practices.     Stay abreast of emerging trends and technologies in Kubernetes and cloud computing.         WHAT WERE LOOKING FOR         bachelors or masters degree in Computer Science, Engineering, or a related field.     Minimum of 2 years of experience in software development, with a focus on Kubernetes and cloud-native technologies.     Intermediate experience in Kubernetes cluster management and orchestration.     Strong programming skills in languages such as Go, Python, or Java.     Experience with containerization technologies like Docker.     Knowledge of cloud service providers, preferably AWS.     Familiarity with CI/CD tools and DevOps practices.     Excellent problem-solving abilities and a passion for technical challenges.     Strong communication skills and the ability to work effectively in a team environment     ",40424500626,04-04-2024,03-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Financial Services,"Cloud computing, orchestration, Finance, Cloud, Cluster Management, Software development life cycle, Programming, Healthcare, Python",-,9am-6pm,"Full Time, Permanent",Earnin,Organization,Earnin,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer,"We are looking Platform Engineer Lvl 2 Skills: Java, Java Springboot, AWS SQS/SNS, Microservices Location: Pune, Bangalore, Kolkata Expereince:6-10 Years Immediate to 30 Days Notice only Regards Amit-7080197058 grconsultants596.in@gmail.com",10524004509,01-05-2024,30-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"java, Java Spring Boot, aws, Microservices",-,9am-6pm,"Full Time, Permanent",GR Engineering Projects,Organization,GR Engineering Projects,-,"Kolkata, Pune, Bengaluru","Kolkata, Pune, Bengaluru",-,-,-,12-20 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Technical Consultant-Azure Cloud Engineer,"               Skillset :    Native SE & Cloud/Backend, DevOps, Cloud, Native, Cloud Integration - Mastery, Azure PaaS - Skill, Azure Data Factory - Skill         ?       Job Description :               Work with the Lead Segment Architect tomaintain the segment architectures, applicationportfolio inventory, landscape diagrams andlifecycle maps for the applications within theportfolio.     Identify and propose potential portfoliochanges and improvements, includingdecommissioning. Develop and maintainapplication roadmaps for the portfolio.     Work directly with the ITSO portfolio andproject teams to understand servicedevelopment plans and these back to ET SOM.Monitor applications portfolio health and addresschronic deficiencies.     Work with the stakeholders to provideconsultancy on architecture and portfoliomatters. This includes, but is not limited tobenchmarking of the portfolio against externalpractice; externalisation efforts to keep a highlyvisible external perspective; advise on anyproject issues; advise touch points with otherLines of Business and opportunities to leverageapplications, processes etc.     Work closely with Lead Segment Architect andother architects to assure an efficientapplications landscape and adherence toprevailing standards and guidelines.     Ensure architecture artifact quality andretention in appropriate tools     Guide and support ET SOM teams on the useof ITSO services, segment and referencearchitectures. Provide, or make available throughothers, technical expertise and support to projectand support personnel on IT matters in theportfolio.     Participate in governance processes, e.g.,assurance review and signoff, and share allrelevant needed documentation. Participate asrequired at the Architecture Review Board andrelevant IT Leadership Teams.     Participate in and contribute to the Line ofBusiness segment architecture network.             ",2.81E+11,28-09-2023,27-12-2023,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Architect, Backend, Assurance, Architecture, Networking, PAAS, Cloud, Consultancy, Inventory, Monitoring",-,9am-6pm,"Full Time, Permanent",Andor Tech,Organization,Andor Tech,https://img.naukimg.com/logo_images/groups/v1/4690465.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Database Professional, Frontend Backend technologies   AWS cloud   Mobile development/Automation   Mid to sr level hiring   Analytic & Data science ,2.81E+11,28-07-2023,26-10-2023,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Automation, Backend, Mobile development, Front end, data science, Cloud, Database, AWS, Analytics",-,9am-6pm,"Full Time, Permanent",Wissda Consulting,Organization,Wissda Consulting,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Workload Solutions Engineer (AI/ML, HPC)","             Demonstrated hands-on expertise and working with container technologies (Kubernetes, Docker) and developing complex solutions                  Demonstrate technical architecture and design skills                  Demonstrated understanding of infrastructure design and hands-on experience working with hardware (e.g. server, storage, networking) preferably                 Strong Linux OS and performance experience                  Demonstrated workload experience in one or more key use-case segments (AI/ML, Analytics, HPC)                  Strong analytical and problem-solving skills.                  Advanced Python, Powershell, Javascript, Node.js or similar languages                  Ansible, Terraform and other scripting / automation toolkits        Experience with Agile development practices   ",2.81E+11,28-07-2023,26-10-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Automation, Linux, Networking, Powershell, Analytical, Agile development, Infrastructure design, Technical architecture, Analytics, Python",-,9am-6pm,"Full Time, Permanent",Wissda Consulting,Organization,Wissda Consulting,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Engineer I - Platform Engineer,"         This is an individual contributor role to manage and build Public, Private and Hybrid cloud infrastructure at in highly hydrogenous environment.                     Familiarity with API & integration patterns to securely communicate with backend services and clients.                  Uses technical acumen to provide alternatives, ask probing questions and support the team in technical issue resolution                  Coordinates communications, escalates, and facilitates resolution of risks, issues, and changes tied to the Infrastructure and Operations                    Qualifications:                      Overall 6+ years of industry experience with 4-5 Years of relevant experience                  development experience in a modern development stack (Java or Golang preferred) is essential.                  3+ years of experience working with public cloud services (preferably AWS or GCP) and a proven track record of building complex infrastructure.                  3+ years of Strong hands-on deployment and troubleshooting expertise experience working with private cloud (OpenShift, Kubernetes, Docker)                  Demonstrated professional expertise in a large Fortune 100 company running an IT Production Operation at scale including Infrastructure (Servers, Network, Storage, Security) and Applications (Distributed, Middleware, Databases across all tiers)                  Ability to work with Infrastructures and Platforms including IaaS, PaaS, Cloud technologies and tools for Continuous Delivery (CD)                  In depth understanding of Linux functionalities/ features as well as good experience of Linux system engineering.                  Good understanding of web technologies (http, headers, authentication, ssl, routers, load balancing, etc.)                      At least medium level expertise in one of following:                              Python/ shell scripting                     Golang and Rest API                         Desirable experience                                 Splunk                              ELK                              Node.js                              Java, JBoss, WAS                              Ansible                              Puppet                              Cloud management and administration                              Continuous delivery experience/DevOps/Agile                    Other skills                                Proven diagnostic, troubleshooting, and service restoration skills.                              Excellent interpersonal skills, customer service skills, and English communication skills with difficult customers in outage situations.                              Metrics management                              Outstanding written and verbal communication skills                              Benefits include:              Competitive base salaries              Bonus incentives              Support for financial-well-being and retirement.              Comprehensive medical, dental, vision, life insurance, and disability benefits (depending on location)              Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need.              Generous paid parental leave policies (depending on your location)              Free access to global on-site wellness centers staffed with nurses and doctors (depending on location)              Free and confidential counseling support through our Healthy Minds program              Career development and training opportunities                       ",2.80E+11,28-02-2024,28-05-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Career development, Backend, Linux, Social media, Shell scripting, Agile, Issue resolution, HTTP, Troubleshooting, Open source",-,9am-6pm,"Full Time, Permanent",Resy,Organization,Resy,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer," As a data engineer at Onehouse, you will contribute directly to Apache Hudi and the surrounding open source ecosystem, while deploying and operating these technologies at massive scale for our customers.           ?     Responsibilities        Be the thought leader around all things data engineering within the company - schemas, frameworks, data models.     Implement new sources and connectors to seamlessly ingest data streams.     Building scalable job management on Kubernetes to ingest, store, manage and optimize petabytes of data on cloud storage.     Optimize Spark or Flink applications to flexibly run in batch or streaming modes based on user needs, optimize latency vs throughput.     Tune clusters for resource efficiency and reliability, to keep costs low, while still meeting SLAs               Must Haves        3+ years of experience in building and operating data pipelines in Apache Spark or Apache Flink.     2+ years of experience with workflow orchestration tools like Apache Airflow, Dagster.     Proficient in Java, Maven, Gradle and other build and packaging tools.     Adept at writing efficient SQL queries and trouble shooting query plans.     Experience managing large-scale data on cloud storage.     Great problem-solving skills, eye for details. Can debug failed jobs and queries in minutes.     Operational excellence in monitoring, deploying, and testing job workflows.     Open-minded, collaborative, self-starter, fast-mover.               Bonus Skills        Hands-on experience with k8s and related toolchain in cloud environment.     Experience operating and optimizing terabyte scale data pipelines     Deep understanding of Spark, Flink, Presto, Hive, Parquet internals.     Hands-on experience with open source projects like Hadoop, Hive, Delta Lake, Hudi, Nifi, Drill, Pulsar, Druid, Pinot, etc.     Operational experience with stream processing pipelines using Apache Flink, Kafka Streams.             ? ",2.71E+11,27-06-2023,25-09-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Maven, orchestration, Packaging, Silicon, Apache, microsoft, Open source, Operations, Monitoring",-,9am-6pm,"Full Time, Permanent",Onehouse,Organization,Onehouse,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
UI Engineer - Low Code Platform,"   ?       Develop and optimize applications using React, Redux, NodeJS, Express, and TypeScript.     Collaborate with cross-functional teams to understand user needs and translate them into technical requirements.     Contribute ideas for new features and identify areas for improvement proactively.     Ensure the technical feasibility and superb performance of UI/UX designs.     Advocate for best practices, great usability, and exceptional quality.     Integrate cutting-edge technologies and approaches to enhance user experience.     Stay abreast of the latest trends and technologies in UI development.         Qualifications:         1-4 years of relevant experience in UI development.     Proficient in TypeScript, React, Redux, NodeJS, Express, and REST APIs.     Experience with Webpack and knowledge of CI/CD pipelines.     Strong problem-solving skills and a passion for creating intuitive and responsive user interfaces.     Excellent communication skills and ability to articulate technical ideas.     A team player who is also comfortable working independently.     A creative thinker with a user-centric approach to development.         What We Offer:         The opportunity to work on a cutting-edge platform and contribute to transformative projects.     A dynamic, innovative atmosphere with a focus on continuous improvement and excellence.     A supportive environment that fosters collaboration and creativity.     Competitive salary and a comprehensive benefits package.     ",2.61E+11,26-12-2023,25-03-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"UI development, Coding, Advocate, Continuous improvement",-,9am-6pm,"Full Time, Permanent",Ivoyant,Organization,Ivoyant,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Staff Engineer,"   Must Have Skills:         Big Data     Azure stack like Azure Synapse, Data Lake, Azure analytics services, ADF pipelines, Azure Data Bricks     SQL and NoSQL (Elasticsearch / MongoDB / Cassandra)             Good To Have:         Python     .Net     AI/ML (1-2yrs Exp)   ",2.61E+11,26-08-2023,24-11-2023,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"NoSQL, cassandra, MongoDB, big data, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Hadoop Admin Professional,"   Good experience in Hadoop Admin      Deep understanding and strong conceptual knowledge in Hadoop architecture components.      Highly skilled on Hadoop Cluster Setup      Expertise in adding and removing nodes, keeping track of jobs, monitoring critical alerts, configuring      Strong hands-on experience and knowledge of Hadoop core components such as HDFS, YARN, Hive, etc      Hands-on experience and knowledge of Linux and Hardware.      Basic experience and knowledge of one of automation tools such as Chef, Puppet, Ansible.      Strong analytical mind to help solve complicated problems.      Desire to resolve issues and dive into potential issue.      Self-starter who works with minimal supervision and ability to work in a team of diverse skill sets.    ",2.60E+11,26-03-2024,24-06-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Administration, Payroll, Linux, Analytical, Hadoop, Automation tools, YARN, Monitoring, Supervision",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
CIAM Platform Engineer,"     Expert knowledge in Customer Identity and Access Management (CIAM) relative to Access Management, Identity Providers (IdP), Authentication/Multi-Factor Authentication (MFA), Authorization, and Single Sign-On (SSO)          Expert knowledge and 4+ Years demonstrable hands on work with Auth0 Customer Identity and Access Management (CIAM) and configuration of authentication, custom database integration, custom branding, action scripts and/or rules, policies, etc.          Expert knowledge and strong hands-on experience with industry standard Authentication / Authorization / Federation / SSO technologies and protocols (SAML, OAuth2.0, OpenID Connect, WS-Fed, FIDO, SCIM, etc.)          Understanding of the Auth0 Deploy CLI tool for integration of Auth0 tenants with CI/CD Pipelines          Understanding of the Auth0 API set          Ability to translate architectural diagrams created by Sr. Architect of future state and migration stages with documentation describing stages of migration into actionable implementation steps          Ability to implement designed architectural deployment models for Auth0 components in relation to the overall Infrastructure, API, and Application integration which may include modern and legacy applications, datacenter, cloud, containers, etc. under direction of a Sr. Architect          Understanding of API design concepts, RESTful Services, and modern application interaction patterns          Strong hands-on experience with securing APIs with OAuth 2.0 and other means.          Familiarity with deployments and integration of IAM solutions within the cloud (Azure, AWS, GCP) and on prem data center environments          Experience in deploying large-scale, global projects and programs          Familiarity with IT security and risk management practices          Excellent written and verbal communication skills          Team player in dynamically changing environment            Desired            Recent experience with Auth0 CIAM environments and/or migration to Auth0          Exposure to Infrastructure as Code integration of Auth0 into the CI/CD pipeline          Degree in Computer Science, Engineering, Network Security or related field          Certifications relevant to IAM platforms (        Auth0        , Azure AD, Okta, etc.)          ITSM or InfoSec certifications (ITIL, CISSP, etc.)          Developer skills in any language working with IdP and IAM workflows      ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Senior Architect, Architecture, Access management, GCP, IT security, Network security, Infrastructure, SAML, Risk management",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Databrick and Datalake/Deltalake with cloud platform," You will work as a Data Modeller who is familiar with Cloud-based architectures and    DW/BI/Analytics    domain knowledge with experience in    Databricks or Spark.       You will partner closely with the clients to deliver state-of-the-art engineering solutions.     You will articulate and communicate with a diverse set of internal and external stakeholders with varying degrees of technical proficiency and deliver critical business and process-related information     to mitigate any risks or failures.     You will persistently look for opportunities to address customer needs by being a thought partner in every moment of engagement.     ?         Responsibilities:         Spearhead the development of conceptual, logical, and physical data models, while implementing data marts/data warehouse/data lake/data lakehouse on Spark-based platforms     Analyse existing models to identify variances/gaps and devise strategies to achieve alignment.     ?     Establish and enforce best practices and standards for data models.     Generate Data Flow diagrams and relevant artifacts to depict data integrations within targeted systems.     Collaborate closely with Data Owners to understand business data requirements and clarify terms and business rules.     Facilitate walkthrough sessions with Data Owners and stakeholders to review and validate data models for accuracy.     Demonstrate strong communication skills and effective engagement with delivery colleagues to ensure successful project outcomes.     Comply with Data Architecture standards and guardrails.     Proactively manage issues, risks, actions, and dependencies, escalating when     necessary.               Experience:           Must have:       Possess a minimum of 5+ years of    Data Modelling experience in Data Lake/Delta     Lakehouse/Data Warehouse on Spark based platforms on Azure/AWS/GCP.       Demonstrate a minimum of 1 year of experience working with/in    Databricks.         Mandatory experience of Data Modelling on Delta Lakehouse       Knowledge in    handling Spark streaming data over Delta Lakehouse.       Hands-on experience with    data modelling tools for performing reverse and forward       engineering of as-is and to-be data models, in any of the industry leading    data modelling   ?   tools such as Erwin/ ER/Studio etc.         Utilize visual notations for expressing data logical and physical models, state transitions,     transformations, and versioning. Familiarity with UML, DPMN, ERDs, etc. is ideal.           Good to have:       Knowledge of Databricks concepts like Apache Spark API, Delta, Delta Live, feature stores,     DBR, job/interactive clusters, notebook management, Repos, MLFlow     Knowledge of orchestration tools like Azure Data Factory, Airflow     Experience in other cloud DWH like Snowflake, Big Query, and Amazon Redshift.     Knowledge of in any Data Governance tool.     Knowledge of Unity Catalog.     Experience in DevOps and CI/CD. ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"orchestration, UML, GCP, Data modeling, data governance, business rules, Business solutions, Data warehousing, Analytics, Data architecture",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"   Must Have Skills- Data Engineer, Python, Scala, Kubernetes, Docker,Kafka, Scala and ETL                Job Responsibities-          Communicates data changes to impacted stakeholders, assist users with data queries, and provides training to users as necessary and applicable.          Communicates data changes to impacted stakeholders, assist users with data queries, and provides training to users as necessary and applicable.          Ability to design and build microservices in Python.          API and data format integration experience (REST, WebServices, XML, JSON)                  Designing and developing API(s) and Microservices using Python frameworks Flask, Django, and FastAPI.          Providing users access to datasets using REST and Python.          Extraction, transformation, and loading (ETL\ELT) of data from a wide variety of data sources (Structured, Unstructured, SQL, NoSQL, Blobs, Files Realtime);                  Create and maintain optimal data pipeline architecture.          Collaborate with data analysts, data scientists, and other stakeholders to understand their data needs and provide data pipeline solutions.          Develop frameworks necessary to monitor and troubleshoot data pipeline issues.          Troubleshoot and resolve issues related to Transformations (DBT\Other).            ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Web services, NoSQL, XML, Django, SCALA, JSON, Business solutions, Troubleshooting, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer," Develop, understand, and enhance code in traditional data warehouse environments, data lake, and cloud environments like Snowflake, Azure, Databricks          Build new end-to-end business intelligence solutions- data extraction, ETL processes applied on data to derive useful business insights, and best representing this data through dashboards.          Write complex SQL queries used to transform data using Python/Unix shell scripting              Understand business requirements and create visual reports and dashboards using Power BI or Tableau.          Upskill to different technologies, understand existing products and programs in place          Work with other development and operations teams.          Flexible with shifts and occasional weekend support.                Key Competencies            Full life-cycle experience on enterprise software development projects.          Experience in relational databases/ data marts/data warehouses and complex SQL programming.          Extensive experience in ETL, shell or python scripting, data modelling, analysis, and preparation          Experience in Unix/Linux system, files systems, shell scripting.          Good to have knowledge on any cloud platforms like AWS, Azure, Snowflake, etc.          Good to have experience in BI Reporting tools - Power BI or Tableau              Good problem-solving and analytical skills used to resolve technical problems.          Must possess a good understanding of business requirements and IT strategies.          Ability to work independently but must be a team player. Should be able to drive business decisions and take ownership of their work.          Experience in presentation design, development, delivery, and good communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.                Required/Preferred Skills            RDBMS and Data Warehousing (Required)          SQL Programming and ETL (Required)          Unix/Linux shell scripting (Required)          Power BI / Tableau (Preferred)          Python, Scala, Java (Preferred)          Cloud Platforms - Azure, Snowflake, Databricks, Datalake (Preferred).    ",2.51E+11,25-08-2023,23-11-2023,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Unix, tableau, Linux, RDBMS, Shell scripting, power bi, Business intelligence, Unix shell scripting, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer,"               We re looking for highly motivated Platform Engineers to join our growing team to facilitate the development, build, and release of existing and new products and services       Our team is responsible for high quality and highly available build and release pipelines as well as infrastructure     The evertz     io Engineering Team builds next-generation systems for content management and distribution in the Media and Entertainment industry     Disney, NBCUniversal, Discovery, BBC, and many other content producers and publishers use our products and services to make the most of their file-based and live content for the least effort     We work with high quality video in real-time and non-real-time scenarios across a wide range of cutting-edge tech     Our technology stack includes a Serverless microservice architecture that capitalizes on the full breadth of AWS services with code                         Skills and experience you will bring:                         5 to 10 years of experience leading large scale migration projects like on-prem to AWS, Jenkins to Jenkins, Major Salt stack version upgrade, Linux OS upgrade                     5 to 10 years of experience using Infrastructure as Code tools like Terraform or AWS CloudFormation                     5+ years of experience using Cloud networking services like AWS VPC, AWS Transit Gateway, AWS Private Link, Route53, AWS Direct Connect                     5+ years of experience automating generation of VM Images like AWS AMI, VMDK, OVA or OVF                     5+ years of experience with any Linux Operating System                     5+ years of experience with Configuration Management and Application deployment tools like Salt stack or Ansible                         As part of this role, you will be expected to:                         Work with the Engineering Manager to technically drive large scale projects that span multiple quarters.                     Maintain and update existing AWS resources with CloudFormation Templates                     Ensure the network in AWS and on-prem is setup correctly and securely and users and services can access services that they must.                     Automate the setup, upgrade and maintenance of our CI/CD services like Jenkins, Nexus, Sonarqube, Ivy, or Bazel-proxy.                     Automate the monitoring for all our CI/CD services so we can get alarms and proactively fix issues before our developers get effected.                     Engage with development teams to understand, define and meet their products CI/CD requirements                     Engage with development teams to understand where the developer-experience needs improving and suggest solutions for them.                     Develop and maintain documentation and diagrams outlining automated solutions and build architecture                     Maintain and update existing on-prem configuration management code-base written using Saltstack.                     Create and maintain our internal package repositories for python, node, java, and docker                     Identify bottlenecks and work on improving our release processes to make releases smoother and robust             ",2.31E+11,23-11-2023,21-02-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Consumer Electronics & Appliances,"Content management, Linux, HP data protector, Architecture, Configuration management, jenkins, AWS, Monitoring, Recruitment, Python",-,9am-6pm,"Full Time, Permanent","Evertz Microsystems, Ltd",Organization,"Evertz Microsystems, Ltd",https://img.naukimg.com/logo_images/groups/v1/4730343.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Machine Learning Engineer,"         We are seeking a talented and passionate Azure Machine Learning Engineer to join our team and play a key role in implementing a petabyte-scale data warehouse storage solution using Azure Fabric        You will work closely with data scientists and engineers to leverage state-of-the-art machine learning techniques to unlock valuable insights from massive datasets                 Responsibilities:              Design and implement machine learning pipelines for anomaly detection, prediction, and other relevant tasks using Azure Machine Learning services.          Develop and train machine learning models using large datasets stored in petabyte-scale data warehouses on Azure Fabric.          Perform data pre-processing, feature engineering, and model evaluation using tools like PySpark, Scala, and Pandas.          Deploy and manage machine learning models in production using Azure Machine Learning deployment options.          Collaborate with data scientists and engineers to understand business needs and translate them into effective machine learning solutions.          Monitor and evaluate model performance, continuously iterating and improving models as needed.          Stay up-to-date with the latest advancements in machine learning and cloud technologies.                  Qualifications:              Minimum of 4+ years of experience as an Azure Machine Learning Engineer or similar role.          Proven experience in developing and deploying machine learning models using tools like PySpark, Scala, and Pandas.          Strong understanding of machine learning concepts, including data pre-processing, feature engineering, model training, evaluation, and deployment.          Experience working with large-scale datasets and distributed computing frameworks.          Experience with cloud platforms like Azure, particularly Azure Machine Learning and Azure Fabric.          Excellent communication and collaboration skills.          Ability to work independently and as part of a team.          Passion for learning and staying up-to-date with the latest advancements in technology.                      Bonus Points:              Experience with anomaly detection and prediction techniques.          Experience with building data pipelines for machine learning.          Knowledge of cloud security best practices.        ",2.20E+11,22-02-2024,22-05-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Procurement, cloud security, Machine learning, SCALA, Manager Technology, Healthcare, Deployment, Oracle, Data warehousing, Software solutions",-,9am-6pm,"Full Time, Permanent",Atlas Systems,Organization,Atlas Systems,https://img.naukimg.com/logo_images/groups/v1/63980.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Cloud Engineer,"   IDESLABS PRIVATE LIMITED is looking for Senior Cloud Engineer to join our dynamic team and embark on a rewarding career journey     Lead the design, implementation, and maintenance of cloud-based solutions for our clients, with a focus on scalability and security     Collaborate with our clients and internal teams to develop cloud-based architectures that meet their business needs and requirements     Develop and implement automation tools and processes for deploying and managing cloud-based infrastructure and applications     Develop and maintain cloud security and compliance policies and procedures     Implement monitoring and logging solutions to ensure the availability and performance of cloud-based infrastructure and applications     Provide technical guidance and mentoring to junior cloud engineers and other team members     Experience with implementing and managing cloud security solutions, such as firewalls, intrusion detection/prevention systems, and access controls     Excellent communication and interpersonal skills ",2.20E+11,22-01-2024,21-04-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Interpersonal skills, cloud security, Scalability, Compliance, Intrusion detection, Automation tools, Infrastructure, Deployment, Management, Monitoring",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
DataBricks Engineer,"   This position will be engaged in design and deploy of large-scale data processing frameworks and pipelines on Databricks to support enterprise-wide BI and data science initiatives. It will also provide user training and data coaching for effective Databricks usage.       Your Responsibilities           Create MLOps frameworks for model development, training, deployment, and monitoring.         Optimize Databricks clusters for performance, scalability, and resource efficiency.         Monitor cluster health, performance metrics, and resource usage, adjusting cluster sizes as needed.         Design efficient data processing systems and pipelines using Databricks, APIs, and other cloud services.         Act as the Databricks expert and uphold platform standards.         Collaborate with Data and Platform architects on defining the product roadmap.         Who we are looking for           Bachelors degree in Computer Science or related field.         7+ years of IT experience, including 5+ years in Databricks development and administration, Spark, and cloud platforms (preferably Azure).         Proficiency in Databricks admin and development, with expertise in Resource and Performance Management.         Expertise in test automation, continuous integration, Docker, GitHub, Jenkins, and Unix/Linux.         Leadership in large project initiatives, willingness to mentor, and support for team members from different time zones.         Your preferred qualifications           Experience with Kafka and Spark, and strong in Python and Pyspark.         Hands-on experience in machine learning pipelines and scripting languages.         Familiarity with Java, Python, SQL/NoSQL databases, and Big Data tools.         Passion for new tech, continuous improvement, and collaborative problem-solving.         Proactive, strong work ethic, and team player.     ",1.90E+11,19-01-2024,18-04-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,Automobile,"Unix, Computer science, Linux, Social media, Machine learning, Analytics, Monitoring, Automotive, SQL, Python",-,9am-6pm,"Full Time, Permanent",MAGNA,Organization,MAGNA,https://img.naukimg.com/logo_images/groups/v1/4824977.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Backend Distributed System Engineer - SMTS," This is a great opportunity for ambitious engineers who want technical growth in distributed systems development, data processing, as well as security. Your talent will find significant outreach and business impact, contributing to the CSO s security goals.       Here's what you'll do         Design and implement highly available, resilient, and scalable distributed systems to continuously determine cloud misconfigurations and drift across Salesforce s public cloud resources.     Build observability and remediation services to help Salesforce s engineers, leaders, and internal customers to determine security risk and act on remediating the misconfigurations.     Operate in an fast paced - agile development environment, including participating in daily scrums.     Own and deliver initiatives adding new features to meet ever-growing product demands.     Support the team s engineering and operational excellence by participating in architecture/code/operational reviews, identifying and fixing issues.     Work cross-functionally with product management and other partner teams to complete large-scale projects with impact across the company.     Adapt to change quickly and eagerly: changing requirements, changing priorities, changing strategies.             Required Skills         Industry experience of 4+ years with a M.Sc/B.E degree.     4+ years of deep understanding of object-oriented programming and profeciency with at least one object-oriented programming language (Java, Go, Python) - Python & GoLang preferred.     Experience building highly performant, highly-available (99.99%), and highly fault-tolerant, large-scale distributed systems using native public cloud stack..     Experience building ETL pipelines using frameworks like Apache Airflow, Kafka, Spark etc     Experience with building systems using services on AWS.     Strong knowledge and experience working with SQL and NoSql Databases.     Demonstrated experience building services with Docker and Kubernetes.     Experience with Scrum or other agile development methodologies, with attention to code quality, and delivering secure code.     Self-starter, independent, and driven with the required emotional intelligence to work in an environment that values team success, collaboration, and business outcomes.     Excellent oral and written communication.             Nice-to-Have Skills             Knowledge or experience of AI/ML.     Experience with building systems using services on Google Cloud Platform (GCP) or Azure     Strong knowledge of security fundamentals including, authentication/authorization frameworks (eg, SSO, SAML, OAuth, etc), secure transport (eg, TLS), and identity management (eg, certificates, PKI) is nice to have.     Deep knowledge of observability frameworks like Splunk, Graffana, Prometheus   ",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Product management, Backend, Data processing, Scrum, Apache, Distribution system, SQL, Python, Salesforce, Identity management",-,9am-6pm,"Full Time, Permanent",Tableau Software,Organization,Tableau Software,https://img.naukimg.com/logo_images/groups/v1/1419080.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Cloud Engineer,"   Endivite Technologies is looking for Azure Cloud Engineer to join our dynamic team and embark on a rewarding career journey              Design, develop, and deploy modular cloud-based systems              Develop and maintain cloud solutions in accordance with best practices              Ensure efficient functioning of data storage and process functions in accordance with company security policies and best practices in cloud security              Identify, analyze, and resolve infrastructure vulnerabilities and application deployment issues              Regularly review existing systems and make recommendations for improvements              Interact with clients, provide cloud support, and make recommendations based on client needs              Troubleshooting and analytical skills8      Strong communication and collaboration skills        ",1.50E+11,15-03-2024,13-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Electronic Components / Semiconductors,"kubernetes, cloud security, vmware, microsoft azure, azure cloud, cloud support, docker, ansible, cloud administration, git, active directory, saas, gcp, devops, linux, paas, powershell, jenkins, terraform, iaas, aws, cloud computing, communication skills",-,9am-6pm,"Full Time, Permanent",Endivite Technologies,Organization,Endivite Technologies,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Architect/Engineer,"   Global Pharma Tek is looking for Azure Architect/Engineer to join our dynamic team and embark on a rewarding career journey     Responsible for designing and implementing solutions on the Microsoft Azure cloud platform     Assessing clients' current infrastructure and making recommendations for migration to Azure     Designing and implementing secure and scalable cloud solutions on Azure     Managing and automating infrastructure deployment through Azure DevOps     Providing guidance and best practices for cloud architecture and security     Collaborating with development teams to ensure that applications are optimized for Azure     Monitoring and optimizing performance of Azure solutions to ensure high availability     Staying current with new Azure services and features, and incorporating them into solutions as appropriate     Providing technical support and troubleshooting for Azure-based solutions     Developing and maintaining documentation of Azure solutions and processes     Good understanding of cloud security and networking     Strong communication and leadership skills are also essential, as the Azure Architect will be working closely with development teams and clients   ",1.50E+11,15-02-2024,15-05-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Architect, PAAS, Infrastructure, Manager Technology, power bi, microsoft azure, Data analytics, Management, SQL, Python",-,9am-6pm,"Full Time, Permanent",Global Pharma Tek,Organization,Global Pharma Tek,https://img.naukimg.com/logo_images/groups/v1/1012164.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Machine Learning Engineer and MLOP/Cloud engineer,"         MLE - 2                     Python (Pyspark not required), OOPS experience needed                 refactoring expertise                  Azure Stack familiarity and experience (Azure Cloud)                 Azure ML and ADB are potential tech stack we will explore                 MLOPs experience                 Git and Agile experience                 Minimal Data science experience expected                               MLOP/Cloud engineer                       identifying the best tech solution, cost and benefit analysis                 migrating and building pipelines for getting the solution to cloud                 refactoring expertise                  ? ",1.10E+11,11-01-2024,10-04-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Internet,"GIT, data science, OOPS, devops, Machine learning, Cloud, Agile, Cost, Python",-,9am-6pm,"Full Time, Permanent",Conneqt,Organization,Conneqt,https://img.naukimg.com/logo_images/groups/v1/257614.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Associate CD," 1. Complete knowledge of Plastic / Sheet metal parts development and costing     2. Preferrable from auto lighting industry. Worked in vendor development or sourcing role     3. Has to do manufacturing feasibility of drawings.     4. Negotiate tool costing     5. Tool feasibility sign off between supplier and RD     6. Multiple tool development programme management at supplier end.     7. Technical assistance to supplier for any development issue     8. Costing of Plastic, sheetmetal, casting parts of 4 wheeler     9. Understanding of PPAP/PSW and conduct audit at suppliers end. ",1.01E+11,10-10-2023,08-01-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Engineering & Construction,"Costing, Program management, Sheet metal, Vendor Development, PPAP, Auditing",-,9am-6pm,"Full Time, Permanent",Varroc,Organization,Varroc,https://img.naukimg.com/logo_images/groups/v1/4774715.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Engineer,"   Be responsible for designing and developing backend software, primarily for the industrial test data applications. The engineer will engage directly with global customers.      Interface with customers to understand user requirements, generate specifications, design, develop, validate and deliver the software for production test applications    Preferred Skills/Experience      Ability to work in AWS services from Compute, Storage, Networking and Security components          Having deep knowledge of VPC configuring, Private/Public Subnet, NAT Gateway, ELB with EC2, Lambda, S3, EBS, EFS, Glacier, RDS, SQS, SNS, Cloudwatch and Cloud Formation          Hands-on experience with using Cloud Platform provided Big Data technologies (i.e. EC2, EMR, RedShift, S3, Kinesis)          Knowledge in Big Data technologies Hadoop, Pig, Hive and Spark          Strong analytical, problem-solving, data analysis and research skills          Experience in experience of Java (core java) and J2EE technologies, Bash Scripts          Experience in migrating workload from on-premise to cloud and cloud to cloud migrations          Experience in programming of MapReduce, Spark is a plus      Desired Profile      Any Bachelor s degree          Excellent design / debugging abilities          Good understanding of software design principles          Excellent communication and presentation skills          Must be detail-oriented; be able to multi-task and manage multiple projects      ",90224501446,09-02-2024,09-05-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Industrial Equipment / Machinery,"NAT, Core Java, Data analysis, Software design, Backend, Networking, Analytical, Debugging, Cloud, J2Ee",-,9am-6pm,"Full Time, Permanent",Etest Automations,Organization,Etest Automations,https://img.naukimg.com/logo_images/groups/v1/6684017.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Coupa Platform Engineer,"   Manage daily administration and general monitoring to keep the Coupa within their portfolio running well.         Provide support for, and timely resolution of, escalated issues on/with, those Coupa.         Support daily administration of Coupa applications, including health monitoring, log inspection, capacity planning, and performance tuning. Automation of many of these tasks is strongly encouraged.     Perform platform specific assignments related to mobile/kiosk platform management     Monitor incident queues dedicated for Coupa, responding to tickets by severity and in accordance with agreed-upon SLAs.     Maintain a roadmap of Coupa future state, inclusive of planned integrations, features, or enhancements.     Create and/or maintain application-specific documentation including data flow diagrams, user training materials, data dictionaries, etc.           Required Experience and Skills           Support experience with mobile ordering platforms and user facing kiosk technology     Deep application knowledge of one finance applications Infor SUN, Coupa     Experience in Inventory systems (Dataworks), Hospitality systems (Oracle Opera), Sales systems (Oracle Simphony), is a added advantage     Excellent communication, organization, and presentation skills           Other Desired Experience           Integration experience with EDI or cXML     Hospitality experience a strong plus   ",80923500086,08-09-2023,07-12-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Procurement, Performance tuning, Hospitality, Automation, Manager Technology, Healthcare, EDI, Oracle, Monitoring, Capacity planning",-,9am-6pm,"Full Time, Permanent",Atlas Systems,Organization,Atlas Systems,https://img.naukimg.com/logo_images/groups/v1/63980.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer,"             5 to 10 years of experience leading large scale migration projects like on-prem to AWS, Jenkins to Jenkins, Major Salt stack version upgrade, Linux OS upgrade                    5 to 10 years of experience using Infrastructure as Code tools like Terraform or AWS CloudFormation                5+ years of experience using Cloud networking services like AWS VPC, AWS Transit Gateway, AWS Private Link, Route53, AWS Direct Connect                5+ years of experience automating generation of VM Images like AWS AMI, VMDK, OVA or OVF          5+ years of experience with any Linux Operating System          5+ years of experience with Configuration Management and Application deployment tools like Salt stack or Ansible          As part of this role, you will be expected to:        Work with the Engineering Manager to technically drive large scale projects that span multiple quarters.                Maintain and update existing AWS resources with CloudFormation Templates            Ensure the network in AWS and on-prem is setup correctly and securely and users and services can access services that they must.                Automate the setup, upgrade and maintenance of our CI/CD services like Jenkins, Nexus, Sonarqube, Ivy, or Bazel-proxy.                            Automate the monitoring for all our CI/CD services so we can get alarms and proactively fix issues before our developers get effected.              Engage with development teams to understand, define and meet their products CI/CD requirements                    Engage with development teams to understand where the developer-experience needs improving and suggest solutions for them.                        Develop and maintain documentation and diagrams outlining automated solutions and build architecture                  Maintain and update existing on-prem configuration management code-base written using Saltstack.          Create and maintain our internal package repositories for python, node, java, and docker                      Identify bottlenecks and work on improving our release processes to make releases smoother and robust          ",80424500926,08-04-2024,07-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Consumer Electronics & Appliances,"Application deployment, Content management, Linux, Architecture, Configuration management, jenkins, network services, AWS, Monitoring, Python",-,9am-6pm,"Full Time, Permanent","Evertz Microsystems, Ltd",Organization,"Evertz Microsystems, Ltd",https://img.naukimg.com/logo_images/groups/v1/4730343.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Pyspark,"         Experience in Cloud platform, e.g., AWS, GCP, Azure, etc.                  Experience in distributed technology tools, viz. SQL, Spark, Python, PySpark, Scala                  Performance Turing - Optimize SQL, PySpark for performance                  Airflow workflow scheduling tool for creating data pipelines                  GitHub source control tool & experience with creating/ configuring Jenkins pipeline              ",71223500384,07-12-2023,06-03-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Payroll, github, GCP, spark, jenkins, Manager Technology, Workflow, Scheduling, SQL, Python",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"   We are looking for Data Engineers, a person who loves solving standardization, has strong platform thinking, opinions, and has solved for Data Engineering, Data Science and analytics platforms.             What You ll Do:               Solely own your product; Build the product from scratch     Collaborate with the business team to understand business requirements and define tech roadmap     Working on complex architectural problems     Have an understanding of loopholes in current system/architecture which can potentially break in future and push towards solving them with other stakeholders.         What we need:               Bachelors/masters in computer science.     Experience with Data pipeline and workflow management tools like Luigi, Airflow etc.     Proven ability to work in a fast paced environment.     Fanatic about building scalable, reliable data products.     Should be hand-on and well versed with Python.     Experience with Big data tools: Hadoop, Kafka/Kinesis, Flume, etc. and knowledge of Scala is an added advantage.     Experience with Relational SQL and NO SQL databases like HBase, Cassandra etc.     Experience with stream processing engines like Spark, Link, Storm, etc. is an added advantage.         What you can expect:               A great work environment with massive ownership and growth opportunities     Work closely with the founding and the leadership team on key projects     Opportunity to explore latest tools and technology to get things done.                     Our Tech-Stack:               JavaScript     React.js     Flutter     NodeJs     MongoDB     Docker     Amazon EC2     Amazon ECS     Amazon Redshift     BigQuery     Data Flow     BigTable     Amazon RDS     SQS     SNS     Lambda     Elasticsearch     Redis     ",51223500849,05-12-2023,04-03-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,Retail,"Computer science, System architecture, Architecture, cassandra, Javascript, MongoDB, Analytics, SQL, Python, HBase",-,9am-6pm,"Full Time, Permanent",Shopdeck,Organization,Shopdeck,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Support Analyst,"   FOX is looking for a dynamic and motivated Data Ops Support Analyst to join our BI, Analytics and Data Strategy team. If you are customer - focused with creative problem solving skills and have experience with DBT, Airflow, Databricks, Snowflake etc. then you may be the best fit for this role. As a member of our team, you will have the opportunity to advance your career while supporting FOX s unique corporate service & BI platforms and employees. This is a great opportunity to leverage your communication skills, technical, analytical and problem solving competencies and most importantly, customer focused experience as the next step in your career.             A SNAPSHOT OF YOUR RESPONSIBILITIES           Core responsibility will be to efficiently collaborate across data Tech teams and help develop solutions by rapidly delivering value to our customers     Focus on data quality and consistency, manage processes and systems to monitor data quality, ensuring production data is accurate and available to Business key stakeholders within agreed upon SLA s     Manage & build distributed, low latency, reliable data pipelines ensuring high availability & timely delivery of data     Focus on best practices and agreed upon solution design under data governance policies across Data platform     Adhere to data security and privacy by following proper access controls, key management and encryption techniques                WHAT YOU WILL NEED             Education:    Bachelors degree in Computer Science or Applications, Information Systems, or related field       Work Experience :    3+ years of experience working with Modern Tech stacks such as Linux, iOS, Ruby, Python, AWS, Apache, MongoDB, Redshift, Snowflake, Fivetran and API services etc.     Proficient in advanced operating systems and programming languages to optimize performance using    Linux / iOS / JavaScript / Python / Ruby       Experience working with Business Intelligence Platforms such as Looker, Tableau     Experience working with data acquisition and transformation tools such as FiveTran and DBT     Experience working with optimized & efficient Data loader ETL pipelines using Databricks, Snowflake etc.      Experience working with fully automated workflow scheduling and orchestration services such as Apache Airflow,      Experience working with any Data Observability/Data Quality platform such as Monte Carlo, Acceldata etc.     Experience working with AWS stack using container and serverless technologies     Experience deploying CI/CD pipelines using Github or Jenkins      Excellent communication skills and ability to work collaboratively with cross-functional teams     Strong analytical and technical skills, attention to detail, and problem-solving skills     ",41223500759,04-12-2023,03-03-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Events / Live Entertainment,"Computer science, Linux, Javascript, Workflow, Scheduling, Apache, Business intelligence, Ruby, Analytics, Python",-,9am-6pm,"Full Time, Permanent",fox Corporation,Organization,fox Corporation,https://img.naukimg.com/logo_images/groups/v1/6564449.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"     The      Data      Platform      Engineer for the      Scipher      platform      is responsible for      expanding and      optimizing      our data and data pipeline architecture,      optimizing      data flow and collection for the product development team.      This Data      Engineer will      support our      data analysts and data scientists on data initiative      s and will ensure      optimal      delivery      of      architecture consistency across projects.                          The ideal candidate is      experienced in data pipeline builds and data wrangling.      self-directed and      confident      supporting the data needs of multiple products features      . They      are energized      by the prospect of      optimizing      or even re-designing our company s data architecture to support our next generation of products and data initiatives.                                        Essential Functions & Key Responsibilities:                              Develop      s      and deliver      s      solutions across      multi      ple      environment      s      using          Continuous Integration and Continuous      Delivery/      Deployment          (      CI/CD      )      tool chain      so that      data pipeline      s are      rigorously tested before      deployment and availability      to users.                  Creates &      maintains      the data platform blueprint and design, encompassing the relevant data platform components      to      enable seamless data management and empower the development teams to meet their end-to-end data needs                  Takes ownership of the code and troubleshoots production issues with operations team.                  Collaborate      s      with      data      engineers      and architects      during      solution reviews and design discussions      to ensure cohesive integration between systems and data models.                  Own      s      Documentation and      C      ommunication      connected      to features developed      on the platform with the cross functional teams such as Data Engineers, Data Scientists, Architects      .                      Work      s      with QA and      the      Utopus      Insights      O      perations team to support and troubleshoot workloads deployed to customer facing environments.                              Required Experience:                              6-      8 years      Overall work experience, preferably in product development                                  3-5 years work experience in Systems or application development                  Proven      e      xpertise      in      debugging and performing root cause analysis on large data platforms.                  Motivation to learn      new technologies      and explore areas outside of      expertise      .                  Proven capability &      track record      of normalizing unstructured datasets                  Ability to build      Extract, Transform & Lo      a      d (      ETL      )      pipelines with multiple data structures, formats, sources, and dependencies.                              Education      Requirements                              Required bachelors degree in computer programming, computer science, or related field.                              Technology Skills:                    Required                    3-5 Years      Expertise in Programming      Languages: Either Python      or      Scala      or      Java      and      additionally      SQL Required                  Data Processing Frameworks: Apache      Spark      or      Hadoop                  Proficiency      in      AWS (Amazon Web Services)      Cloud technologies (      AWS Lambda, Kinesis Streams      &      IAM      - all 3      Required      )                      Database Technologies:      MySQL/PostgreSQL/MongoDB                            Good to have.                    Exposure to      IOT (internet of things)      and timeseries databases (      O      penTSDB      /      TimescaleDB      /Timestream      )                  Working knowledge on      Data Warehousing: Amazon Redshift      /      Snowflake                      Working experience of using Kafka/      NiFi                  Advanced skills in      API development and      D      esign      E      xperience.        ",20823501543,02-08-2023,31-10-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Power,"Health insurance, Data management, Renewable energy, MySQL, Data processing, Genetics, Application development, SQL, Data architecture",-,9am-6pm,"Full Time, Permanent",Utopus Insights,Organization,Utopus Insights,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
SRE - Software Engineer - Observability,"   SRE Software Engineer will be a part of the Engineering team and will require a strong knowledge of application monitoring,  infrastructure monitoring,  automation,  maintenance,  and Service Reliability Improvements.   Specifically,  we are searching for someone who brings fresh ideas,  demonstrates a unique and informed viewpoint,  and enjoys collaborating with a cross-functional team to develop real-world solutions and positive user experiences at every interaction.             Proven proficiency in Observability,  monitoring,  and logging tools Like   Dynatrace,        Prometheus,  Grafana,  ELK stack and       Splunk.             3+ years of experience in Dynatrace,  synthetic URL monitoring,  installing agents,  forwarders,  APIs,  performance monitoring tool alerts,  dashboards and data trend analysis in a monitoring tool.           Should have a minimum 4 to 5 years' working experience in   OpenShift   and Docker/K8s          Proficiency in implementing monitoring and observability solutions using   GCP monitoring services such as Cloud Monitoring,  Logging,  and Tracing            Deep understanding of IT infrastructure monitoring and observability best practices          Experience with gathering and organizing large amounts of data to use for instrumentation into an Enterprise monitoring solution.           Experience with recommending baseline monitoring thresholds and   performance monitoring KPIs and SLAs            Experience of at least 4 + years of experience in development of   Grafana Dashboards ,  develop Metrics / monitoring Standardization   - Metrics,  collection,  Dashboards with Grafana a must.           3-5 years of experience with SQL and     familiarity with at least one managed Kubernetes platforms (EKS,  AKS,    GKE  )          Strong background in software engineering,  with expertise in relevant programming languages (like Python,  Java,  Go) and cloud platforms (like   AWS,  GCP,  Azure  )              The Monitoring and Observability engineer will be responsible for Designing,  configuring,  monitoring,  implementing,  and maintaining our observability solutions and troubleshooting IT systems and applications to ensure optimal performance and reliability.           They will be utilizing Observability and Monitoring tools to detect and resolves issues effecting positive user experience.           The engineer will also be responsible for automating alerting and remediation processes to   reduce mean time to resolution (MTTR) and improve system uptime.             Splunk query language and Monitored Database Connection Health by using Splunk DB connect health dashboards,  log parsing,  complex Splunk searches,  including external table lookups,  Splunk data flow,  components,  features,  and product capability.             Observability: Implement comprehensive monitoring and alerting solutions using GCP monitoring services and external services            Configure dashboards,  alerts,  and notifications to ensure timely identification and resolution of issues.           Troubleshoot issues and outages,  working closely with development and operations teams to identify root causes and develop solution      ",141000000000.0,14-05-2024,12-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Auto Components,"remediation, Automation, GCP, Instrumentation, splunk, Troubleshooting, application monitoring, Monitoring, SQL, Python",-,9am-6pm,"Full Time, Permanent",Ford,Organization,Ford,https://img.naukimg.com/logo_images/groups/v1/247012.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 yrs of full time education Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using DevOps. Your typical day will involve collaborating with cross-functional teams, developing and deploying applications, and ensuring their smooth functioning.  Roles & Responsibilities: Lead the design, development, and deployment of applications using DevOps methodologies. Collaborate with cross-functional teams to identify and analyze business requirements and translate them into technical specifications. Ensure the smooth functioning of applications by monitoring and troubleshooting issues, and implementing necessary fixes. Develop and maintain documentation related to application design, development, and deployment. Stay updated with the latest advancements in DevOps and related technologies, and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience in DevOps methodologies. Must To Have Skills:Experience in application design, development, and deployment. Good To Have Skills:Experience with containerization technologies such as Docker and Kubernetes. Good To Have Skills:Experience with cloud platforms such as AWS, Azure, or Google Cloud. Good To Have Skills:Experience with automation tools such as Ansible, Chef, or Puppet. Additional Information: The candidate should have a minimum of 5 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful solutions. This position is based at our Bengaluru office. Qualification 15 yrs of full time education",141000000000.0,14-05-2024,12-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"application design, microsoft azure, docker, devops, aws, kubernetes, python, javascript, ansible, application development, sql, puppet, business requirement analysis, automation tools, java, gcp, troubleshooting, technical specifications, mysql",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Mlops Engineer,"Title: SENIOR MLOps/DevOps ENGINEER Location: Bangalore, India Juniper Experience and Operations (JXO) organization is incubating a Technology group to transform CX(customer experience) and EX(employee experience). The charter of this group is to discover, evaluate and leverage technology to enhance and simplify the experience of stakeholders. Knowledge and data will be central to this journey of creating a proactive and predictive support experience. The use of automation, AI and other modern technology will enable reduction of time taken to resolve issues or perform tasks. This role is part of Junipers strategic future of support team and will involve design of automated solutions. The role requires enabling business transformation projects using technology, review of data to enable process, systems and tool re-engineering in customer support and services. In addition, the role requires to support the enhancement of self-service, automation, omnichannel strategy by seeking solutions and drivers to achieve seamless customer experiences and increased customer loyalty. Primary Tech skills : MLOPs in Databricks, AWS, Python, Docker, Kubernetes, Terraform, Ansible, Prometheus, Grafana, ELK.   Secondary Tech skills : Snowflake, Data Engineering using PySpark   Responsibilities:   Integrate, deploy and maintain key Data Engineering and ML workflows using Databricks and AWS to ensure seamless data movement from raw source to final consumption. Manage the end to end lifecycle DevOps, DataOps and ModelOps. Involve in troubleshooting and resolving integration and data-related issues.   Set up Databricks Repos to integrate with Git and sync notebooks and source code with Databricks workspaces. Use features in Databricks  for Git integration and version control.  Create and manage CICD pipeline for smooth deployment of codes and workflows between development to stage to production environment.  Create and manage clusters by setting up access policies as needed by data engineers and data scientists.  Use Databricks APIs to automate and create reports on Feature Store and MLflow usage and behaviour. Create and manage access controls for raw and feature tables stored in Delta tables.  Enable integration between Databricks and AWS S3 buckets by setting up right IAM policies. Set-up optimised S3 Lifecycles for data retention and storage.  Enable monitoring of Data Engineering job workflows and automate job failures notification and fallback solutions. Optimise and suggest best practices for usage of clusters and data storage.  Enable ingestion of streaming data into delta live tables to support realtime time based Anomaly detection models. Use Databricks MLflow to track model development, registry and deployment and save model artifacts like code snapshots, model parameters, metrics and other metadata.  Use Unity Catalog to manage data and model versioning, governance and deployment. Build model drift pipelines and monitor model performance over time and enable workflows to retrain drifted models.  Build process to automate model movement from Dev to Stage to Production and enabling A/B testing of different model versions. Experience in containerization technologies and orchestration frameworks , understanding of microservices architecture and deployment patterns. Creating Kubernetes cluster and deploying application on top of it via package manager tools like helm. Implement security best practices and ensure compliance with industry standards. Collaborate with development teams to optimize application performance and scalability. Stay updated with emerging technologies and industry trends, and evaluate their potential impact on our infrastructure and development processes. Qualification and Desired Experiences:   7+ years of SRE experience in building CICD pipelines, managing and owning code movements between environments, creating access controls to objects for users based on environments and roles. 3+ years of relevant MLOps experience in Databricks : involving GIT integration,  setting up Access Controls in Databricks and AWS S3,  setting up CICD pipeline for code\model deployment between environments,  manage/maintain/monitor data pipelines and ML model performance. Bachelors's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.  Experience with big data tools: Spark, Kafka, Spark & Kafka Streaming, Python, and Snowflake  Working knowledge of Databricks API to automate process. Experience with Databricks Unity Catalog. Experience with Databricks Feature Store and Delta Live Tables Experience with Databricks MLFlow, model registry and creating endpoints for inference for realtime\batch\streaming applications. Experience with AWS S3 for data storage, creating S3 Lifecycles for data storage and retention.   Personal Skills: Ability to collaborate cross-functionally in a fast-paced environment and build sound working relationships within all levels of the organization Ability to handle sensitive information with keen attention to detail and accuracy. Passion for data handling ethics. Ability to solve complex, technical problems with creative solutions while anticipating stakeholder needs and providing assistance to meet or exceed expectations Able to demonstrate perseverance and resilience to overcome obstacles when presented with a complex problem.  Assist in combining large data sets and data analysis to create optimization strategies Comfortable with ambiguity and uncertainty of change when assessing needs for stakeholders Have effective time management skills which enable you to work successfully across functions in a dynamic and solution-oriented environment while meeting deadlines Self-motivated and innovative; confident when working independently, but an excellent team player with a growth-oriented personality Will be required to routinely or customarily troubleshoot items related to applications that require independent judgement, decision-making, and unique approaches   Juniper is an Equal Opportunity workplace and Affirmative Action employer. We do not discriminate in employment decisions on the basis of race, color, religion, gender (including pregnancy), national origin, political affiliation, sexual orientation, gender identity or expression, marital status, disability, genetic information, age, veteran status, or any other applicable legally protected characteristic. All employment decisions are made on the basis of individual qualifications, merit, and business need.   We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.    ",141000000000.0,14-05-2024,12-08-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Databricks, Mlops, Aws",-,9am-6pm,"Full Time, Permanent",Naukri Premium - Employer Services,Organization,Naukri Premium - Employer Services,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"   Hiring for Client  Job Title: Data Platform / AIOPS Engineer Job Type: Contract  Job Location: India remote Okay to use own laptop  Apply -nidhi@intersourcesinc.com Job Responsibilities: Contributing member of a high performing, agile team focused on generative Artificial Intelligence/ Client in Data & Analytics. Assist Data Engineering and Generative AI teams during design and development for highly complex and critical Generative AI projects. Responsible for Setup and performance tuning Transformers. RAG production setup/deployments. Responsible for setup/maintenance and optimization of language models using PyTorch, Tensorflow, Streamlit and other relevant frameworks. Responsible for setup/maintain advanced tools like Semantic Kernel, Langchain, and others for efficient AI model management. Code infrastructure and integrate Platform solutions into the data analytic ecosystem. Be part of teams delivering all data projects including implementing new data technologies for unstructured, streaming, and high-volume data. Developing and deploying distributed computing Big Data applications using frameworks like Apache Spark and Kafka. Utilizing programming languages like Java, Spark, Python, with an emphasis in tuning, optimization, and best practices for Data Engineers. Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test-Driven Development to enable the rapid delivery of end user capabilities. Developing data solutions on cloud deployments such as AWS Industry experience in Financial Services or other regulated industries and compliance and audit practice. Hands on experience leading delivery through Agile methodologies Management practices by adhering to required standards and processes. Works with Architects to design complex solutions and lead from inception to production. Creates and maintains DevOps processes, application infrastructure, and utilizes cloud services (including systems, network and other artifacts) Reduces technical debt over time with root cause identification and resolution of system problems Innovates on and advocates for best practices and improved team processes, mentors junior team members Supports live systems to ensure business continuity. Role Description: The Data Platform / AIOPS Engineer will have responsibility for working on a variety of data projects. This includes orchestrating pipelines to stage high quality data for consumption by business analysts, data scientists and visualization developers using modern Big Data tools/architectures This position requires the candidate to be a team player and results oriented. High sense of integrity, good communication skills, great attention to detail and hands on approach with next generation technologies to contribute to a team that delivers the latest streaming & next generation data technologies. Job Experience Requirements: Bachelor's degree in the areas of Computer Science, Engineering, Information Systems, Business, or equivalent field of study required. 10+ years of experience as AIOps engineer with strong background in deploying cutting-edge Client/AI platforms. Heavy experience using Python (TensorFlow, PyTorch) for production-grade work. Commercial experience building Client AI platforms with large datasets. Fluent in AWS with IAM (identity and Access Management), elastic containers (ECS), Kubernetes History of working on models from concept to production / end-to-end Experience in working with data solutions (Data lakes, cloud data warehouses, i.e., Snowflake or Redshift). 5+ years Linux/Unix including basic commands, shell scripting and solution engineering 5+ years of experience with S3, AWS Glue, EMR, MKS, RDS, Serverless technologies, DynamoDB, lambda, Kubernetes, Vector databases and graph databases. 2+ years of experience supporting Data Pipelines using Matillion/Python/Serverless. Experience developing software solutions to build out capabilities on a Big Data and other Enterprise Data Platforms Experience with solving problems and delivering high quality results in a fast-paced environment AWS SysOps certifications and/or Client/AI Ops related certifications preferred.  ",141000000000.0,14-05-2024,12-08-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Tensorflow, AIOPS Engineer, Python",-,9am-6pm,"Full Time, Temporary/Contractual",Intersources Inc,Organization,Intersources Inc,-,"Pune, Bengaluru, Mumbai (All Areas)","Pune, Bengaluru, Mumbai (All Areas)",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Azure Engineer,"4+ yrs in Azure services Proficient in Terraform IaC, with expertise in cloud-native design & troubleshooting Fluent in Python for automation Design scalable Azure solutions,Disaster recovery, security enforcement, Azure governance, &monitoring setup Required Candidate profile Specialized in Azure implementation and management. Proficient in cloud-native database management Technical concept articulation",50424008117.0,15-05-2024,13-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Azure Cloud Services, Python, Azure Infrastructure, Azure Iaas, Azure Devops",-,9am-6pm,"Full Time, Permanent",Numino Labs,Organization,Numino Labs,https://img.naukimg.com/logo_images/groups/v1/4615451.gif,"South Goa, Pune","South Goa, Pune",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Cloud Developer,"Your Role and Responsibilities As a python cloud development engineer you will Design, development and test Power Private Cloud software.  Able to understand the architecture and turn the requirements into high level and low level designs  Active open source contributions and participation and open source tools knowledge is preferred  Software development in Cloud Domain, OpenStack, Virtualization, Linux OS Internals, Networking Storage/ Security Infrastructure as a Service is preferred Understanding of any cloud based virtualization software [GCP, Azure] and Openstack concepts. Work with OpenSource community and contribute towards open source development Testing. Good understanding and hands on with full stack cloud development is preferred.  Responsibilities: IBM India Systems Development Lab is looking for Software Engineer Developer to be integral part of a team responsible for Product Development/testing of PowerVC Platform on Power. The product is built on the Openstack cloud computing platform using Python. In addition, there is an opportunity to work on several OpenSource communities and front end/back-end development as well. Required Technical and Professional Expertise 4+ years of experience in the IT industry 4+ years of experience working as a developer well versed with feature enablement on private/public cloud platforms or in an equivalent role supporting partners and enterprises 4+ years of experience with solutions development or implementation in Unix or Linux environments using python programming language. 2+ years of experience working with Ansible [ Nice to have ]. Proficient in Python programming and scripting experience and Openstack concepts. Experience working in Open Source communities is a big plus. Proven experience architecting, designing, and developing complex customer solutions in a rapidly evolving technology domain Ability to perform customer-facing activities in a fast-paced environment with short timeframes Experience with concepts of Openstack cloud computing platform. Ability to uncover business challenges and develop custom solutions to solve these challenges Working knowledge of object-oriented design and design patterns applicable to modern software development Applied knowledge of working with agile, scrum, and DevOps teams Clear understanding of cloud service and deployment models Comfortable working with highly distributed teams, including interaction with open source communities Ability to study on your own, learn quickly, and put new knowledge into practice Any commercial experience with technologies like Openstack, virtualization and public cloud services, e.g., Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) Ability to quickly learn new technologies, frameworks, and techniques; ability to facilitate technical conversations within your team and with external stakeholders Keen interest contributing to and building communities in open source Ability to work both on your own and as part of an agile team. Preferred Technical and Professional Expertise Hands-on experience on Openstack based clouds, basic concepts of virtualization, strong with Python programming, automation using Ansible.",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"salesforce lightning, service cloud, salesforce crm, salesforce sales cloud, vlocity, public cloud, cloud services, microsoft azure, linux internals, ansible, gcp, design patterns, devops, scrum, aws, unix",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Cloud Developer,"Your Role and Responsibilities As a python cloud development engineer you will Design, development and test Power Private Cloud software.  Able to understand the architecture and turn the requirements into high level and low level designs  Active open source contributions and participation and open source tools knowledge is preferred  Software development in Cloud Domain, OpenStack, Virtualization, Linux OS Internals, Networking Storage/ Security Infrastructure as a Service is preferred Understanding of any cloud based virtualization software [GCP, Azure] and Openstack concepts. Work with OpenSource community and contribute towards open source development Testing. Good understanding and hands on with full stack cloud development is preferred.  Responsibilities: IBM India Systems Development Lab is looking for Software Engineer Developer to be integral part of a team responsible for Product Development/testing of PowerVC Platform on Power. The product is built on the Openstack cloud computing platform using Python. In addition, there is an opportunity to work on several OpenSource communities and front end/back-end development as well. Required Technical and Professional Expertise 4+ years of experience in the IT industry 4+ years of experience working as a developer well versed with feature enablement on private/public cloud platforms or in an equivalent role supporting partners and enterprises 4+ years of experience with solutions development or implementation in Unix or Linux environments using python programming language. 2+ years of experience working with Ansible [ Nice to have ]. Proficient in Python programming and scripting experience and Openstack concepts. Experience working in Open Source communities is a big plus. Proven experience architecting, designing, and developing complex customer solutions in a rapidly evolving technology domain Ability to perform customer-facing activities in a fast-paced environment with short timeframes Experience with concepts of Openstack cloud computing platform. Ability to uncover business challenges and develop custom solutions to solve these challenges Working knowledge of object-oriented design and design patterns applicable to modern software development Applied knowledge of working with agile, scrum, and DevOps teams Clear understanding of cloud service and deployment models Comfortable working with highly distributed teams, including interaction with open source communities Ability to study on your own, learn quickly, and put new knowledge into practice Any commercial experience with technologies like Openstack, virtualization and public cloud services, e.g., Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) Ability to quickly learn new technologies, frameworks, and techniques; ability to facilitate technical conversations within your team and with external stakeholders Keen interest contributing to and building communities in open source Ability to work both on your own and as part of an agile team. Preferred Technical and Professional Expertise Hands-on experience on Openstack based clouds, basic concepts of virtualization, strong with Python programming, automation using Ansible.",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"openstack cloud, python, linux, openstack, agile, public cloud, architecting, cloud services, web services, cloud development, microsoft azure, linux internals, networking, ansible, docker, open source, gcp, design patterns, devops, full stack, jenkins, scrum, aws, unix",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : Java and DB knowledgeManual Automation Minimum  3  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of Agile methodologies and DevOps principles. Good To Have Skills:Experience with cloud platforms such as AWS, Azure, or GCP. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, aws, agile methodology, kubernetes, python, aws cloudformation, ansible, application development, sql, high availability, java, gcp, jenkins, troubleshooting, terraform, bash, mysql, agile, db, application deployment",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : PySpark Good to have skills : Databricks Unified Data Analytics Platform Minimum  3  year(s) of experience is required Educational Qualification : Any technical Graduation Required Skills: Minimum 7 year of Experience in Python programming language  Minimum 2 years of experience in AWS developer using Glue, Lambda, S3, IAM, VPC, EC2, Athena, Cloudwatch, Dynamo and RDS  Understanding of the serverless mindset on architectural solutioning Experience with DevOps & CI/CD tools Jenkins, Cloudbees, Please Build, etc. Proficiency working with large data stores and data sets Deep understanding of database concepts and design for SQL (primarily) and NoSQL (secondarily) -- schema design, optimization, scalability, etc. Solid experience with git software version control and good understanding of code branching strategies and organization for code reuseKey Responsibilities: Drive automation and integrate with CI/CD tools for continuous validation. Drive mentality of building well architected applications for AWS Cloud Identify code defects and work with other developers to address quality issues in product code. Finding bottlenecks and thresholds in existing code through the use of automation tools. Articulate clear business objectives aligned to technical specifications and work in an iterative agile pattern daily. Ownership over your work task, and are comfortable interacting with all levels of the team and raise challenges when necessary.Delivery Responsibilities: Core code production for back, middle and front end applications Deploying and developing AWS cloud applications and services end to end Creating and optimizing infrastructure performance metrics Architecting pilots and proofs-of-concept effort to spur innovation Working in all stages of the development lifecycle Automation of manual data object creation and test cases Ask smart questions, collaborate, team up, take risks, and champion new ideasWilling to work in B shift Qualification Any technical Graduation",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"pyspark, sql, nosql, git, database creation, continuous integration, python, data analytics, glue, version control, ci/cd, dynamo, amazon rds, amazon ec2, iam, lambda expressions, aws cloud, devops, jenkins, cloudify, athena, aws, amazon cloudwatch",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer: AWS Cloud FullStack,"As an Application Developer, you will lead IBM into the future by translating system requirements into the design and development of customized systems in an agile environment. The success of IBM is in your hands as you transform vital business needs into code and drive innovation. Your work will power IBM and its clients globally, collaborating and integrating code into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world's technology leader. Come to IBM and make a global impact! Your Role and Responsibilities Developer leads the cloud application development/deployment for client based on AWS development methodology, tools and best practices. A developer responsibility is to lead the execution of a project by working with a senior level resource on assigned development/deployment activities and design, build, and maintain cloud environments focusing on uptime, access, control, and network security using automation and configuration management tools. Experience level knowledge on AWS Deployment Services, AWS beanstalk, AWS tools & SDK, AWS Cloud9, AWS CodeStar, AWS Command line interface etc and hands on experience on AWS ECS, AWS ECR, AWS EKS, AWS Fargate, AWS Lambda function, Elastic Chache, S3 objects, API Gateway, AWS Cloud Watch and AWS SNS. Required Technical and Professional Expertise Should be AWS certified Developer Experience in Modernizing applications to Container based platform using EKS, ECS, Fargate Should have worked on at least 3 engagements modernizing client applications to Container based solutions. Should be expert in any of the programming languages like Java, .NET, Node .js, Python, Ruby, Angular .js Proven experience on using DevOps tools during Modernization. Solid experience around No-SQL database. Should have used Orchestration engine like Kubernetes, Mesos Preferred Technical and Professional Expertise Creative problem-solving skills and superb communication Skill.",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"amazon cloudwatch, aws lambda, amazon ec2, aws, sql database, kubernetes, api gateway, eks, aws technologies, sql, mesos, java, ecs, amazon sns, aws cloud, ecr, devops, python, aws beanstalk, aws deployment, ruby, application development, angular, node.js, .net, sdk",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Development Engineer,"Project Role : Software Development Engineer Project Role Description : Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.  Must have skills : Microsoft Azure Analytics Services Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : BTech or 15 years of full time Education Summary :As a Software Development Engineer, you will be responsible for analyzing, designing, coding, and testing multiple components of application code using Microsoft Azure Analytics Services. Your typical day will involve working with cross-functional teams, performing maintenance, enhancements, and development work to deliver impactful data-driven solutions.  Roles & Responsibilities: Design, develop, and maintain Azure Analytics Services solutions, including data ingestion, storage, ETL processing, and visualization. Collaborate with cross-functional teams to analyze, design, code, and test multiple components of application code across one or more clients. Perform maintenance, enhancements, and development work to ensure the quality, performance, and scalability of Azure Analytics Services solutions. Utilize strong analytical and problem-solving skills to troubleshoot and resolve complex technical issues related to Azure Analytics Services solutions. Stay updated with the latest advancements in Microsoft Azure Analytics Services, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Strong experience in Microsoft Azure Synapse Analytical service and Azure Data Factory. Good To Have Skills:Experience with other Azure services such as Azure Databricks, Snowflake.  Solid understanding of data warehousing concepts, including data modeling, ETL, and data visualization. Experience with programming languages such as Python, SQL, and R. Strong understanding of cloud computing concepts and architectures. Experience with Agile methodologies and DevOps practices.Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure Analytics Services. The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Gurugram office.",1.51E+11,15-05-2024,13-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"analytics services, azure analytics, azure data factory, data warehousing concepts, software engineering, python, azure synapse, microsoft azure, data warehousing, sql, r, data modeling, devops, data visualization, etl, cloud computing, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.51E+11,15-05-2024,13-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, gcp, bash, aws, agile methodology, kubernetes, aws cloudformation, ansible, application development, jenkins, troubleshooting, agile, application deployment",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.51E+11,15-05-2024,13-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"python, gcp, bash, aws, agile methodology, kubernetes, aws cloudformation, ansible, application development, jenkins, troubleshooting, agile, application deployment",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, terraform, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, bash, agile, aws, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, aws, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, terraform, bash, agile, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of Agile methodologies and DevOps principles. Good To Have Skills:Experience with cloud platforms such as AWS, Azure, or GCP. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, aws, agile methodology, kubernetes, python, aws cloudformation, ansible, application development, sql, high availability, java, gcp, jenkins, troubleshooting, terraform, bash, mysql, html, agile, application deployment",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, terraform, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, bash, agile, aws, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  12  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,144,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, aws, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, terraform, bash, agile, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, terraform, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, bash, agile, aws, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, terraform, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, bash, agile, aws, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, aws, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, terraform, bash, agile, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of Agile methodologies and DevOps principles. Good To Have Skills:Experience with cloud platforms such as AWS, Azure, or GCP. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, aws, agile methodology, kubernetes, python, aws cloudformation, ansible, application development, sql, high availability, java, gcp, jenkins, troubleshooting, terraform, bash, mysql, html, agile, application deployment",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  12  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,144,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, terraform, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, bash, agile, aws, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of Agile methodologies and DevOps principles. Good To Have Skills:Experience with cloud platforms such as AWS, Azure, or GCP. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, terraform, agile methodology, kubernetes, python, aws cloudformation, ansible, application development, sql, high availability, java, gcp, jenkins, troubleshooting, bash, mysql, html, agile, aws, application deployment",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, terraform, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, bash, agile, aws, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Cloud Security Architecture Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : Minimum 15 years of full time education  Summary :As an Application Developer with expertise in Cloud Security Architecture, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with cloud security architecture, ensuring the security of cloud-based applications and infrastructure.  Roles & Responsibilities:The Cloud Security Engineer will be responsible for modifying the cloud infrastructure to mitigate security risks. Resolve findings for Cloud Security vulnerabilities, including container images Create and modify cloud infrastructure using Terraform scripts as well as AWS CLI Good understanding and working knowledge of Kubernetes clusters Lead the design and implementation of cloud security architecture for cloud-based applications and infrastructure. Collaborate with cross-functional teams to ensure the security of cloud-based applications and infrastructure, including experience with cloud security tools and technologies. Conduct detailed analysis of complex cloud-based applications and infrastructure, employing security methodologies and techniques for actionable insights. Communicate technical findings effectively to stakeholders, utilizing data visualization tools for clarity. Stay updated with the latest advancements in cloud security architecture, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: -In-depth understanding of AWS services (VPC, EC2, S3, Postgres RDS, API Gateway, SQS, SNS, Lambda, ECR) with the ability to remediate security findings in these services- Understanding of containerization and container images- Good understanding of security concepts of cloud infrastructure- Experience with Terraform scripting- Experience with BASH scripting - Basic understanding of Linux Must To Have Skills:Expertise in Cloud Security Architecture. Good To Have Skills:Experience with cloud security tools and technologies. Strong understanding of cloud-based applications and infrastructure. Experience with data visualization tools. Solid grasp of security methodologies and techniques. Additional Information: The candidate should have a minimum of 7.5 years of experience in Cloud Security Architecture. The ideal candidate will possess a strong educational background in computer science, or a related field, along with a proven track record of delivering impactful solutions. This position is based at our Hyderabad office. Qualification Minimum 15 years of full time education",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloud security, aws iam, amazon ec2, ecr, terraform, kubernetes, python, api gateway, aws cli, javascript, application development, sql, amazon sqs, java, lambda expressions, linux, sns, bash, mysql, html, data visualization, api, aws, bash scripting",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, terraform, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, bash, agile, aws, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : DevOps Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 Years Full Time Education Summary :As an Application Developer with a primary skill in DevOps, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with DevOps tools and technologies to ensure seamless integration and deployment of applications.  Roles & Responsibilities: Design, develop, and maintain automated deployment pipelines using DevOps tools and technologies. Collaborate with cross-functional teams to ensure seamless integration and deployment of applications. Implement and maintain monitoring and logging solutions to ensure high availability and performance of applications. Troubleshoot and resolve issues related to application deployment and infrastructure. Stay updated with the latest advancements in DevOps tools and technologies, integrating innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Experience with DevOps tools and technologies such as Jenkins, Docker, Kubernetes, and Ansible. Must To Have Skills:Strong understanding of cloud computing platforms such as AWS, Azure, or Google Cloud Platform. Good To Have Skills:Experience with scripting languages such as Python or Bash. Good To Have Skills:Knowledge of infrastructure as code tools such as Terraform or CloudFormation. Good To Have Skills:Familiarity with Agile methodologies and DevOps best practices. Additional Information: The candidate should have a minimum of 3 years of experience in DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful DevOps solutions. This position is based at our Bengaluru office. Qualification 15 Years Full Time Education",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, docker, devops, cloud computing platform, terraform, kubernetes, python, aws cloudformation, ansible, application development, gcp, jenkins, troubleshooting, bash, agile, aws, application deployment, agile methodology",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Data Platform Engineer :: Full Time :: Remote,"5+ years of experience in a data engineering or cloud/infrastructure engineering role Experience building batch data pipelines using DAG-based tools such as Dagster or Airflow Experience deploying applications and service to Kubernetes and using related tools inthe Kubernetes ecosystem (i.e. Helm, ArgoCD, Istio) Experience implementing DevOps best practices within the data platform, including solutions for CI/CD, data observability, monitoring, and lineage Experience in producing and consuming topics to/from Apache Kafka, AWS Kinesis, or Azure Event Hubs Experience with Infrastructure as code tools such as Terraform Experience developing real-time data pipelines using frameworks such as Apache Beam, Flink, Storm, Spark Streaming, etc. Experience with data warehouses, data lakes, and their underlying infrastructure Proficiency in Python, SQL, RESTful API development Experience with cloud computing platforms such as Azure, AWS Experience with data governance, schema design, and schema evolution Role Type:  Full Time with  MSR Cosmos Group LLC Work Mode:  Remote Shift Timings:  General Shift (9:30 AM - 6:30 PM) Interested candidates can share resumes  nitin.paul@msr-it.com",20524004678,02-05-2024,31-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Recruitment / Staffing,"Dagster, Airflow, Orchestration, Apache Flink, isito, Kafka, Spark Streaming, Flink, Pipeline, Apache Storm, DevOps, Execution, Terraform, Kinesis, Event Hubs, Rest Api Development, Helm Charts, Argocd, Monitoring, Kubernetes, apache beam",-,9am-6pm,"Full Time, Permanent",MSR Cosmos Group LLC,Organization,MSR Cosmos Group LLC,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Google BigQuery Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Min 15 years of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Google BigQuery.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Google BigQuery. Design and implement data security and access controls. Troubleshoot and optimize data platform performance. Professional & Technical Skills: Must To Have Skills:Proficiency in Google BigQuery. Good To Have Skills:Experience with data security and access controls. Experience with data pipeline development and maintenance. Strong understanding of data platform architecture and design. Experience with troubleshooting and optimizing data platform performance. Additional Information: The candidate should have a minimum of 5 years of experience in Google BigQuery. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Pune office.",1.51E+11,15-05-2024,13-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, java, data modeling, hadoop, aws, hive, kubernetes, python, enterprise architecture, data warehousing, infrastructure architecture, docker, sql, gcp, spark, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Coimbatore,Coimbatore,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Google BigQuery Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Min 15 years of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Google BigQuery.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data pipelines using Google BigQuery. Design and implement data security and access controls. Troubleshoot and optimize data platform performance. Professional & Technical Skills: Must To Have Skills:Proficiency in Google BigQuery. Good To Have Skills:Experience with data security and access controls. Experience with data pipeline development and maintenance. Strong understanding of data platform architecture and design. Experience with troubleshooting and optimizing data platform performance. Additional Information: The candidate should have a minimum of 5 years of experience in Google BigQuery. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Pune office.",1.41E+11,14-05-2024,12-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"data security, pipeline, troubleshooting, platform architecture, bigquery, hive, kubernetes, python, enterprise architecture, microsoft azure, data warehousing, infrastructure architecture, docker, sql, java, data modeling, gcp, spark, hadoop, aws, big data, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Coimbatore,Coimbatore,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Data Platform Engineer,"   Design, develop and maintain Vimeo s product and event analytics platform that processes billions of records every day in real-time to enable analytics and event-driven systems     Partner with analytics leaders and analysts to ensure adherence to data governance and data modeling best practices     Partner with product engineering teams to enhance the efficiency of product analytics clients and data collection mechanisms     Contribute software designs, code, tooling, testing, and operational support to a multi-terabyte analytics platform     Provide technical leadership to the data engineering team and actively lead design discussions     Constantly monitor our data platform and make recommendations to enhance system architecture      Work collaboratively with other data engineers, analysts, and business stakeholders to understand and plan technical requirements for projects     Prioritize project intake, perform cost/benefit analysis, and make decisions about what work to pursue that best balances our platform s users, stakeholders, and technology         Skills and knowledge you should possess:         5+ years of engineering experience in a fast-paced environment; 2+ years of experience in scalable data architecture, fault-tolerant ETL, and monitoring of data quality in the cloud     Deep understanding of distributed data processing architecture and tools such as Kafka and Spark     Working knowledge of design patterns and coding best practices     Experience with and understanding of data modeling concepts, techniques, and best practices     Experience with Airflow, Celery, or other Python-based task-processing systems     Proficiency in SQL     Proficiency in Python or Java      Proficiency with modern source control systems, especially Git     Experience working with non-technical business stakeholders on technical projects         Bonus points (nice skills to have, but not needed):          Cloud-based DevOps: AWS or Google Cloud Platform or Azure     Experience with Amplitude, Snowplow, Segment, or other event analytics platforms      Relational database design     Snowflake or other distributed columnar-store databases     Basic Linux/Unix system administration skills     Familiarity with containerization technologies (e.g., Docker, Kubernetes)               ",2.70E+11,27-03-2024,25-06-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Internet,"Unix, System architecture, Linux, Data modeling, Coding, Database design, Analytics, SQL, Python, System administration",-,9am-6pm,"Full Time, Permanent",Vimeo,Organization,Vimeo,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : AWS Architecture Good to have skills : Amazon Web Services (AWS) Minimum  5  year(s) of experience is required Educational Qualification : Educational Qualification:15 years of full term education Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements.  Must have Skills :AWS Architecture, SSI: NON SSI:Good to Have Skills :SSI:Amazon Web Services (AWS) NON SSI :Job Requirements :Key Responsibilities :Key Responsibilities :1-Minimum 7 year of Experience in Python programming language 2-Minimum of 2 years of experience in AWS developer using S3, Glue, Lambda 3-Minimum of 3 years of experience years of experience in ETL, Big Data/Hadoop, database and data warehouse architecture and delivery 4- 3 years of experience in writing complex SQL queries - Experience working Teradata added advantage. 5-3 years of developing CICD pipelines using GIT, Jenkins, Terraform Technical Experience : Technical Experience :Backend Data engineer to setup the data pipeline for UI with technical expertise on python, AWS services - Appsync endpoint setup/GraphQl, lambda, Glue, open search, s3, Dynamodb. Professional Attributes :2-A Client facing skills:solid experience working in client facing environments, to be able to build trusted relationships with client stakeholders 3-Good critical thinking and problem-solving abilities 4-Health care knowledge 5-Good Communication Skills Educational Qualification:15 years of full term educationAdditional Info :Level and Across Accenture Location Facilities flex Qualification Educational Qualification:15 years of full term education",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"aws iam, ssi, etl, python, aws, web services, glue, dbms, sql, git, java, iam, jenkins, mysql, html, hadoop, big data, graphql, sql queries, oracle, dynamo db, javascript, application development, lambda expressions, data warehousing concepts, terraform, ci cd pipeline",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Cloud Infrastructure Good to have skills : NA Minimum  7.5  year(s) of experience is required Educational Qualification : 15 years of full term education Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with Cloud Infrastructure and collaborating with cross-functional teams to deliver high-quality solutions.  Roles & Responsibilities: Design, develop, and maintain cloud-based applications using Cloud Infrastructure. Collaborate with cross-functional teams to identify and prioritize application requirements. Develop and maintain technical documentation, including design documents, test plans, and user manuals. Ensure the scalability, reliability, and performance of cloud-based applications. Stay updated with the latest advancements in Cloud Infrastructure and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Strong experience in Cloud Infrastructure. Good To Have Skills:Experience with AWS, Azure, or Google Cloud Platform. Experience in designing, developing, and maintaining cloud-based applications. Strong understanding of cloud-based architecture and design patterns. Experience with containerization technologies such as Docker and Kubernetes. Experience with DevOps tools such as Jenkins, Git, and Ansible. Additional Information: The candidate should have a minimum of 7.5 years of experience in Cloud Infrastructure. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality cloud-based solutions. This position is based at our Bengaluru office. Qualification 15 years of full term education",1.10E+11,11-04-2024,10-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, git, design patterns, jenkins, cloud infrastructure, python, microsoft azure, javascript, docker, ansible, application development, sql, microservices, spring, java, iam, gcp, devops, test planning, mysql, aws, technical documentation",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer: AWS Cloud FullStack,"As an Application Developer, you will lead IBM into the future by translating system requirements into the design and development of customized systems in an agile environment. The success of IBM is in your hands as you transform vital business needs into code and drive innovation. Your work will power IBM and its clients globally, collaborating and integrating code into enterprise systems. You will have access to the latest education, tools and technology, and a limitless career path with the world's technology leader. Come to IBM and make a global impact! Your Role and Responsibilities Experience in Modernizing applications to Container based platform using EKS, ECS, Fargate Proven experience on using DevOps tools during Modernization. Solid experience around No-SQL database. Should have used Orchestration engine like Kubernetes, Mesos Java8, spring boot, sql, Postgres DB and AWS Secondary Skills: React, redux , JavaScript Experience level knowledge on AWS Deployment Services, AWS beanstalk, AWS tools & SDK, AWS Cloud9, AWS CodeStar, AWS Command line interface etc and hands on experience on AWS ECS, AWS ECR, AWS EKS, AWS Fargate, AWS Lambda function, Elastic Chache, S3 objects, API Gateway, AWS Cloud Watch and AWS SNS. Required Technical and Professional Expertise Creative problem-solving skills and superb communication Skill. Should have worked on at least 3 engagements modernizing client applications to Container based solutions. Should be expert in any of the programming languages like Java, .NET, Node .js, Python, Ruby, Angular .js Preferred Technical and Professional Expertise Experience in distributed/scalable systems Knowledge of standard tools for optimizing and testing code Knowledge/Experience of Development/Build/Deploy/Test life cycle",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"redux, react.js, postgresql, ecs, sql database, kubernetes, api gateway, eks, sql, java, amazon sns, testing life cycle, aws cloud, devops, amazon cloudwatch, python, aws beanstalk, aws lambda, javascript, ruby, spring boot, angular, amazon ec2, node.js, .net, sdk, aws",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Google BigQuery Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 plus years of full time education Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements.  Must have Skills :Google BigQuery, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :1:Assists with the data platform blueprint and design, encompassing the relevant data platform components.2:Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.3:The Data Engineer performs tasks such as data modeling, data pipeline build, data lake build, scalable programming frameworks Technical Experience :1:Expert in Python - NO FLEX. Strong hands-on- knowledge in SQL - NO FLEX, Python programming using Pandas, NumPy, deep understanding of various data structure dictionary, array, list, tree etc, experiences in pytest, code coverage skills2:Exp with building solutions using cloud native services:bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes NO FLEX3:Pro with tools to automate AZDO CI CD pipelines like Control-M , GitHub, JIRA, confluence , CI CD Pipeline Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:Additional Info : Qualification 15 plus years of full time education",1.10E+11,11-04-2024,10-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"kubernetes, ssi, sql, bigquery, python, css, confluence, web services, ci/cd, numpy, hibernate, jquery, spring, java, data modeling, flex, j2ee, json, html, mysql, jira, c#, rest, github, oracle, pytest, javascript, sql server, application development, pandas, spring boot",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Google BigQuery Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements.  Must have Skills :Google BigQuery, SSI: NON SSI:Good to Have Skills :SSI:No Technology Specialization NON SSI :Job Requirements :Key Responsibilities :1:Proven track record of delivering data integration, data warehousing solutions 2:Exp with data integration and migration projects 3:Proficient in BigQuery SQL language4:Good understanding on cloud native services :bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes5:Exp working with cloud solutions, mainly data platform services Willingness to get GCP Certifications 6:Required Preferred work experience in Shell Scripting, Python, Oracle, SQL Server Technical Experience :1:Expert in PytStrong hands-on and strong knowledge in SQL, Python programming using Pandas, NumPy, deep understanding of various data structure dictionary, array, list, tree etc, experiences in pytest, code coverage skills are preferred2:Strong hands-on experience with building solutions using cloud native services:bucket storage, Big Query, cloud function, pub sub, composer, and Kubernetes etc. Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Educational Qualification:Additional Info :",1.10E+11,11-04-2024,10-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"ssi, sql, bigquery, python, cloud native, kubernetes, data warehousing, numpy, microservices, spring, java, gcp, shell scripting, mysql, html, pubsub, rest, oracle, code coverage, pytest, sql server, javascript, application development, pandas, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Google Cloud Platform Architecture-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Google Cloud Platform Architecture  Good to have skills :NA Minimum 5 year(s) of experience is required  Educational Qualification :Fulltime 15 years qualification Key Reponsibilities :1 Being the first point of contact for Scrum master dev team for any architectural review of new functionality being introduced in the application both from best practices point of viewconsidering teams current skill set also for any potential performance impacts as well 2 Understanding existing cloud architecture suggesting ways means to help improve on it in terms of best practices performance etc Technical Experience : 1 5-7yrs of good developer full stack experience ASPNet Java Angular Cloud Tech with experience of atleast 1-2 yrs on cloud tech in particular google cloud is needed any prior experience of working as a tech arch for a team would be helpful 2 Any certifications to formalize their knowledge on goole cloud cloud tech is also recommended 3 Hands-on work on pub subs, GKE, terraform modules, cloud functions, writing optimizing BQ queries, firestore, microservice arch Professional Attributes :1:Good communication 2:Good Leadership skills and team handling skills 3:Analytical skills, presentation skills, ability to work under pressure 4:Should be able to work in shifts whenever required Qualification NA",1.10E+11,11-04-2024,10-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"google, cloud platform, java, gcp, platform architecture, kubernetes, project management, enterprise architecture, microsoft azure, docker, ansible, infrastructure architecture, angular, cloud architecture, devops, asp.net, linux, paas, jenkins, scrum, terraform, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
DC Engineer Specialist,"   Assessment and Planning           Conduct a comprehensive assessment of the existing QlikView environment to identify dependencies, data sources, and potential challenges for migration. Develop a detailed migration plan outlining tasks, timelines, and resource requirements.           Infrastructure Setup           Configure and optimize AWS infrastructure components such as EC2 instances, RDS databases, S3 storage, and networking to support QlikView deployment and data storage requirements.           Migration Execution           Execute the migration plan, including the transfer of QlikView applications, data, and configurations from on-premises servers to AWS. Ensure minimal downtime and data integrity throughout the migration process.           Security and Compliance           Implement security best practices and compliance standards for data protection and access control in the AWS environment. Configure IAM roles, security groups, and encryption mechanisms to safeguard sensitive information.           Performance Optimization           Fine-tune QlikView performance in the AWS environment by optimizing resource utilization, load balancing, and scalability. Monitor system metrics and troubleshoot performance issues as needed.           Integration and Testing           Integrate QlikView with other AWS services and third-party tools as required. Conduct thorough testing of migrated applications to validate functionality, data accuracy, and performance in the cloud environment.           Documentation and Knowledge Transfer           Document the migration process, configuration settings, and best practices for future reference. Provide training and knowledge transfer sessions to internal teams on managing and maintaining QlikView in the AWS cloud.             Required Skills and Qualifications:           ?Bachelors degree in Computer Science Information Technology or related field    ?Proven experience as a QlikView developer or administrator with at least 3 years of handson experience in QlikView development deployment and administration    ?Indepth knowledge of QlikView architecture data modeling scripting and visualization design    ?Strong understanding of AWS services including EC2 RDS S3 IAM VPC and CloudFormation    ?Experience in migrating onpremises applications and data to AWS cloud environments preferably with QlikView or similar BI tools    ?Familiarity with scripting languages such as PowerShell Python or Bash for automation and infrastructure management tasks    ?Excellent analytical and problemsolving skills with the ability to troubleshoot complex technical issues and optimize system performance    ?Strong communication and collaboration skills with the ability to work effectively in crossfunctional teams and interact with stakeholders at all levels  ",1.01E+11,10-06-2024,08-09-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Automation, Compliance, Networking, Data modeling, Infrastructure management, Oracle, Information technology, SQL, Python",-,9am-6pm,"Full Time, Permanent",Hexaware Technologies,Organization,Hexaware Technologies,https://img.naukimg.com/logo_images/groups/v1/12466.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Analyst,         SSL and SPN Certificate renewal.                  Azure 104 Admin certification is plus                 IAAS and PAAS Service experience of 5+ years is mandate.                 Should have handled deployments on Productions and debugged issues after release.                  Should have experience of SNOW Ticketing tools.                 Should have experience in troubleshooting PowerBI and Citirix issues.                 Should have handled Patching Activities on Azure cloud         ,2.81E+11,28-05-2024,26-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Administration, Analyst, ticketing tools, PAAS, Cloud, Troubleshooting, SSL",-,9am-6pm,"Full Time, Permanent",Essenware,Organization,Essenware,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Immediate Joiner-Hadoop Administrator,"  Hands on experience on CDP cluster.  Hadoop Cluster environment administration includes - cluster capacity planning, performance tuning, cluster Monitoring, Troubleshooting, and adding / removing cluster nodes.   CDP Cluster Build  JDK upgrade in Cloudera cluster  CDP Upgrade and rollback  Spark Upgrade  Master service migration  OS major version upgrade in Hadoop Cluster  Python upgrade  Maintaining cluster health and HDFS space for better performance   Fixing Vulnerabilities Strong knowledge in Shell Scripting and Python Strong verbal and written communication.  Bachelor or Masters degree in Computer Science, related field or equivalent work experience required. Good to have:  CDP Kafka Cluster build, CDP upgrade, administration, OS Upgrade  CDP Solr Cluster build, CDP upgrade, administration, OS Upgrade  AWS EMR cluster build, upgrade, administration  Coding knowledge in Puppet Please note this vacancy is only for Immediate joiners with Max NP as 30 days",60624012084,06-06-2024,04-09-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Hadoop Administration, Hadoop Admin, Hdfs",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://img.naukimg.com/logo_images/groups/v1/13832.gif,"Bhubaneswar, Hyderabad","Bhubaneswar, Hyderabad",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Workplace Engineering Track Engineer,"         Position      -     L     2     -     Exchange On-Prem      and      Office 365                   Exchange Server (On-premises)                     SMTP Relay management                    Manage Journaling Mailbox in DAG      environment                       Mail flow troubleshooting                    Perform Exchange server cumulative updates, vulnerability and security      patching                       Mailbox administration                 Troubleshoot issues with Exchange DAG Cluster                 Create and manage tickets to the OEM in support of the service.                     Exchange Online                     Recover deleted items in user mailboxes.                 Configure and manage archive and deletion policies for mailboxes in organization.                   Manage user      mailboxes                     Manage mailbox features such as the mailbox sharing      policy                   Set up Send asand Send on behalfdelegates for user mailboxes.                 Create and manage shared mailboxes so a group of people can monitor and send email from common email addresses.                   Create and manage email anti-spam protection,      malware     and content filters for the organization.                     Create and manage mail transport      rules                     Create and manage send/receive      connectors                     Manage Microsoft 365      groups                                 Manage DLP (Data Loss Prevention)      policies                     Perform Compliance eDiscovery mailbox      searches                     Troubleshoot issues with mailbox and calendar      permissions                     Troubleshoot Outlook client      issues                     Monitor Exchange Online service      health                   Create and manage tickets to the OEM in support of the service.                                 O365 Administration                     Manage meetings, including meeting policies, configurations, and conference bridges.                 Manage voice, including calling policies and phone number inventory and assignment.                 Manage messaging, including messaging policies.                   Manage all org-wide settings, including federation, teams upgrade, and      teams     client settings.                   Manage the teams in the organization and their associated settings, including membership (group management supported via PowerShell, team management in the Teams admin center).                 Manage Teams-certified devices and set up and assign configuration policies.                 View user profile page and troubleshoot user call quality problems using advanced troubleshooting toolset.                   Access all reports in the Microsoft Teams admin      center                     Access, monitor and troubleshoot tenants call quality and reliability using data exposed in Call Quality Dashboard (CQD) down to the users impacted by poor call quality. Create new call quality reports,      update     and remove call quality reports as needed. Upload and update CQD building data.                     Publish apps to the tenant app catalog in the Microsoft Teams admin      center                   Monitor Teams service health.                 Create and manage tickets to the OEM in support of the service.                                     Sharepoint     Administration                       Create, delete and manage      sites                         Add and remove site      admins                   Manage sharing settings at organizational level (including collaboration access for guests)                   Manage site      collections                   Manage Microsoft 365 groups (including creating, deleting, restoring groups, and changing group owners)                   Manage site storage      limits                     Monitor      Sharepoint     service health.                   Create and manage tickets to the OEM in support of the service.                     One Drive for Business Administration                      Configure, manage, and monitor storage quotas.                    Set and manage retention policy for      ODFB                   Manage the sharing of files.                 Perform e-Discovery search for the ODFB data.                   Restore deleted ODFB      data                     Troubleshoot desktop     application synchronization issues.                   Troubleshoot ODFB file sharing issues.                 Monitor One Drive service health.                 Create and manage tickets to the OEM in support of the service     .                           Intune - MDM administrations                     Provide provisioning, configuration, and user administration support for authorized user mobile      devices                                 Manage enrollment of      devices (     including creating and deploying policies that configure security settings, set password requirements and deploy certificates)                   Assign devices to configuration groups based on a devices profile.                  Create and deploy custom applications to mobile devices.                 Manage and deploy Organizational Polices to devices.                 Replace expiring Apple MDM Push Certificate                    Manage enrollment program      tokens                     Retire mobile devices, perform device wipes to remove organization      data as     part of managed mobile service disposal process or if lost or stolen.                                 Autopilot Administration                           Manage the enrollment, wiping and retirement of Windows 10/11 devices.                   Create and manage compliance and configuration policies for      Windows devices     .                   Create and manage update ring profiles (defines when devices get Windows updates installed) for Windows devices.                 Create and manage feature update profiles (specifies what devices use what featured version of the OS) for Windows devices.                 Create and manage quality update (expedites the deployment of security patches) and driver update profiles for Windows devices.                 Create and deploy desktop applications to Windows devices.               ",1.61E+11,16-05-2024,14-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SMTP, Administration, Team management, data loss prevention, OEM, Vulnerability, Windows, Troubleshooting, microsoft, Monitoring",-,9am-6pm,"Full Time, Permanent",Hexaware Technologies,Organization,Hexaware Technologies,https://img.naukimg.com/logo_images/groups/v1/12466.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Snowflake Administrator,"  Total Yrs. of Experience: 6 Years + Relevant Yrs. of experience:  5 Years Detailed JD         Minimum 5+ years of hands-on administration experience, from setting up the Snowflake environments to successfully administering it (AWS preferred)  Experience automating, scripting, and streamlining processes for efficiency and accuracy utilizing Unix shell scripting, Python etc.  Some experience with AWS with knowledge of S3, EC2, VPC, IAM, Security, Networking etc.  Ability and experience with the development of processes and procedures to standardize Database configuration  Extensive experience with implementation and maintenance of Disaster Recovery and High availability.  Ability to work on unusually complex technical problems and provide solutions that are highly innovative and ingenious.  Ability to provide technical documentation and project plans for technical staff members.  Excellent communication, presentation, and customer relationship skills.  Excellent organizational and time management skills to handle multiple tasks simultaneously  Mandatory skills: Snowflake Admin Desired/ Secondary skills: AWS",10624003322,01-06-2024,30-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Snowflake Admin, Snowflake Administrator, Cloud Admin",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://img.naukimg.com/logo_images/groups/v1/13832.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Cloud Engineer II,"     1. Cluster Management     You will manage and administer Hadoop clusters running on Hortonworks and Cloudera Data Platform. Ensure high availability and performance of HBase clusters. Perform regular upgrades, patches, and configuration changes to maintain cluster health and security.       2. Monitoring and Troubleshooting:     You will monitor cluster performance and implement measures to ensure optimal operation. Troubleshoot complex issues related to HBase and Hadoop ecosystems. Perform root cause analysis for service disruptions and implement corrective measures.       3. Data Management:     You will oversee data ingestion, transformation, and extraction processes. Ensure data integrity and security across the Big Data infrastructure. Implement and maintain backup and recovery procedures.       4. Automation and Scripting:     You will develop and maintain scripts for automation of routine tasks such as monitoring, maintenance, and data processing. Leverage tools like Ansible, Puppet, or similar for configuration management and automation.       5. Security and Compliance:     You will implement security best practices for Big Data environments. Ensure compliance with relevant data protection regulations and corporate policies. Manage access controls and monitor for unauthorized access attempts.       6. Collaboration and Documentation:     You will work closely with data engineering, analytics, and DevOps teams to support data-driven initiatives. Document processes, configurations, and best practices. Provide guidance and support to junior administrators and other team members.         Must Have         Job Respobsibilities         1. Hands-on experience with HBase administration and optimization.   2. Proficiency with Hortonworks Data Platform (HDP) and Cloudera Data Platform (CDP).   3. Experience with cluster monitoring and management tools.   4. Expertise in security best practices and compliance for Big Data environments.   5. Excellent problem-solving skills and the ability to perform root cause analysis.         Minimum Qualifications:         1. Bachelor s degree in Computer Science, Information Technology, or a related field.   2. 5+ years of system administration experience with 3+ years in Big Data administration.   3. Extensive experience with HBase, Hadoop, Hortonworks, and Cloudera Data Platform.   4. Strong understanding of cloud infrastructure services and management.   5. Proficiency in Linux system administration and scripting languages such as Python, Shell, or Perl.         Principal Working Relationship         1. Reports to the Engineering Supervisor/Engineering Manager   2. Collaborates with Data Engineers, Data Scientists, DevOps, and Cloud Governance teams.   3. Interacts with internal stakeholders to gather requirements and provide updates.   4. Coordinates with vendors and external support teams for technical assistance and upgrades.         Nice to Haves         1. Experience with cloud platforms such as AWS, Azure, or Google Cloud.   2. Familiarity with containerization and orchestration tools like Docker and Kubernetes.   3. Knowledge of other Big Data technologies such as Spark, Hive, or Kafka.   4. Experience in a regulated industry or with healthcare data.   5. Certification in Big Data or Cloud technologies (e.g., Cloudera Certified Administrator, AWS Certified Solutions Architect).   ",40624500865,04-06-2024,02-09-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Automation, Compliance, Data management, Configuration management, Healthcare, Perl, Troubleshooting, Information technology, Analytics, Python",-,9am-6pm,"Full Time, Permanent",Medtronic,Organization,Medtronic,https://img.naukimg.com/logo_images/groups/v1/722108.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Cloud Engineer,"     1. Cluster Management     You will manage and administer Hadoop clusters running on Hortonworks and Cloudera Data Platform. Ensure high availability and performance of HBase clusters. Perform regular upgrades, patches, and configuration changes to maintain cluster health and security.       2. Monitoring and Troubleshooting:     You will monitor cluster performance and implement measures to ensure optimal operation. Troubleshoot complex issues related to HBase and Hadoop ecosystems. Perform root cause analysis for service disruptions and implement corrective measures.       3. Data Management:     You will oversee data ingestion, transformation, and extraction processes. Ensure data integrity and security across the Big Data infrastructure. Implement and maintain backup and recovery procedures.       4. Automation and Scripting:     You will develop and maintain scripts for automation of routine tasks such as monitoring, maintenance, and data processing. Leverage tools like Ansible, Puppet, or similar for configuration management and automation.       5. Security and Compliance:     You will implement security best practices for Big Data environments. Ensure compliance with relevant data protection regulations and corporate policies. Manage access controls and monitor for unauthorized access attempts.       6. Collaboration and Documentation:     You will work closely with data engineering, analytics, and DevOps teams to support data-driven initiatives. Document processes, configurations, and best practices. Provide guidance and support to junior administrators and other team members.         Must Have         Job Respobsibilities         1. Hands-on experience with HBase administration and optimization.   2. Proficiency with Hortonworks Data Platform (HDP) and Cloudera Data Platform (CDP).   3. Experience with cluster monitoring and management tools.   4. Expertise in security best practices and compliance for Big Data environments.   5. Excellent problem-solving skills and the ability to perform root cause analysis.         Minimum Qualifications:         1. Bachelor s degree in Computer Science, Information Technology, or a related field.   2. 8+ years of system administration experience with 5+ years in Big Data administration.   3. Extensive experience with HBase, Hadoop, Hortonworks, and Cloudera Data Platform.   4. Strong understanding of cloud infrastructure services and management.   5. Proficiency in Linux system administration and scripting languages such as Python, Shell, or Perl.         Principal Working Relationship         1. Reports to the Engineering Supervisor/Engineering Manager   2. Collaborates with Data Engineers, Data Scientists, DevOps, and Cloud Governance teams.   3. Interacts with internal stakeholders to gather requirements and provide updates.   4. Coordinates with vendors and external support teams for technical assistance and upgrades.         Nice to Haves         1. Experience with cloud platforms such as AWS, Azure, or Google Cloud.   2. Familiarity with containerization and orchestration tools like Docker and Kubernetes.   3. Knowledge of other Big Data technologies such as Spark, Hive, or Kafka.   4. Experience in a regulated industry or with healthcare data.   5. Certification in Big Data or Cloud technologies (e.g., Cloudera Certified Administrator, AWS Certified Solutions Architect).   ",40624500864,04-06-2024,02-09-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Automation, Compliance, Data management, Configuration management, Healthcare, Perl, Troubleshooting, Information technology, Analytics, Python",-,9am-6pm,"Full Time, Permanent",Medtronic,Organization,Medtronic,https://img.naukimg.com/logo_images/groups/v1/722108.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : SAS Visual Text Analytics Good to have skills : SAS Visual Data Mining and Machine Learning, SAS Platform Minimum  3  year(s) of experience is required Educational Qualification : Role  Summary /PurposeWe are looking for an experienced system administrator to deploy and manage SAS Viya on AWS and to participate in platform responsibilities related to on-prem SAS and migration to the public/private cloud, including working/monitoring/administrating SAS VIYA 4 platform, automating deployment with Terraform, IaC scripts and help us building CICD pipeline for SAS Viya 4 Deployment/Upgrade while working in collaboration with SYF DevOps Team. The profile must also understand security roles/policies (IAM) and other AWS Services, including S3, EKS, Redshift, RDS, FSx (Filesystem) and Terraform, IaC, etc.Essential Responsibilities Work with cloud architects, SAS vendors, and other stakeholders to deploy SAS Viya on public/private cloud on Containers (Kubernetes) as available for AWS (EKS) or on-prem (TKGI) and help design the IaC and Terraform scripts for automated deployment while working with SYF DevOps and Cloud Platform Engineering Team. Work with the SAS platform and run teams to ensure the environment remains highly available and current. Complete any required documentation related to the build and run. Perform other duties as needed to ensure the effective running of the environment. Implement best practices for automating the SAS Viya Deployment process using Terraform and IaC scripts and work with the DevOps team to deploy the automated scripts in the CICD pipeline.  Good understanding of IAM Roles and Policies and working with the IAM Team to deploy/design roles/policies as required by the Application.  Hands-on experience with various AWS services like S3, Redshift, RDS, EKS, FSx (Filesystem),including DevOps, Terraform, etc. Qualifications / Requirements Bachelor's Degree, or equivalent, in a quantitative field, such as Engineering, Computer Science, etc.) At least seven (5-7) years of experience in Information Technology. At least two (2-3) years of experience with public cloud deployments (MUST) for DevOps, IaC, and IAM roles and policies. Proficient in DevOps and Terraform/IaC in the public cloud. Strong communication skills Good to have experience as an SAS Viya administratorDesired Characteristics Able to work effectively with multiple teams and stakeholders. Willingness to stay abreast of the latest developments in technology. Able to efficiently balance the demands of supporting the environment, Platform Engineering Team, and Run team. Able to effectively troubleshoot issues with installation and configuration. Demonstrated ability to work effectively in a team environment. Self-motivated person who can complete tasks with minimal supervision from Platform Leads. Qualification NA",90624901322,09-06-2024,07-09-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"public cloud, iac, text analytics, devops, terraform, kubernetes, python, sas, information technology, amazon redshift, data mining, cloud deployment, file system, amazon rds, eks, machine learning, sas viya, application development, sql, iam, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Hadoop Administrator,"Hands on experience on CDP cluster ,Hadoop Cluster environment administration includes - cluster capacity planning, performance tuning, cluster Monitoring, Troubleshooting, and adding / removing cluster nodes.",30624009083,03-06-2024,01-09-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Hadoop Administration, CDP, CLUSTER",-,9am-6pm,"Full Time, Temporary/Contractual",BR Raysoft,Organization,BR Raysoft,-,Hyderabad,Hyderabad,-,-,-,18-25 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Microsoft Azure DevOps Good to have skills : Microsoft SQL Server Minimum  5  year(s) of experience is required Educational Qualification : Btech Summary :As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Microsoft Azure DevOps. Your typical day will involve collaborating with cross-functional teams, developing and deploying applications, and ensuring their functionality and performance.  Roles & Responsibilities: Design, build, and configure applications to meet business process and application requirements using Microsoft Azure DevOps. Collaborate with cross-functional teams to identify and prioritize application requirements. Develop and deploy applications, ensuring their functionality and performance. Troubleshoot and debug applications to resolve issues and improve performance. Ensure the security, scalability, and maintainability of applications. Professional & Technical Skills: Must have-Good knowledge of Azure Cloud and Azure PaaS offeringsWork experience in Azure Devops CI/CD pipelines, preferably YamlWork experience in application integration using REST API and other integration patternsProgramming experience preferably Python.ETL knowledge, preferably ADF and Data BricksSQL knowledge Good To Have Skills:Knowledge of SAP PowerDesigner or any other data modeling toolData Governance and Data catalog knowledgeGood understanding on the Relational Databases and Datalakes.Good to have Bash scriptingGood to have Azure Cloud networking concepts.Certification on DP203Window's administration Additional Information: The candidate should have a minimum of 5 years of experience in Microsoft Azure DevOps. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful applications. This position is based at our Bengaluru office.",2.91E+11,29-05-2024,27-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"azure paas, ci/cd, azure cloud, sql server, microsoft azure devops, continuous integration, rest, python, sap, networking, relational databases, azure devops, application development, sql, application integration, oracle adf, devops, bash, api, etl",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Architect: Big Data,"In this role, you'll work in one of our IBM Consulting Client Innovation Centres (Delivery Centers), where we deliver deep technical and industry expertise to a wide range of public and private sector clients around the world. Our delivery centres offer our clients locally based skills and technical expertise to drive innovation and adoption of new technology. Information and Data are some of the most important organizational assets in today's businesses. As a Security Consultant, you will be a key advisor for IBM's clients, analyzing business requirements to design and implement the best security solutions for their needs. You will apply your technical skills to find the balance between enabling and securing the client's organization with the cognitive solutions that are making IBM the fastest growing enterprise security business in the world. Your Role and Responsibilities Create Solution Outline and Macro Design to describe end to end product implementation in Data Platforms including, System integration, Data ingestion, Data processing, Serving layer, Design Patterns, Platform Architecture Principles for Data platform  Contribute to pre-sales, sales support through RfP responses, Solution Architecture, Planning and Estimation Contribute to reusable components asset accelerator development to support capability development  Participate in Customer presentations as Platform Architects Subject Matter Experts on Big Data, AWS Cloud and related technologies Participate in customer PoCs to deliver the outcomes Participate in delivery reviews product reviews, quality assurance and work as design authority Required Technical and Professional Expertise Experience in designing of data products providing descriptive, prescriptive, and predictive analytics to end users or other systems Experience in data engineering and architecting data platforms  Experience in architecting and implementing data platforms on AWS Cloud Platform Experience in AWS services - S3, Glue, Glue Catalog, Lambda, Kinesis, RedShift, RedShift Spectrum, Snowflake, DataBricks on AWS, DynamoDB, AWS EMR, Kubernetes, Terraform, Airflow Experience in Big Data stack (Hadoop ecosystem Hive, HBase, Kafka, Spark, Scala PySpark, Python etc.) with Cloudera or Hortonworks Preferred Technical and Professional Expertise Agile and DevSecOps in Data Platform development and adoption of tools like Github, Bitbucket, Jira, Jenkins, Artifactory, Maven, Docker, SVN or AWS DevOps Exposure to Gen AI offerings in AWS Implementation of Data Fabric and Data Mesh concepts and solutions like AWS Data Zone or Starburst or Denodo or IBM Data Virtualisation or Talend or Tibco Data Fabric",60624907086,06-06-2024,04-09-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"docker, aws cloud, spark, hadoop, big data, hive, cloudera, artifactory, kubernetes, aws iam, scala, glue, amazon redshift, bitbucket, devops, aws emr, jenkins, hbase, snowflake, github, python, dynamo db, maven, svn, denodo, data bricks, kafka, terraform, devsecops, agile, aws",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Hiring For Snowflake Architect Role,"  Job Title:  Snowflake Lead/Architect    Location:  Bangalore   Experience:  8+ Years About role:  We are seeking a highly skilled Snowflake Lead/Architect to join our Data & AI team in Bangalore. The ideal candidate will have extensive experience in designing, implementing, and managing Snowflake-based data solutions. This role involves leading the development of data architectures and ensuring the effective use of Snowflake to drive business insights and innovation.   Key Responsibilities: Design and architect scalable,       efficient, and secure Snowflake solutions to meet business requirements. Define data architecture       frameworks, standards, and principles, including modelling, metadata,       security, and reference data. Lead the development and       implementation of Snowflake-based data warehouses, data lakes, and data       integration solutions. Oversee data ingestion,       transformation, and loading processes to ensure data quality and       performance. Provide technical leadership and       mentorship to a team of data engineers and analysts. Collaborate with business       stakeholders and IT teams to develop data strategies and ensure alignment       with business goals. Drive continuous improvement by       leveraging the latest Snowflake features and industry trends. Qualifications: Bachelors or Master?? degree in       Computer Science, Information Technology, Data Science, or a related       field. 8+ years of experience in data       architecture, data engineering, or a related field. Extensive experience with       Snowflake, including designing and implementing Snowflake-based       solutions. Proven track record of leading       data projects and teams in complex environments. Experience with data modelling,       ETL processes, and data warehousing concepts. Familiarity with cloud platforms       (e.g., AWS, GCP) and their data services. Snowflake certification (e.g., SnowPro Core, SnowPro Advanced).  (Nice to have)  To learn more about us and our culture visit:  www.relanto.ai.com & https://www.linkedin.com/company/relanto-inc/ Regards, Relanto - TAG",30624008203,03-06-2024,01-09-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"cloud, snowflake, Snowflake architect, Data modelling, snowflake certification, Azure, GCP, Data Architecture, snowflake architecture, ETL, AWS, SQL",-,9am-6pm,"Full Time, Permanent",Relanto Global,Organization,Relanto Global,-,Bangalore Rural,Bangalore Rural,-,-,-,10-20 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Engineer,"     ?       We are seeking a Data Engineer with 4-6 years of experience to join our Data Platform team. This role will report to the Manager of data engineering and be involved in the planning, design, and implementation of our centralized data warehouse solution for ETL, reporting and analytics across all applications within the company.     Deep knowledge and experience working with Scala and Spark.     Experienced in Azure data factory, Azure Data bricks, Azure Synapse Analytics, Azure Data Lake.     Experience working in Full stack development in .Net & Angular.     Experience working with SQL and NoSQL database systems such as MongoDB, Couchbase.     Experience in distributed system architecture design.     Experience with cloud environments (Azure Preferred).     Experience with acquiring and preparing data from primary and secondary disparate data sources (real-time preferred).     Experience working on large scale data product implementation, responsible for technical delivery, mentoring and managing peer engineers.     Experience working with Databricks is preferred.     Experience working with agile methodology is preferred.     Healthcare industry experience is preferred.               Job Responsibilities:           Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions.     Work with other team with deep experience in ETL process, distributed microservices, and data science domains to understand how to centralize their data.     Share your passion for staying experimenting with and learning new technologies.     Perform thorough data analysis, uncover opportunities, and address business problems.     Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.               Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.                     ",1.61E+11,16-05-2024,14-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Medical Services / Hospital,"System architecture, Data analysis, NoSQL, SCALA, Agile, Healthcare, Agile methodology, MongoDB, Analytics, SQL",-,9am-6pm,"Full Time, Permanent",R1 RCM,Organization,R1 RCM,https://img.naukimg.com/logo_images/groups/v1/2483944.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Engineer,"     ?         We are seeking a Software Data Engineer with 4-7 year of experience to join our Data Platform team. This role will report to the Manager of data engineering and be involved in the planning, design, and implementation of our centralized data warehouse solution for ETL, reporting and analytics across all applications within the company.      Qualifications:       Deep knowledge and experience working with    Python/Scala and Apache Spark       Experienced in    Azure data factory, Azure Data bricks, Azure Blob Storage, Azure Data Lake, Delta lake.       Experienced in orchestration tool    Apache Airflow   .         Experience working with SQL and NoSQL database systems such as    MongoDB, Apache Parquet           Experience with Azure cloud environments     Experience with acquiring and preparing data from primary and secondary disparate data sources     Experience working on large scale data product implementation.     Experience working with agile methodology preferred.     Healthcare industry experience preferred.           Responsibilities:       Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions     Work with other team with deep experience in ETL process and data science domains to understand how to centralize their data     Share your passion for staying experimenting with and learning new technologies.     Perform thorough data analysis, uncover opportunities, and address business problems.       Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.             Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.                     ",1.61E+11,16-05-2024,14-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Medical Services / Hospital,"Data analysis, Agile, Healthcare, MongoDB, Agile methodology, Apache, Revenue cycle management, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",R1 RCM,Organization,R1 RCM,https://img.naukimg.com/logo_images/groups/v1/2483944.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"TLM  Developers., with MNC in Bangalore","Candidates with 7+ to 15 Yrs with good understanding & hands-on experience with at least one SmartStream TLM product and/or 3rdparty solutions such as Java, UNIX, SQL, Kubernetes, Camel, Kafka, OpenShift, Docker etc. Exp., in TLM Configuration etc Required Candidate profile Responsible for the development/configuration of solutions, and working with users, and project teams utilizing the SmartStream application suite to support, enhance and develop the platforms.",1.41E+11,05-06-2024,03-09-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"TLM, TLM Studio, TLM Developer, TLM Premium, TLM Schema, TLM Configuration",-,9am-6pm,"Full Time, Permanent",Disha Hr Services,Organization,Disha Hr Services,-,"Bengaluru, Mumbai (All Areas)","Bengaluru, Mumbai (All Areas)",-,-,-,14-24 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Intern - Cloud Engineer / Cloud Consultant,"Experience: Freshers Work Timings: Rotational Shift Internship Scope: Intern to work on Cloud Computing: To learn & develop, implement, optimize, and maintain cloud-based solutions in collaboration with our team. Automation on Cloud Upskill on Cloud, DevOps, Big Data, AI and ML technologies Researching and designing Cloud POCs Working on deploying and debugging cloud solutions, Understanding & work on the concept of the security of the cloud infrastructure. Pre-requisites: Understanding of cloud technologies such as AWS, Azure, GCP, etc. Good networking Skills Basic Programming and scripting Knowledge like Python, Power shell and Bash. Understanding of Software Development Lifecycle and Development Practices. Degree in computer science or a similar field. Azure certifications preferred. Troubleshooting and analytical skills. Good communication and collaboration skills. Flexible to work in different shifts. Stipend: Month 1 No Stipend Month 2 and 3 - INR 10,000/- per month, purely subject to your performance and contribution. This decision of stipend eligibility lies with the concerned Department Head Month 4 Full time offer or termination of Internship (based on performance and evaluation during the Internship period)",2.71E+11,27-05-2024,25-08-2024,EducationalOccupationalCredential,1,Engineering - Software & QA,Data Platform Engineer,Software Product,"Cloud Computing, Cloud Consultant, scripting, DevOps, Azure, Cloud Engineer, GCP, Big Data, debugging, Troubleshooting, AWS",-,9am-6pm,"Full Time, Permanent",Spektra Systems,Organization,Spektra Systems,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Rest API/Senior Consultant Specialist/Pune/HSBC Technology,"  We are currently seeking an experienced professional to join our team in the role of Senior Consultant Specialist In this role, you will: Contribute to design and coding to build the end to end solutions. Collaborate with stakeholders and end-users to understand and gather requirements to develop the best automation solutions. Coordinate and embed technical standards and help other team members to gain knowledge on above mentioned technologies. Facilitate issue resolution to ensure that the release schedule remains on track and that functional issues are solved through appropriate, agreed solutions To be successful in this role, you should meet the following requirements: Minimum 8 years of software development experience Must have development experience in Python (3.x) and strong practical knowledge Must have Project Management & Lifecycle experience Good to have experience in Natural Language Programming & Machine Learning. Experience in automating the continuous integration/continuous delivery pipeline within a DevOps Product/Service team driving a culture of continuous improvement  Good to have hands-on Experience on Docker, Kubernetes, Kafka and Google Cloud Good to have hands-on experience in Rest API (Python) & Angular is good-to-have Good to have knowledge of Microservices based architecture and patterns. Good to have knowledge of cloud concepts and cloud architecture, different types of cloud computing (preferably GCP). Good to have knowledge of DevOps principles & tools like GitHub/Jenkins, Jira, Confluence, Ansible. Youll achieve more when you join HSBC. www.hsbc.com/careers",50624010619,05-06-2024,03-09-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Investment Banking / Venture Capital / Private Equity,"DevOps, Docker, Rest APIs, Python, Kubernetes, Angularjs, AI/ML, GCP",-,9am-6pm,"Full Time, Permanent",Hsbc,Organization,Hsbc,https://img.naukimg.com/logo_images/groups/v1/164080.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
ETL Technical Consultant - Pharma exp MUST,"Job Requirement Required Experience, Skills & Competencies: Strong Hands-on experience in implementing Data Lake with technologies like Data Factory (ADF), ADLS, Databricks, Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and Purview. Experience of using big data technologies like Hadoop (CDH or HDP), Spark, Airflow, NiFi, Kafka, Hive, HBase or MongoDB, Neo4J, Elastic Search, Impala, Sqoop etc. Strong programming & debugging skills either in Python and Scala/Java. Experience of building REST services is good to have. Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner. Good understanding and Experience of using CI/CD with Git, Jenkins / Azure DevOps. Experience of setting up cloud-computing infrastructure solutions. Hands on Experience/Exposure to NoSQL Databases and Data Modelling in Hive 9+ years of technical experience with at-least 2 years on MS Azure and 2 year on Hadoop (CDH/HDP). B.Tech/B.E from reputed institute preferred.",50624912988,05-06-2024,03-09-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Software Product,"ETL, cloud, Airflow, ETL Technical Consultant, Hive, ADLS, Azure Synapse Analytics, Kafka, Databricks, Spark, NiFi, HBase",-,9am-6pm,"Full Time, Permanent",AVE-Promagne Business Solutions,Organization,AVE-Promagne Business Solutions,-,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Data Factory  ETL Consultant - Pharma exp MUST,"Job Requirement Required Experience, Skills & Competencies: Strong Hands-on experience in implementing Data Lake with technologies like Data Factory (ADF), ADLS, Databricks, Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and Purview. Experience of using big data technologies like Hadoop (CDH or HDP), Spark, Airflow, NiFi, Kafka, Hive, HBase or MongoDB, Neo4J, Elastic Search, Impala, Sqoop etc. Strong programming & debugging skills either in Python and Scala/Java. Experience of building REST services is good to have. Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner. Good understanding and Experience of using CI/CD with Git, Jenkins / Azure DevOps. Experience of setting up cloud-computing infrastructure solutions. Hands on Experience/Exposure to NoSQL Databases and Data Modelling in Hive 9+ years of technical experience with at-least 2 years on MS Azure and 2 year on Hadoop (CDH/HDP). B.Tech/B.E from reputed institute preferred.",50624912983,05-06-2024,03-09-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Software Product,"ETL, Airflow, Kafka, SSIS, Impala, HBase, Azure Data Factory, Hive, Neo4J, Sqoop, Cloud, MongoDB, Spark, NiFi, Elastic Search",-,9am-6pm,"Full Time, Permanent",AVE-Promagne Business Solutions,Organization,AVE-Promagne Business Solutions,-,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Informatica ETL Consultant - Pharma exp MUST,"Job Requirement Required Experience, Skills & Competencies: Strong Hands-on experience in implementing Data Lake with technologies like Data Factory (ADF), ADLS, Databricks, Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and Purview. Experience of using big data technologies like Hadoop (CDH or HDP), Spark, Airflow, NiFi, Kafka, Hive, HBase or MongoDB, Neo4J, Elastic Search, Impala, Sqoop etc. Strong programming & debugging skills either in Python and Scala/Java. Experience of building REST services is good to have. Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner. Good understanding and Experience of using CI/CD with Git, Jenkins / Azure DevOps. Experience of setting up cloud-computing infrastructure solutions. Hands on Experience/Exposure to NoSQL Databases and Data Modelling in Hive 9+ years of technical experience with at-least 2 years on MS Azure and 2 year on Hadoop (CDH/HDP). B.Tech/B.E from reputed institute preferred.",50624911891,05-06-2024,03-09-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Software Product,"ETL, Data Modelling, Impala, cloud, Jenkins, Git, Neo4J, Sqoop, NoSQL Databases, MongoDB, informatica, Azure DevOps, Elastic Search",-,9am-6pm,"Full Time, Permanent",AVE-Promagne Business Solutions,Organization,AVE-Promagne Business Solutions,-,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Glue ETL Consultant,"Pharma exp MUST Job Requirement Required Experience, Skills & Competencies: Strong Hands-on experience in implementing Data Lake with technologies like Data Factory (ADF), ADLS, Databricks, Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and Purview. Experience of using big data technologies like Hadoop (CDH or HDP), Spark, Airflow, NiFi, Kafka, Hive, HBase or MongoDB, Neo4J, Elastic Search, Impala, Sqoop etc. Strong programming & debugging skills either in Python and Scala/Java. Experience of building REST services is good to have. Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner. Good understanding and Experience of using CI/CD with Git, Jenkins / Azure DevOps. Experience of setting up cloud-computing infrastructure solutions. Hands on Experience/Exposure to NoSQL Databases and Data Modelling in Hive 9+ years of technical experience with at-least 2 years on MS Azure and 2 year on Hadoop (CDH/HDP). B.Tech/B.E from reputed institute preferred.",50624909957,05-06-2024,03-09-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Software Product,"AWS Glue, Hive, Azure Synapse Analytics, Hadoop, Kafka, Azure Data Lake, Spark, ETL, SSIS, HBase",-,9am-6pm,"Full Time, Permanent",AVE-Promagne Business Solutions,Organization,AVE-Promagne Business Solutions,-,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Hadoop Technical Consultant," Pharma exp MUST Job Requirement Required Experience, Skills & Competencies: Strong Hands-on experience in implementing Data Lake with technologies like Data Factory (ADF), ADLS, Databricks, Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and Purview. Experience of using big data technologies like Hadoop (CDH or HDP), Spark, Airflow, NiFi, Kafka, Hive, HBase or MongoDB, Neo4J, Elastic Search, Impala, Sqoop etc. Strong programming & debugging skills either in Python and Scala/Java. Experience of building REST services is good to have. Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner. Good understanding and Experience of using CI/CD with Git, Jenkins / Azure DevOps. Experience of setting up cloud-computing infrastructure solutions. Hands on Experience/Exposure to NoSQL Databases and Data Modelling in Hive 9+ years of technical experience with at-least 2 years on MS Azure and 2 year on Hadoop (CDH/HDP). B.Tech/B.E from reputed institute preferred.",50624909955,05-06-2024,03-09-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Software Product,"Hadoop, Hive, Azure Synapse Analytics, Kafka, Azure Data Lake, MongoDB, Spark, ETL, Cosmos DB, HBase",-,9am-6pm,"Full Time, Permanent",AVE-Promagne Business Solutions,Organization,AVE-Promagne Business Solutions,-,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Workplace Strategy Intern,"   Support WPS pitches and develop reports needed.      Collaborate on projects with our regional design studios, project consultants as required.      Attend and support in ongoing project meetings and Client presentations as needed.      Work closely with the Workplace Strategy team within the allocated tenure      Coordinate with various studios to gather relevant documents and layouts for the purpose of analytics      Support with manual historic data collection of identified projects to form a part of the company database      Support with transferring gathered data to the company proprietary tool      You will perform any additional responsibilities as may be requested or assigned from time to time                What we expect:          You are a team player with a positive attitude to go the extra mile      You have multi-tasking ability, with time management and organizational skills.      You have knowledge of the design tool - AutoCAD.      You will subscribe to the company core values of Teamwork, Integrity and Excellence.      You are interested in design, strategic approaches to design, research, data collection and analytics.    ",10923501371.0,01-09-2023,30-11-2023,EducationalOccupationalCredential,1,Engineering - Software & QA,Data Platform Engineer,Engineering & Construction,"Intern, wps",-,9am-6pm,"Full Time, Permanent",Space Matrix Design Consultants,Organization,Space Matrix Design Consultants,https://img.naukimg.com/logo_images/groups/v1/4043268.gif,Pune,Pune,-,-,-,Unpaid P.M ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Software Engineer,               Stakeholder management                         Good Comms skills                         Problem-solving skill                           PLSQL/SQL skills (Mandate)                             Any ETL tool/Pentaho skills (Mandate)                             Any Cloud knowledge (AWS/Azure/GCP) (Mandate)                           Big Query knowledge (optional)                               Must have                                ?             1: Strong PL/ SQL knowledge                      2: Knowledge Any ETL tool                      3: Any cloud knowledge good to have               ,60624502962.0,06-06-2024,04-09-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"GCP, Cloud, query, PLSQL, Stakeholder management, ETL tool, AWS, Pentaho",-,9am-6pm,"Full Time, Permanent",Essenware,Organization,Essenware,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Java Platform Engineer - Java8 / Microservices / Kafka,"   You will join a team of insatiably curious data engineers, software architects, and product experts who never settle for ""good enough""      Our Java Platform teams tech stack is based on Java8 (Spring Boot) and RESTful web services      We typically build and deploy applications as cloud-native Kubernetes microservices and integrate with scalable technologies such as Kafka in Docker container environments      Our developers work in an agile process to efficiently deliver high value data driven applications and product packages                    Required Experience:        Minimum of Bachelor s Degree or its equivalent in Computer Science, Computer Information Systems, Information Technology and Management, Electrical Engineering or a related field.      Have experience working and strong understanding of object-oriented programing and cloud technologies      End to end experience delivering production ready code with Java8, Spring Boot, Spring Data, and API libraries      Strong experience with unit and integration testing of the Spring Boot APIs.      Strong understanding and production experience of RESTful APIs and microservice architecture.      Strong understanding of SQL databases and NoSQL databases and experience with writing abstraction layers to communicate with the databases.                Nice to haves (but not required):        Exposure to Kotlin or other JVM programming languages      Strong understanding and production experience working with Docker container environments      Strong understanding and production experience working with Kafka      Cloud Environments: AWS, GCP or Azure          ",231000000000.0,23-08-2023,21-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Computer science, Electrical engineering, spring boot, NoSQL, GCP, Integration testing, Cloud, Agile, Information technology, SQL",-,9am-6pm,"Full Time, Permanent",Egen Solutions,Organization,Egen Solutions,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Journeyman Data Ingest Platform Engineer," The    Data Ingest Platform Engineer    responsibilities include conducting full development lifecycle of data that includes requirements from DHS, other OMB initiatives, and provide support for the whole program. This position also requires building a new data automation practice on the program to address our client s most pressing needs with Cyber Security Threats and Data. The successful candidate will bring a consultative approach to data to improve the value of the data that s being collected by our customers. This position is also a thought leader in the practice of Big Data in solving our clients cyber security problems, coupled with demonstrated experience designing and developing enterprise data solutions for large clients by providing a new approach to the team, presenting white papers and other solutions.           Responsibilities include, but are not limited to:         Design, implement, and support a highly available and fault tolerant distributed Cribl LogStream architecture     Develop standards and governance policies for management of the extended Cribl architecture     Develop disaster recovery plan and implement cloud-native capabilities to ensure the Cribl-based Data Ingest component remains available at all times     Develop automated deployments that support IaaS, container, and Kubernetes     Develop and maintain tool integration packages consisting of a number of endpoint types to include REST APIs, Database connections, and proprietary connections like Splunk or ServiceNow.     Manage a centralized and curated registry of pre-packaged tool integrations ready for Agency consumption.     Develop data processing strategies to ensure efficient collection, aggregation, and transport of relevant cybersecurity data.             Basic Qualifications:         Bachelors Degree complete or in progress preferably in applied mathematics, statistics, computer science, data science, electrical engineering, physics, or closely related field     A minimum of (6) six years of overall experience     Familiar with JavaSrcript     Experience with JSON parsing and YAML     Experience interacting with RESTful APIs     Experience with scripting languages like Python, Bash, and PowerShell     Experience with TLS/SSL to secure data in transit.     Experience collaborating with US Government Agencies, state or local governments, or commercial entities to develop IT service program maturity in accordance with Federal IT mandates and best practices.             Preferred Qualifications:         Demonstrated ability to investigate data and present findings to internal teammates and client audiences.     Experience in conducting assessments of an Enterprise by reviewing technical documentation, conducting interviews and workshops to identify gaps and develop a tailored solution is highly desired.             Clearance Requirements:         Must be a US citizen and pass a background investigation.     Able to obtain and maintain a DHS Suitability/Entry on Duty (EOD)   ",1.00E+11,10-04-2024,09-07-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,Software Product,"Automation, Powershell, Disaster recovery, Data processing, splunk, JSON, SSL, Python, Technical documentation",-,9am-6pm,"Full Time, Permanent",Hackajob,Organization,Hackajob,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Senior Software Engineer,"Key Responsibilities Develops code for moderately difficult software projects. Design and implement bug fixes. Designs moderately difficult software projects. Develops software documentation. Troubleshoots a variety of moderately difficult software problems. Performs software tests on code and enhancements. Defines software specifications. Interfaces with external customers regarding software issues Interfaces with internal customers for requirements analysis and schedule. Compiles data for regularly scheduled or special reports, analysis and statements. 1.  DataOPS : - Proficiency in Python Core/Advanced for development and data pipelining. - Strong understanding of data structures, Pandas, Numpy, sklearn, concurrency, and design patterns. 2.  DevOPS : - Experience in deploying applications using CI/CD tools such as Jenkins, Jfrog, Docker, Kubernetes, and Openshift Container Platform. 3.  Microservices & REST APIs : - Familiarity with FastAPI, Flask, and Tornado for developing microservices and REST APIs. 4.  Cloud : - Knowledge of building and deploying applications using cloud platforms. 5.  Databases & SQL : - Proficiency in working with databases such as Postgres, Clickhouse, and MongoDB. 6.  Caching & Queuing : - Experience with Pub/Sub (RabbitMQ), Redis, and Diskcache for caching and queuing purposes. 7.  Operating system : - Strong understanding of both Linux and Windows operating systems. 8.  Monitoring and Logging : - Familiarity with Splunk for monitoring and logging applications. Good to have skills include: 1.  Generative AI knowledge : - Knowledge of the Langchain framework and ChatGPT for generative AI applications. 2.  MLOPS knowledge : - Experience with Databricks, MLFlow, Kubeflow, and ClearML for managing machine learning operations. 3.  Testing knowledge : - Proficiency in integration testing, Python Behave, and Pytest for ensuring code quality. 4.  Maintaining code quality standards : - Working knowledge of Pylint for maintaining code quality standards. 5.  Logging : - Familiarity with Kibana and Elastic search for advanced logging and analysis. As a Software Engineer, you will be responsible for developing and maintaining software applications, designing data pipelines, deploying applications using CI/CD tools, building microservices and REST APIs, working with various databases, implementing caching and queuing mechanisms, monitoring and logging applications, and potentially working with APIs, MLOPS, testing, and maintaining code quality.",60624908774,06-06-2024,04-09-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Electronic Components / Semiconductors,"Software Engineering, Kubeflow, REST API, MLOPS, Linux, APIs, CI/CD, MLFlow, Databricks, Splunk, software documentation, Python",-,9am-6pm,"Full Time, Permanent",Applied Materials,Organization,Applied Materials,https://www.naukri.com/hotjobs/images/v3/amiplaug16.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Staff Engineer, Data Analytics Engineering","   Minimum of 6+ years of experience in developing ETL jobs using any industry leading ETL tool.     Ability to design,  develop,  and optimize   Apache Spark   applications for large-scale data processing.     Ability to implement efficient data transformation and manipulation logic using Spark RDDs and Data Frames.     Ability to design,  implement,  and maintain Apache Kafka pipelines for real-time data streaming and event-driven architectures.     Development and deep technical skill in   Python  ,    PySpark  ,  Scala,    NIFI   and    SQL/Procedure.     Working knowledge and understanding on Unix/Linux operating system like awk,  ssh,  crontab,  etc. ,     Ability to write transact SQL,  develop and debug stored procedures and user defined functions in python.     Working experience on   Postgres   and/or   Redshift/Snowflake   database is required.     Exposure to CI/CD tools like bit bucket,  Jenkins,  ansible,  docker,    Kubernetes   etc.  is preferred.     Ability to understand relational database systems and its concepts.     Ability to handle large table/dataset of 2+TB in a columnar database environment.     Ability to integrate data pipelines with Splunk/Grafana for real-time monitoring,  analysis,  and   Power BI   visualization.     Ability to create and schedule the Airflow Jobs.   ",1.81E+11,18-05-2024,16-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,Electronic Components / Semiconductors,"Unix, Linux, power bi, SSH, Stored procedures, Apache, SDLC, Monitoring, SQL, Python",-,9am-6pm,"Full Time, Permanent",Western Digital,Organization,Western Digital,https://img.naukimg.com/logo_images/groups/v1/4627297.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Machine Learning Engineer and MLOP/Cloud engineer,"         MLE - 2                     Python (Pyspark not required), OOPS experience needed                 refactoring expertise                  Azure Stack familiarity and experience (Azure Cloud)                 Azure ML and ADB are potential tech stack we will explore                 MLOPs experience                 Git and Agile experience                 Minimal Data science experience expected                               MLOP/Cloud engineer                       identifying the best tech solution, cost and benefit analysis                 migrating and building pipelines for getting the solution to cloud                 refactoring expertise                  ? ",1.10E+11,11-01-2024,10-04-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"GIT, data science, OOPS, devops, Machine learning, Cloud, Agile, Cost, Python",-,9am-6pm,"Full Time, Permanent",Conneqt,Organization,Conneqt,https://img.naukimg.com/logo_images/groups/v1/257614.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Oracle ERP Cloud Engineer - Q2C,"   Basic            Bachelor?? or Master?? degree in Computer Science or related field     8 years of experience as Technical analyst in Oracle ERP     3+ years of experience in implementing solutions in Oracle Cloud modules in - ERP Cloud Order Management, Subscription Management, AR, SLA and GL modules     3+ years of experience in Oracle SaaS and PaaS environments, customizing and extending cloud ERP     3+ years of experience in configuring BPM Flows, SLA, page customization, BI data models, reports and dashboards     2+ years of experience in integrating SaaS application with 3rd parties such as Banks, Vendors and enable B2B Solutions within SaaS framework     Experience with OTBI, FBDI, BI Publisher     Experience with API??, REST/SOAP based web services available in Oracle cloud ERP, Groovy Scripting and must have strong SQL skills     Experience with test plans, test cases, test scripts and QA process and automation of data extracts and data loads             Preferred            Certification in Oracle ERP Cloud       Advanced MS Excel skills     Experience with VBCS     OSS, Advanced Collections, Integration to 3rd party systems, banks     Experience with OIC development     Strong problem solving, analytical and critical thinking skills     Excellent teammate who is able to work with virtual and global multi-functional teams     Excellent spoken and written communication as well as receptive listening skills, with ability to convey sophisticated ideas in a clear, concise fashion to technical and non-technical audiences     Must have excellent organizational and multitasking skills with a can-do approach       ",30624502186,03-06-2024,01-09-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Data Platform Engineer,Software Product,"Cloud, Oracle ERP",-,9am-6pm,"Full Time, Permanent",Docusign,Organization,Docusign,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Salesforce Omnistudio Platform Good to have skills : Salesforce Lightning Web Components Minimum  5  year(s) of experience is required Educational Qualification : 15 years of education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Salesforce Omnistudio Platform.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components using Salesforce Omnistudio Platform. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain data platform components using Salesforce Omnistudio Platform. Troubleshoot and resolve issues related to data platform components using Salesforce Omnistudio Platform. Ensure data platform components are scalable, reliable, and secure. Professional & Technical Skills: Must To Have Skills:Experience in Salesforce Omnistudio Platform. Good To Have Skills:Experience in Salesforce Lightning Web Components. Strong understanding of data platform components and their integration with systems and data models. Experience in developing and maintaining data platform components. Ability to troubleshoot and resolve issues related to data platform components. Knowledge of scalability, reliability, and security of data platform components. Additional Information: The candidate should have a minimum of 5 years of experience in Salesforce Omnistudio Platform. The ideal candidate will possess a strong educational background in software engineering or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Bengaluru office.",40624910754,04-06-2024,02-09-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"salesforce lightning, apex, salesforce, data modeling, software engineering, visualforce, rest, soql, css, web services, sfdc, triggers, machine learning, javascript, salesforce crm, sales force development, data loader, java, installation, html",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Neo4j Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of full time education Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and utilizing Neo4j to develop and maintain the data platform.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain the data platform utilizing Neo4j. Ensure the data platform is scalable, reliable, and secure. Troubleshoot and resolve any issues related to the data platform. Professional & Technical Skills: Must To Have Skills:Experience with Neo4j. Strong understanding of data platform components and architecture. Experience with data modeling and database design. Experience with ETL processes and tools. Experience with cloud-based data platforms such as AWS or Azure. Additional Information: The candidate should have a minimum of 3 years of experience with Neo4j. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Pune office. Qualification Minimum 15 years of full time education",70624914180,07-06-2024,05-09-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, neo4j, data modeling, aws, etl, visualforce, rest, python, css, web services, sfdc, machine learning, triggers, javascript, apex, sql, salesforce, spring, salesforce crm, spring boot, java, html, mysql, mongodb",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://img.naukimg.com/logo_images/groups/v1/10476.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.  Must have skills : Spring Boot Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : 15 year Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models using Spring Boot.  Roles & Responsibilities: Collaborate with Integration Architects and Data Architects to design and implement data platform components using Spring Boot. Assist with the development of the data platform blueprint and design, ensuring cohesive integration between systems and data models. Implement and maintain data platform components, ensuring high availability and scalability. Troubleshoot and resolve issues related to data platform components, working closely with cross-functional teams. Stay updated with the latest advancements in data platform technologies and integrate innovative approaches for sustained competitive advantage. Professional & Technical Skills: Must To Have Skills:Proficiency in Spring Boot. Strong understanding of data platform components and their integration with systems and data models. Experience with implementing and maintaining data platform components, ensuring high availability and scalability. Experience with troubleshooting and resolving issues related to data platform components. Solid grasp of data platform technologies and their integration with other systems. Experience with cloud-based data platforms such as AWS or Azure. Additional Information: The candidate should have a minimum of 5 years of experience in Spring Boot. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Mumbai office.",40624910524,04-06-2024,02-09-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, spring boot, data modeling, troubleshooting, aws, sosl, visualforce, rest, soql, sfdc, machine learning, triggers, dashboards, javascript, computer assembling, apex, salesforce, salesforce crm, data loader, java, installation, html, apex classes",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Genesys Cloud CX Developer,"   Solution Design: Collaborate with stakeholders to gather requirements and design custom solutions that leverage Genesys Cloud CX capabilities to improve customer experiences     Custom Development: Develop and customize applications, integrations, and workflows within the Genesys Cloud CX platform using its API, SDKs, and scripting capabilities     Integration: Integrate Genesys Cloud CX with other systems and data sources, such as CRM systems, databases, and third-party applications, to create a unified customer experience     Automation: Implement automation and workflows to streamline customer interactions and agent tasks, improving efficiency and consistency     Routing and Queuing: Configure call and interaction routing strategies to ensure customers are connected to the right agents or resources     Scripting: Create and maintain scripts for IVR (Interactive Voice Response) systems, chatbots, and other customer self-service channels     Reporting and Analytics: Implement reporting and analytics solutions to track key performance indicators, monitor customer interactions, and gain insights into customer behavior   ",2.61E+11,26-10-2023,24-01-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Solution design, Automation, IVR, Cloud, Routing, Customer experience, Analytics, Monitoring, CRM, Scripting",-,9am-6pm,"Full Time, Permanent",Nbits,Organization,Nbits,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Engineer (GCP),"         We are seeking a skilled and experienced Cloud Engineer to join our dynamic team      As a Cloud Engineer, you will play a crucial role in implementing and managing cloud architectures for our clients software applications      Your strong expertise in Google Cloud Platform (GCP) implementations, programming languages, and cloud ecosystem design will contribute to the success of our cloud-based solutions                       We are offering a highly competitive salary commensurate with industry standards.                    Minimum Qualifications:            - Demonstrated experience in implementing cloud architecture for software applications.            - Extensive expertise in GCP implementations.            - Proficiency in at least one of the programming/scripting languages.            - Proficient in using Linux CLI commands and Google Cloud SDK.            - Ability to design holistic cloud ecosystems with a focus on Google Cloud Platform capabilities and features.            - Familiarity with Cloud Shell and GCP commands such as gcloud and gsutil.            - Hands-on experience with GCP IaaS services such as GCE, GAE, GKE, VPC, DNS, Interconnect VPN, CDN, Cloud Storage, FileStore, Firebase, Deployment Manager, and Stackdriver.            - Familiarity with GCP services including Cloud Endpoints, Dataflow, Dataproc, Datalab, Dataprep, Cloud Composer, Pub/Sub, and Cloud Functions.                  Responsibilities:            - Troubleshoot issues, actively seeking out problems, and providing effective solutions.            - Implementing HA and DR solutions            - Be an active participant in the running of the team, fostering a great place to work.            - Engage with the wider business to identify opportunities for future work for the team.            - Experiment with new technologies to help push the boundaries of what the team is building.                            Requirements        - Professional certifications related to cloud platforms, specifically Google Cloud Platform.            - Experience with other cloud platforms such as AWS or Azure.            - Knowledge of containerization technologies (e.g., Docker, Kubernetes).            - Familiarity with DevOps practices and tools.            - Understanding basic network and security principles in cloud environments.            - Experience with automation and infrastructure-as-code tools, preferably terraform            - Excellent problem-solving and analytical skills.            - Strong communication and collaboration abilities.                    Benefits        Health Insurance for a worry-free lifestyle.            Flexible work hours for better work-life balance.            Informal dress code to express your individuality.            Enjoy a 5-day work week to pursue your passions outside of work.            Indulge in free snacks and beverages for those energy boosts!          ",1.51E+11,15-09-2023,14-12-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"SAN, Automation, Linux, GCP, VPN, SOC, SMS, DNS, SDK, CRM",-,9am-6pm,"Full Time, Permanent",Transcloud Labs,Organization,Transcloud Labs,-,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Adobe Analytics Platform Engineer,"  Purpose and Scope: As a Platform Engineer of a team of individuals in a specific area of digital expertise, you will be a crucial player in driving our digital initiatives forward in our agile organization. Our agile operating model consists of two components Digital Capability and Digital Execution. Digital Execution are about aligning multiple missions around business goals and facilitating collaboration on a larger scale. Digital Capability, on the other hand, focus on the growth and development of individuals within a specific expertise area. This dual structure enables us to scale agile practices efficiently while maintaining a focus on both product development and individual skill enhancement. An Enterprise Business Platforms Engineer focuses on the development, deployment, and integration of software platforms that support our business processes and operations. This role involves a blend of technical expertise, business acumen, and a deep understanding of the specific platforms that drive business functions such as CRM (Customer Relationship Management, Sales Force, Veeva), ERP (Enterprise Resource Planning such as SAP), and SCM (Supply Chain Management) and Clinical, Patient Safety, Quality Management Solutions. The goal is to ensure these platforms are optimized to support business goals, enhance efficiency, and drive growth. You will be at the forefront of implementing innovative solutions and will have the opportunity to work on cutting-edge technologies in your field. As an Analytics Engineer, you are responsible for developing, implementing, and maintaining the Adobe Analytics platform. Essential Job Responsibilities: Platform Development and Configuration: Design, develop, and configure business platforms to meet the specific needs of our organization. This could involve programming, configuring settings, and integrating various software solutions. System Integration: Ensure seamless integration between different business platforms and systems (e.g., integrating CRM and ERP systems) to enhance data flow and business processes Performance Monitoring and Optimization: Regularly monitor the performance of business platforms, identify bottlenecks, and implement optimizations to improve efficiency and user experience. User Support and Training: Provide technical support to platform users, resolve issues, and conduct training sessions to ensure users can effectively utilize the platforms. Initiative Execution: Actively participate in and contribute to various initiatives, applying your specialized skills to achieve the objectives and expected Value. Contribute to the ongoing realization of Value from these enterprise platforms through continuous integration and deployment. Collaboration: Work collaboratively with team members within the subdivision and across other digital and business units. Continuous Learning: Engage in continuous learning and professional development to stay abreast of the latest trends and technologies in a specific area of expertise. Innovation: Contribute innovative ideas and approaches to enhance project outcomes and digital capabilities. Reporting: Regularly report on the progress of the various Value Teams and outcomes to your Capability Lead and team members. Problem-Solving: Employ analytical and problem-solving skills to overcome project challenges and deliver effective solutions. Quality Assurance: Ensure the highest quality and consistency in the work delivered. Qualifications: Required Bachelors or Master?? degree in relevant field, e.g., Computer Science, Data Science, Business Administration. Relevant experience in business analysis, technical analysis, or a related area, demonstrating a track record of bridging business needs with technical solutions. Analytical Skills: Strong analytical and problem-solving skills to understand complex business issues and develop appropriate technical solutions. Technical Knowledge: Good understanding of information technology, software development life cycles, and architectural frameworks. Familiarity with programming languages, databases, and software development tools is beneficial. Communication Skills: Excellent verbal and written communication skills, with the ability to convey technical concepts to non-technical stakeholders and vice versa. Collaboration: Ability to work effectively in a team environment, collaborating with diverse groups of stakeholders, including business users, Digital professionals, and management. Project Management: Good knowledge of Agile methodology, principles and practices.  Attention to Detail: Precision in documenting requirements, specifications, and project details to ensure clarity and alignment with objectives. Adaptability: Flexibility to adapt to changing requirements, technologies, and project priorities. Minimum 5-7 Years of hands-on experience implementing Adobe Analytics for websites Experience and knowledge of utilizing the Adobe Experience Cloud ID Define, develop and interpret, maintain a solution design reference document to handle requirements. Experience activating Adobe Analytics (using tools like Adobe Experience Platform Launch or DTM) and creating/configuring rules within the tag management system. Experience Configuring the Adobe Analytics environment to accept data Experience Leveraging a data layer within a deployment for efficient data collection Extensive hands-on implementation experience using Custom JavaScript, CSS, HTML, jQuery selector and AJAX to accurately track user interactions and behaviors across digital platforms (web, mobile, apps) Identifying and resolving issues related to data collection and reporting. Experience using debugging tools relevant to Adobe Analytics, such as Adobe Debugger, and other browser developer tools. Handling JavaScript errors and resolving issues related to data collection. Strong experience and knowledge of Adobe Analytics Admin console and report suite settings Extensive experience understanding processing rules and classification rules for data manipulation. Experience with available Adobe Analytics-related APIs and Adobe I/O integration Preferred Life Sciences / Pharma industry experience Marketing exposure is a plus Certification in Adobe Analytics is a plus Benefits: Medical, Dental and Vision Insurance Generous Paid Time Off options, including Vacation, Sick time, plus national holidays including Heritage Days, and Summer and Winter Breaks 401(k) match and annual company contribution Company paid life insurance Annual Corporate Bonus and Quarterly Sales Incentive for eligible positions Long Term Incentive Plan for eligible positions Referral bonus program Role & responsibilities  ",1.11E+11,11-06-2024,09-09-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Project Management, Adobe Experience Cloud, Adobe Analytics, Data Science, CSS, Query, Javascript, HTML, JQuery",-,9am-6pm,"Full Time, Permanent",Astellas Pharma,Organization,Astellas Pharma,https://img.naukimg.com/logo_images/groups/v1/3037096.gif,Mumbai (All Areas),Mumbai (All Areas),-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
"Platform Engineer, Site Contracting & Monitoring, BoldX","This is a remote position and is based in India. Remote work from certain states may be permitted in accordance with Astellas Responsible Flexibility Guidelines. Candidates interested in remote work are encouraged to apply. Purpose and Scope: As a Platform Engineer of a team of individuals in a specific area of digital expertise, you will be a crucial player in driving our digital initiatives forward in our agile organization. Our agile operating model consists of two components ??Digital Capability and Digital Execution. Digital Execution are about aligning multiple missions around business goals and facilitating collaboration on a larger scale. Digital Capability, on the other hand, focus on the growth and development of individuals within a specific expertise area. This dual structure enables us to scale agile practices efficiently while maintaining a focus on both product development and individual skill enhancement. A Platforms Engineer focuses on the development, deployment, and integration of software platforms that support our business processes and operations. This role involves a blend of technical expertise, business acumen, and a deep understanding of specific platforms in the Clinical Operations space focused on site contracting and site monitoring. The goal is to ensure these platforms are optimized to support business goals, enhance efficiency, and drive growth. You will be at the forefront of implementing innovative solutions and will have the opportunity to work on cutting-edge technologies in your field. Essential Job Responsibilities: Platform Development and Configuration: Design, develop, and configure business platforms to meet the specific needs of our organization. This could involve programming, configuring settings, and integrating various software solutions. System Integration: Ensure seamless integration between different business platforms and systems (e.g., integrating CRM and ERP systems) to enhance data flow and business processes. Performance Monitoring and Optimization: Regularly monitor the performance of business platforms, identify bottlenecks, and implement optimizations to improve efficiency and user experience. User Support and Training: Provide technical support to platform users, resolve issues, and conduct training sessions to ensure users can effectively utilize the platforms. Initiative Execution: Actively participate in and contribute to various initiatives, applying your specialized skills to achieve the objectives and expected Value. Contribute to the ongoing realization of Value from these enterprise platforms through continuous integration and deployment. Collaboration: Work collaboratively with team members within the subdivision and across other digital and business units. Continuous Learning: Engage in continuous learning and professional development to stay abreast of the latest trends and technologies in a specific area of expertise. Innovation: Contribute innovative ideas and approaches to enhance project outcomes and digital capabilities. Reporting: Regularly report on the progress of the various Value Teams and outcomes to your Capability Lead and team members. Problem-Solving: Employ analytical and problem-solving skills to overcome project challenges and deliver effective solutions. Quality Assurance: Ensure the highest quality and consistency in the work delivered. Qualifications: Required Bachelor?? degree in relevant field, e.g., Computer Science, Data Science Minimum of 3-5 years of demonstrated relevant experience Demonstrated configuration experience in the Clinical Operations space with a specific focus on site Contracting and site Monitoring Business Process Understanding: Knowledge of site contracting and site monitoring business processes within Clinical Operations and how systems such as Agiloft/Cluepoints SPOT supports these processes Strong analytical and problem-solving skills Ability to work effectively in a team environment Excellent communication skills, both written and verbal Agile and adaptable to changing environments and project scopes Present technical solutions based on the business requirement of the Clinical Veeva system. Able to support technical upgrade on quarterly basis by conducting technical impact assessment, addressing any technical issues, support the environment management, support testing, and document changes per the SLC process Manage user account, create roles, and support quarterly user access review request. Support system deliverables and provide technical feedback during internal and external Veeva Clinical system audit Work closely with the internal privacy and security team to assess Personal Information requests and act according per the privacy laws Maintain and report any issues and communicate effectively to all stakeholders during project/enhancement phase, service disruptions, and during the technical upgrade. Preferred Veeva or Salesforce Admin Certification 1-3 years demonstrated experience developing and configuring platforms such as Salesforce, VeeVa Vault, Agiloft, and/or Cluepoints Agile Champion: Adherence to DevOps principles and a proven track record with CI/CD pipelines for continuous delivery Technical Proficiency: Strong coding skills in relevant tools and technologies, e.g., Java, C++, Python, R, SQL is highly beneficial. Benefits: Medical, Dental and Vision Insurance Generous Paid Time Off options, including Vacation, Sick time, plus national holidays including Heritage Days, and Summer and Winter Breaks 401(k) match and annual company contribution Company paid life insurance Annual Corporate Bonus and Quarterly Sales Incentive for eligible positions Long Term Incentive Plan for eligible positions Referral bonus program",1.11E+11,18-06-2024,16-09-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Data Platform Engineer,Pharmaceutical & Life Sciences,"Devops Ci Cd, Clinical Operations, Veeva, Salesforce Admin, Site Monitoring, Java, C++, Analytical Skills, Agiloft, SQL, Data Science, R, Cluepoints, Python",-,9am-6pm,"Full Time, Permanent",Astellas Pharma,Organization,Astellas Pharma,https://img.naukimg.com/logo_images/groups/v1/3037096.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Cloud Platform Engineer,"Project Role : Cloud Platform Engineer Project Role Description : Designs, builds, tests, and deploys cloud application solutions that integrate cloud and non-cloud infrastructure. Can deploy infrastructure and platform environments, creates a proof of architecture to test architecture viability, security and performance.  Must have skills : Red Hat OpenShift Good to have skills : Red Hat 3Scale API Management Minimum  5  year(s) of experience is required Educational Qualification : RedHat OCP Admin Summary :As a Cloud Platform Engineer, you will be responsible for designing, building, testing, and deploying cloud application solutions that integrate cloud and non-cloud infrastructure. Your typical day will involve deploying infrastructure and platform environments, creating a proof of architecture to test architecture viability, security, and performance using Red Hat OpenShift and Red Hat 3Scale API Management.  Roles & Responsibilities: Design, build, test, and deploy cloud application solutions that integrate cloud and non-cloud infrastructure using Red Hat OpenShift. Deploy infrastructure and platform environments, create a proof of architecture to test architecture viability, security, and performance. Collaborate with cross-functional teams to ensure successful delivery of cloud application solutions. Provide technical support and troubleshooting for cloud application solutions. Professional & Technical Skills: Must To Have Skills:Experience in Red Hat OpenShift. Good To Have Skills:Experience in Red Hat 3Scale API Management. Experience in designing, building, testing, and deploying cloud application solutions. Experience in deploying infrastructure and platform environments. Strong understanding of cloud architecture, security, and performance. Experience in providing technical support and troubleshooting for cloud application solutions. Additional Information: The candidate should have a minimum of 5 years of experience in Red Hat OpenShift. The ideal candidate will possess a strong educational background in software engineering or a related field, along with a proven track record of delivering impactful cloud application solutions. This position is based at our Bengaluru office. Qualification RedHat OCP Admin",80624906769,08-06-2024,06-09-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"redhat openshift, api management, cloud architecture, troubleshooting, software engineering, kubernetes, python, openshift, redhat linux, microsoft azure, cloud platform, docker, ansible, red, java, git, linux, microsoft windows, jenkins, cloud infrastructure, hat, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : SAS Visual Text Analytics Good to have skills : SAS Visual Data Mining and Machine Learning, SAS Platform Minimum  3  year(s) of experience is required Educational Qualification : Role  Summary /PurposeWe are looking for an experienced system administrator to deploy and manage SAS Viya on AWS and to participate in platform responsibilities related to on-prem SAS and migration to the public/private cloud, including working/monitoring/administrating SAS VIYA 4 platform, automating deployment with Terraform, IaC scripts and help us building CICD pipeline for SAS Viya 4 Deployment/Upgrade while working in collaboration with SYF DevOps Team. The profile must also understand security roles/policies (IAM) and other AWS Services, including S3, EKS, Redshift, RDS, FSx (Filesystem) and Terraform, IaC, etc.Essential Responsibilities Work with cloud architects, SAS vendors, and other stakeholders to deploy SAS Viya on public/private cloud on Containers (Kubernetes) as available for AWS (EKS) or on-prem (TKGI) and help design the IaC and Terraform scripts for automated deployment while working with SYF DevOps and Cloud Platform Engineering Team. Work with the SAS platform and run teams to ensure the environment remains highly available and current. Complete any required documentation related to the build and run. Perform other duties as needed to ensure the effective running of the environment. Implement best practices for automating the SAS Viya Deployment process using Terraform and IaC scripts and work with the DevOps team to deploy the automated scripts in the CICD pipeline.  Good understanding of IAM Roles and Policies and working with the IAM Team to deploy/design roles/policies as required by the Application.  Hands-on experience with various AWS services like S3, Redshift, RDS, EKS, FSx (Filesystem),including DevOps, Terraform, etc. Qualifications / Requirements Bachelor's Degree, or equivalent, in a quantitative field, such as Engineering, Computer Science, etc.) At least seven (5-7) years of experience in Information Technology. At least two (2-3) years of experience with public cloud deployments (MUST) for DevOps, IaC, and IAM roles and policies. Proficient in DevOps and Terraform/IaC in the public cloud. Strong communication skills Good to have experience as an SAS Viya administratorDesired Characteristics Able to work effectively with multiple teams and stakeholders. Willingness to stay abreast of the latest developments in technology. Able to efficiently balance the demands of supporting the environment, Platform Engineering Team, and Run team. Able to effectively troubleshoot issues with installation and configuration. Demonstrated ability to work effectively in a team environment. Self-motivated person who can complete tasks with minimal supervision from Platform Leads. Qualification NA",2.51E+11,25-05-2024,23-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"public cloud, iac, text analytics, devops, terraform, kubernetes, python, sas, information technology, amazon redshift, data mining, cloud deployment, file system, amazon rds, eks, machine learning, sas viya, application development, sql, iam, aws",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills : Qlik Sense Good to have skills : NA Minimum  5  year(s) of experience is required Educational Qualification : Graduation Manage day to day operations of Qlik Platforms.Deliver customer service within specified service levels and typically around more complex problems including diagnosis and resolution of issues within a Qlik Platform.Perform periodic maintenance and upgrades of Qlik Software components.Collaborating with the Qlik global support team to solve customer technical issuesResolving customers technical issues through diligent research, reproduction, and troubleshootingKnowledge of ITSM Tools and processes.Actively contributing with creating technical articles, using and maintaining QLIK knowledge baseInstall, configure and upgrade Qlik Sense application.Manage migration of QLIK objects between Development, QA and Production environmentsAnalyze Windows Event logs and Qlik Sense Server log files to troubleshoot and solve issues.Identify potential solutions to resolve and prevent service interruptions.Provide Qlik Sense administration services via Management Console :managing licenses, setting up permission, designing folder structure and scheduling tasks for publishing purposes.Administer and maintain access to Qlik Sense Management Console.Proactively monitor the Qlik Sense environment to ensure performance and scalability.Perform day to day administrative activities on the Qlik applications and servers.Monitor daily task execution on Qlik Sense.Work with product teams in trouble shooting, new feature discussion and operational support.Prepare and update disaster recovery steps for the applications with the help of infrastructure teams.Perform fire drill operations on the Applications.Perform regular compliance adherence and monitoring activities like user access management & roles, maintain vulnerability scan results and fixes, following enterprise standards on data security.Follow SLA adherence and fulfill service request and resolve incidents.Define best practices and governance for Qlik Sense deploymentsExperience with Sense Monitoring Dashboard.Experience in performance tuning of Qlik Sense server.Working knowledge of clustering/multi node installation of Qlik Sense in AWS VM environment.Working knowledge of AWS with load balancer, DNS and firewallMentor developers and provide technical documentation, developer guides and assist in trouble shooting.Participate in the design reviews to ensure conformance to design/certification standardsMaintain knowledge on the latest Qlik Sense and other BI technology and best practicesEffectively communicate with client teams and internal teamExperience in NPrinting and Alerting is added advantageFormal Qlik Sense certification is a plusExperience with shell scripting is a plus",50624903750,05-06-2024,03-09-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"performance tuning, bi, dns, qlikview, aws, python, load balancing, oracle, data analysis, qlikview development, power bi, data warehousing, machine learning, business intelligence, sql server, sql, qlik nprinting, tableau, data modeling, vm, shell scripting",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements.  Must have skills : Google Cloud Platform Administration Good to have skills : Accenture Delivery Methods (ADM) Minimum  3  year(s) of experience is required Educational Qualification : Minimum 15 years of fulltime education  Summary :As a Google Cloud Platform Administrator, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve working with the Google Cloud Platform, ensuring the smooth functioning of applications, and providing technical support to stakeholders.  Roles & Responsibilities: Design, build, and configure applications to meet business process and application requirements using Google Cloud Platform. Ensure the smooth functioning of applications by monitoring and maintaining the Google Cloud Platform infrastructure. Provide technical support to stakeholders, including troubleshooting and resolving issues related to the Google Cloud Platform. Collaborate with cross-functional teams to ensure the successful delivery of projects and applications. Professional & Technical Skills: Must To Have Skills:Experience in Google Cloud Platform Administration. Good To Have Skills:Experience with Accenture Delivery Methods (ADM). Strong understanding of cloud computing concepts and technologies. Experience with infrastructure automation tools such as Terraform or Ansible. Experience with containerization technologies such as Docker and Kubernetes. Additional Information: The candidate should have a minimum of 3 years of experience in Google Cloud Platform Administration. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful solutions. This position is based at our Mumbai office. Qualification Minimum 15 years of fulltime education",2.41E+11,24-05-2024,22-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"google cloud platform administration, docker, ansible, terraform, cloud computing, kubernetes, python, vmware, microsoft azure, application development, sql, automation tools, java, gcp, linux, microsoft windows, troubleshooting, mysql, aws, jira",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Application Developer,"Project Role : Application Developer Project Role Description : Design, build and configure applications to meet business process and application requirements. Must have skills : Hadoop Administration Good to have skills : NA Minimum  3  year(s) of experience is required Educational Qualification : Graduate Project Role :Application Developer Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have Skills :Hadoop AdministrationGood to Have Skills : No Technology SpecializationJob Requirements :Key Responsibilities :Responsible for Deployment and Configuration Of Various Hadoop Ecosystems ComponentsManaging and administering Hadoop servicesInvolved in cluster and capacity planning, Deployment, Performance tuning and benchmarking of the Hadoop clusterSkilled in Commissioning of nodes in the clusters, Creation of snapshots, setting Quota for users Technical Experience :Back up and Disaster Recovery BDRSetting up Hadoop security using authentication mechanism of AD Kerberos, Integration LDAP and AuthorizationSecured multiple Hadoop clusters with Kerberos, sentry, ACL, KMS, SSL/TLSTroubleshooting service failures like hdfs, yarn, spark, hive and hbase Professional Attributes :Resolving errors and warnings in Cloudera ManagerBalancing the clusterKMS and KTS for data EncryptionImporting and exporting the data using sqoop Educational Qualification:GraduateAdditional Info:Only Male candidates, Rotational shift, only Mumbai candidates, no scope for WFH & Hybird mode not allowed. Qualification Graduate",2.01E+11,20-05-2024,18-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"hadoop, performance tuning, application development, node, hadoop administration, hive, css, web services, disaster recovery, hibernate, ssl, jquery, sql, spring, react.js, java, ldap, spark, j2ee, mysql, html, hbase, rest, python, oracle, javascript, sql server, angular",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Ceph Data Migration Engineer,"The IBM Storage Software team is looking for a Ceph L3 Engineer to join us in Bangalore, India. In this role, you'll be required to have a deep background in Linux storage system administration and development. In this role, you will deliver an outstanding sustaining experience to our Ceph and its layered product support teams and customers. You will be an escalation point for critical customer issues and a mentor across the globe for L2 Support Engineers and partners. As a Ceph L3 Engineer, you will need to have experience with large and complex systems and be able to think creatively, learn new things, and adapt to a constantly changing environment. Your Role and Responsibilities You will be part of the Storage Development Business of the Infrastructure organization with the following key responsibilities: Responsibilities: Develop and maintain data migration strategy, processes, tools, and techniques to ensure that the data is migrated accurately, securely, and efficiently.  Design, implement, and maintain migration scripts. Troubleshoot and resolve migration automation or manual failures promptly. Develop and execute comprehensive migration plans to migrate data to Ceph from different storage systems. Analyze automation, and manual procedures, identify improvement areas, and collaborate with teams for enhancements. Required Technical and Professional Expertise 5- 10 years of experience in storage and automation Proficiency in data migration development tools. Strong scripting (Python, Bash, etc.) and programming skills (C/C++). Experience with storage protocols like iSCSI, and NVMe/TCP. Experience with VMware virtualization, and storage.  In-depth knowledge of Ceph storage architecture, components, and deployment. Hands-on experience with configuring and tuning Ceph clusters. Understanding of RADOS, CephFS, and RBD (Rados Block Device). Ability to design and implement end-to-end automation solutions. Proven ability to troubleshoot and debug complex system issues. Preferred Technical and Professional Expertise Knowledge of Opensource development and working experience in Opensource projects. Certifications related to Ceph storage and performance testing are a plus. Familiarity with cloud platforms (AWS, Azure, GCP) and their storage services. Experience with container orchestration tools such as Kubernetes. Knowledge of monitoring tools (Prometheus, Grafana) and logging frameworks. Ability to work effectively in a collaborative, cross-functional team environment. Knowledge of AI/ML, exposure to Gen AI",3.11E+11,31-05-2024,29-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, rbd, bash, prometheus, aws, kubernetes, python, performance testing, vmware, networking, data migration, artificial intelligence, docker, ansible, grafana, gcp, troubleshooting, mysql, openstack",-,9am-6pm,"Full Time, Permanent",IBM,Organization,IBM,https://www.naukri.com/hotjobs/images/v3/ibmsep14.gif,"Pune, Bengaluru","Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Data Platform Engineer,"Project Role : Data Platform Engineer Project Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Must have skills : Microsoft Azure Analytics Services Good to have skills : UNIX Shell Scripting, Python (Programming Language), Hadoop Administration Minimum  3  year(s) of experience is required Educational Qualification : BTeck/BCA/BE/MCA Summary :As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, and utilizing Microsoft Azure Analytics Services to develop and maintain the data platform.  Roles & Responsibilities: Assist with the blueprint and design of the data platform components. Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Develop and maintain the data platform using Microsoft Azure Analytics Services. Troubleshoot and resolve issues related to the data platform. Implement security measures to protect the data platform from unauthorized access. Professional & Technical Skills: Must To Have Skills:Experience with Microsoft Azure Analytics Services. Good To Have Skills:UNIX Shell Scripting, Python (Programming Language), Hadoop Administration. Strong understanding of data platform components and architecture. Experience with data integration and data modeling. Knowledge of database management systems and SQL. Familiarity with cloud computing and virtualization technologies. Additional Information: The candidate should have a minimum of 3 years of experience in Microsoft Azure Analytics Services. The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions. This position is based at our Mumbai office. Qualification BTeck/BCA/BE/MCA",50624905581,05-06-2024,03-09-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"analytics services, azure analytics, microsoft azure, sql, hadoop administration, c#, hive, python, azure data lake, power bi, azure data factory, machine learning, sql server, unix shell scripting, data modeling, hadoop, cloud computing, ssis, data integration",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Kubernetes Platform Engineer, 6+ years of experience in industry performing Kubernetes Platform Engineering.   Must Experience and strong knowledge on container technology platforms such as Red Hat  OpenShift / Azure Kubernetes / AWS EKS etc is a must require.   Experience working with Azure and AWS cloud platform is must. Preferably AWS.   Experience with Agile and DevOps development methodologies.  ??Automate our container infrastructure hosted on Cloud.  Good to have an experience in implementing Service mesh Administration on Kubernetes  platform and hands-on with mesh capabilities. ??Experience with monitoring and observability tools such as Splunk and Grafana. ??Experience with configuration management systems such as Terraform and Ansible is a  must require. ??Experience working with continuous integration/continuous deployment tools (e.g. GitHub  Actions),50624008872,05-06-2024,03-09-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,Software Product,"Docker, Openshift, Eks, Kubernates, AWS, Terraform, Redhat Openshift, Ansible, mesh",-,9am-6pm,"Full Time, Permanent",Recruise India Consulting,Organization,Recruise India Consulting,https://img.naukimg.com/logo_images/groups/v1/3362864.gif,Bengaluru,Bengaluru,-,-,-,9-19 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Kubernetes Platform Engineer,"  Kubernetes Platform Engineer- 9-11yrs They are looking for candidates who are very good in Kubernetes Platform not DevOps engineers. Must have- Kubernetes admin platform exp, containerization, docker, splunk or Grafana, AWS preferred azure also is ok, Redhat, redshift Location:Bangalore",50624006137,05-06-2024,03-09-2024,EducationalOccupationalCredential,108,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"containerization, Kubernetes admin platform exp, docker, AWS preferred azure also is ok, Redhat, redshift, splunk or Grafana",-,9am-6pm,"Full Time, Permanent",Recruise India Consulting,Organization,Recruise India Consulting,https://img.naukimg.com/logo_images/groups/v1/3362864.gif,Bengaluru,Bengaluru,-,-,-,25-30 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
AWS Cloud Practice Engineer - III / IV,"Rackspace Technology is looking for AWS Cloud Practice Engineer - III / IV to join our dynamic team and embark on a rewarding career journey      As an AWS Cloud Practice Engineer, you will be a key member of our cloud services team responsible for designing, implementing, and managing AWS cloud solutions for our clients      You will leverage your expertise in AWS technologies to architect scalable, reliable, and cost-effective cloud environments that meet our clients' business objectives      This role requires a deep understanding of AWS services, best practices, and infrastructure as code (IaC) principles, as well as strong communication and collaboration skills to work effectively with clients and internal teams        Key Responsibilities:        Cloud Architecture and Design:Collaborate with clients to understand their business requirements and design AWS cloud architectures that align with their goals and objectives      Develop comprehensive cloud architecture diagrams, specifications, and implementation plans      Design solutions that leverage AWS services such as EC2, S3, RDS, Lambda, VPC, and IAM to meet scalability, reliability, and security requirements      Infrastructure as Code (IaC):Implement infrastructure as code using tools such as AWS CloudFormation, Terraform, or AWS CDK to automate the provisioning and management of cloud resources      Define reusable templates and modules to streamline the deployment of infrastructure components and ensure consistency across environments      Manage version control and configuration management of infrastructure code using Git or similar tools      Cloud Implementation and Migration:Deploy and configure AWS cloud environments according to design specifications and best practices      Lead cloud migration projects, including lift-and-shift migrations, re-platforming, and cloud-native application development      Coordinate with client stakeholders and internal teams to ensure successful migration of workloads to the AWS cloud      Security and Compliance:Implement security best practices and compliance standards in AWS environments, including network security, identity and access management, and data encryption      Perform security assessments and audits of AWS architectures to identify and remediate security vulnerabilities      Monitor and respond to security incidents and events in AWS using native and third-party security tools    ",1.50E+11,15-03-2024,13-06-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"linux system administration, kubernetes, continuous integration, python, github, ubuntu, redhat linux, microsoft azure, docker, ansible, amazon ec2, git, apache, aws cloud, linux administration, devops, linux, jenkins, shell scripting, aws, rackspace, cloud computing",-,9am-6pm,"Full Time, Permanent",Rackspace Technology,Organization,Rackspace Technology,https://img.naukimg.com/logo_images/groups/v1/3080116.gif,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer,"Rethinksoft is looking for Python Developer to join our dynamic team and embark on a rewarding career journey.      Coordinating with development teams to determine application requirements.     Writing scalable code using Python programming language.     Testing and debugging applications.     Developing back-end components.     Integrating user-facing elements using server-side logic.     Assessing and prioritizing client feature requests.     Integrating data storage solutions.     Reprogramming existing databases to improve functionality.     Developing digital tools to monitor online traffic.     Write effective, scalable code     Develop back-end components to improve responsiveness and overall performance     Integrate user-facing elements into applications     Test and debug programs     Improve functionality of existing systems     Implement security and data protection solutions     Assess and prioritize feature requests     Coordinate with internal teams to understand user requirements and provide technical solutions.   ",70624502625,07-06-2024,05-09-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,Python,-,9am-6pm,"Full Time, Permanent",Rethinksoft,Organization,Rethinksoft,-,Ahmedabad,Ahmedabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Associate,"     Collaboratively build and maintain Infrastructure for the Internal stakeholders and external clients (using Terraform)     Administer Linux Servers (RedHat and Ubuntu)     Support internal Dockerized platforms for the Internal analytics users (Posit Containerized products)     Administer Linux Servers (RedHat and Ubuntu)     Ready to work in the 2 PM to 11 PM shift.           What You Will Need   :         Ready to work in the 2 PM to 11 PM shift.     Candidates should be from computer background (B. Tech Computer Science or B.Sc CS, BCA etc)     Beginner level understanding of Cloud platforms (Azure or AWS)     Candidate must be Linux trained (RedHat or Ubuntu)           What Would Be Nice To Have   :         Cloud certification     System Administrator level experience with Linux (Red Hat or CentOS preferred)     RedHat Certified Engineer (RHCE) certification     Knowledge in DevOps tools such as Terraform, Docker, Kubernetes.      Knowledge in version control     ",2.71E+11,27-05-2024,25-08-2024,EducationalOccupationalCredential,1,Engineering - Software & QA,Data Platform Engineer,Management Consulting,"Ubuntu, Redhat, Redhat Linux, Version control, Business analysis, Staffing, Analytics, RHCE, System Administrator",-,9am-6pm,"Full Time, Permanent",Guidehouse,Organization,Guidehouse,https://img.naukimg.com/logo_images/groups/v1/4614147.gif,Thiruvananthapuram,Thiruvananthapuram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Python Developer,"GKTCS Innovations Pvt. Ltd. is looking for Python Developer to join our dynamic team and embark on a rewarding career journey      Coordinating with development teams to determine application requirements.      Writing scalable code using Python programming language.      Testing and debugging applications.      Developing back-end components.      Integrating user-facing elements using server-side logic.      Assessing and prioritizing client feature requests.      Integrating data storage solutions.      Reprogramming existing databases to improve functionality.      Developing digital tools to monitor online traffic.      Write effective, scalable code      Develop back-end components to improve responsiveness and overall performance      Integrate user-facing elements into applications      Test and debug programs      Improve functionality of existing systems      Implement security and data protection solutions      Assess and prioritize feature requests      Coordinate with internal teams to understand user requirements and provide technical solutions.    ",2.81E+11,28-05-2024,26-08-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,E-Learning / EdTech,Python,-,9am-6pm,"Full Time, Permanent",Gktcs Innovations,Organization,Gktcs Innovations,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Staff Cloud Platform Engineer,"   We are seeking Platform Engineers with proven expertise in the design and building of large-scale Cloud Infrastructure Platforms for Datalake to join our team and help us deliver the foundation on which Calix products can thrive. You will help us achieve our mission through innovative solutions.                 ?       Responsibilities:             Design, build, and maintain a scalable and reliable cloud-native data lake infrastructure using Presto, Alluxio, S3, Hive and Spark technologies.     Implement integration with Zookeeper for coordination and leader election across Datalake components.     Configure Presto to access and query data stored in Alluxio and S3, optimizing performance and resource utilization.     Deploy and manage Spark clusters for data processing and analytics tasks, leveraging Alluxio and S3 as storage layers.     Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and implement solutions that meet business objectives.     Develop and maintain automated workflows for data ingestion, processing, and transformation using Spark and other relevant tools.     Implement monitoring and alerting solutions to ensure the health and performance of the data lake infrastructure.     Provide technical guidance and support to junior team members, fostering a culture of continuous learning and improvement.             Requirements:             Bachelors degree in Computer Science, Engineering, or related field; or equivalent work experience.     10+ years of experience as a Data Engineer or similar role, with expertise in building and managing cloud-native data lake infrastructure.     Strong proficiency in Presto, Alluxio, S3, Spark, Hive and Zookeeper, with hands-on experience in designing and implementing solutions using these technologies.     Experience with Spark Administration, Hive administration, Presto administration and AWS resource management.     Experience with monitoring of distributed systems.     Experience with infrastructure as code tools such as Terraform/Ansible for provisioning and managing cloud resources.     Experience with container orchestration platforms such as Kubernetes for deploying and managing Datalake components.     Solid understanding of distributed computing principles, data modelling, and database technologies.     Experience with serverless architecture would be plus like Lambda and EMR     Experience in Observability and Monitoring in AWS.     Fast learner and able to troubleshoot complex scenarios while process large volumes of data (Terabytes and Petabytes)     Excellent problem-solving skills, with the ability to troubleshoot complex issues and implement effective solutions.     Strong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.     Having Databricks experience will be an added advantage.             Optional Skills:         JVM tuning.      Alluxio cluster management.      ",1.71E+11,17-05-2024,15-08-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"hive, spark, Cloud, Data processing, Resource management, Distribution system, Monitoring, Analytics",-,9am-6pm,"Full Time, Permanent",Calix,Organization,Calix,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Lead Python Developer,"       Design, develop, and maintain the server-side logic of web applications. Implement scalable and efficient components and APIs.         Work with relational and/or NoSQL databases to store and retrieve data efficiently.          Design and optimize database schemas for performance and scalability.         Integrate third-party APIs and web services to enhance application functionality.         Develop custom APIs to facilitate communication between front-end and back-end systems.         Implement security best practices to protect against common web application vulnerabilities.         Conduct regular security audits and implement necessary measures. Optimize server-side code for maximum speed and scalability.         Monitor and troubleshoot performance issues. Test server-side components to identify and fix bugs.         Collaborate with QA engineers for comprehensive testing.         Create clear and comprehensive documentation for back-end systems and APIs.         Manage code versioning and collaboration using Git       Must have:            Proficiency in Python server-side asynchronous programming.         Experience in developing on GCP         Proficient knowledge of FastApi, Flask, or Django.         Understanding of TDD and PyTest.         Experience with database systems, e.g. PostgreSQL.         Proven experience in back-end development and building scalable web applications.         Strong analytical and problem-solving skills with attention to detail.         Effective communication skills to collaborate with cross-functional teams.         Ability to adapt to changing project requirements and technologies.           Nice To Have :            Solid understanding of cache and Redis.         Experience with Docker and Kubernetes.       ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"Front end, GIT, NoSQL, TDD, Postgresql, Django, Analytical, Artificial Intelligence, Business intelligence, Python",-,9am-6pm,"Full Time, Permanent",Lingaro Sp Z Oo,Organization,Lingaro Sp Z Oo,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Platform Engineer - AWS Cloud,"Responsibilities  : - Administer systems including configuration, monitoring and maintenance of Application servers; - Able to discern stakeholder needs, effectively communicate development plans, and track progress milestones - Work closely with IT Platform Manager to implement and support infrastructure for key project work and company objectives - Administration and maintenance of existing on premise and cloud infrastructure - Excellent organizational and time management skills to handle multiple tasks simultaneously - Analyze and improve system performance; - Excellent communication skills with both technical and non-technical audiences. - Resolve anomalies observed in collaboration with other teams - Deployments and migrations of projects - Ensure Infrastructure maintains its high standard of security by regularly carrying out mitigation and preventive maintenance tasks - Keep abreast of best practices and develop a leading-edge expertise on supported technologies. Qualifications  : - Bachelor's or Graduate's Degree in computer science, engineering or information systems. - Candidate should have 6 - 10 years of platform experience with at least 2-3 years of cloud experience. - Good Knowledge of Big Data systems (AWS), including Hadoop, HDFS, Hive - Scripting Experience with Apache Spark, Python, Shell - Proficient in installing Apache Airflow, configuring, and monitoring Airflow cluster - Proficient in Manage application security - Proficient with developing guidelines for Airflow clusters and DAG's - Proficient in building CICD Pipeline with Git & Jenkins - Familiar in AWS clusters, EC2, S3, EFS & other components - Familiar with SQL and writing queries - Familiar in BI and data science tools such as Tableau, Superset, Jupyter. Nice to Have : - Familiar in Tableau Administration & Vertica administration - Experience in Administration of Dremio Application Technical skills :  Infrastructure administration, System performance analysis, Deployment and migrations, Security maintenance, Big Data systems (AWS, Hadoop, HDFS, Hive), Apache Spark, Python., Shell scripting, Apache Airflow (installation, configuration, monitoring), Application security management, CICD Pipeline (Git, Jenkins), AWS clusters (EC2, S3, EFS), SQL and query writing, Tableau, Superset, Jupyter,. Dremio Application Administration, Vertica Administration (Nice to have), Tableau Administration (Nice to have)",3.11E+11,31-05-2024,29-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"AWS Cloud, Infrastructure administration, Application security management, Shell Scripting, system configuration, Big Data, CI/CD, AWS, Python, Apache Spark",-,9am-6pm,"Full Time, Permanent",Mobile Programming,Organization,Mobile Programming,https://img.naukimg.com/logo_images/groups/v1/1403926.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer
Azure Cloud Engineer," 1. Implementing, managing, and monitoring identity, governance, storage, compute, and virtual networks in a cloud environment, plus provision, size, monitor, and adjust resources, when needed.    2. Planning and Implementation/deployment of Azure Infrastructure.    3. Components like Azure AD, Microsoft Intune, VPN, Backup, etc. Configuration of Infrastructure.    4. Test and debug various Azure Infrastructure Scenarios (DC Migration, WVD,etc.).    5. Work with Senior Team members to achieve goals for the project.    6. Document the activities and operational procedures.    7. Interface independently with various groups of customers and provide support and service.        Requirements        1. Work experience as System Administrator and Azure Admin.    2. Azure Fundamentals certification completion.    3. Troubleshooting abilities.    4. Ability to learn new emerging technologies.    5. Good communication skills (Written as well as Spoken).        Qualification        1. Min 6 months and Max 2 years of experience with respect to Azure.    2. Overall experience 1-3 years.    3. Good knowledge of Cloud Technologies and related emerging Technologies.    4. BE, BSc in Computer Science, or relevant field.        Microsoft Certifications        Product Certification (AZ-104) will be an added advantage. Certification is not mandatory and TMTPL can provide training for the same.  ",2.01E+11,20-05-2024,18-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Data Platform Engineer,IT Services & Consulting,"microsoft azure, intune, cloud technologies, azure cloud, azure devops, azure infrastructure, azure networking, sql azure, active directory, vpn, azure iaas, writing, paas, powershell, iaas, azure active directory, aws, cloud computing, communication skills",-,9am-6pm,"Full Time, Permanent",Themaverics Technologies,Organization,Themaverics Technologies,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Data Platform Engineer

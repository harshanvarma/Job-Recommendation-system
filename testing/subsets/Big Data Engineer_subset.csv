Hadoop Developer,"Hadoop, Pyspark Scala, Hive",30524007181,03-05-2024,01-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Pyspark, Hive, Hadoop, SCALA",-,9am-6pm,"Full Time, Permanent",Tata Consultancy Services (TCS),Organization,Tata Consultancy Services (TCS),https://img.naukimg.com/logo_images/groups/v1/223346.gif,"Hyderabad, Bangalore Rural, Chennai","Hyderabad, Bangalore Rural, Chennai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"Resources must have at least 5 years of coding experience in Scala, Spark, Python and BigData/Hadoop.  Required skills are  Scala, Spark, Hadoop, Big Data, Python. Must be good in communication. ",40524003632,13-05-2024,11-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Analytics / KPO / Research,"Scala, Hadoop, Big Data, Spark, Python",-,9am-6pm,"Full Time, Permanent",Collabera Technologies,Organization,Collabera Technologies,https://img.naukri.com/logo_images/v3/395661.gif,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Redis Developer,  Job Description:  End to End Redis knowledge. Redis hands on        experience. Java is secondary skills.   Redis hands on experience   Java   programming skills,80524004634,08-05-2024,06-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Java Programming, Redis, Redis Database",-,9am-6pm,"Full Time, Permanent",Lancesoft,Organization,Lancesoft,https://img.naukimg.com/logo_images/groups/v1/24048.gif,"Hyderabad, Mangaluru, Bengaluru","Hyderabad, Mangaluru, Bengaluru",-,-,-,15-20 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Apache Spark-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Apache Spark  Good to have skills :Software Design & Solutions Minimum 5 year(s) of experience is required  Educational Qualification :Graduate Key Reponsibilities :Engineering solutions to aggregate and automate large scale data flows from varying sources Build ETLs from internal and external sources to provide insights into the business Help continually improve ongoing reporting and analysis processes, automating or simplifying operational support for stakeholders Create and maintain workflow/technical documents for all the development activities Technical Experience : Primary/must have skills Scala, functional programming, Spark, Kafka, Argo, Airflow, Spark Streaming, CDC, Informatica, Vertica, Python , SQL 2 Secondary / nice to have skills o Ability to understand logical and physical data models, big data storage architecture, data modelling methodologies, metadata management AWS Serverless native services Professional Attributes :Self-oriented with strong interpersonal skills o Strong Team Player o Strong Written and Oral communication skills o Good organizational and troubleshooting skills o Good time-management and self-starter skills are also desirable",1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"functional, scala, spark, airflow, kafka, hive, vertica, software design, written and oral communication, apache pig, sql, spark streaming, apache, java, data modeling, hadoop, big data, hbase, python, cdc, oracle, oozie, mapreduce, troubleshooting, sqoop, aws, informatica",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Apache Spark-Application Developer,"Project Role :Application Developer  Project Role Description :Design, build and configure applications to meet business process and application requirements. Must have skills :Apache Spark  Good to have skills :Apache Hadoop Minimum 5 year(s) of experience is required  Educational Qualification :15 year full time education Key Reponsibilities :A:To Design, build and configure applications to meet business process and application requirements B:Develop, Test and Build Server side code Technical Experience : A Must to have:Apache Spark B Good to have : Hadoop, HDFS and Scala Professional Attributes :A:Ability to perform multiple tasks concurrently B:Excellent communication skills written and verbal, and interpersonal skills C:Excellent organizational and time management skills",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Apache Spark, hive, cloudera, python, scala, oozie, apache pig, impala, sql, nosql, java, mapreduce, linux, kafka, flume, mysql, shell scripting, hadoop, sqoop, big data, aws, hbase, unix",-,9am-6pm,"Full Time, Permanent",Accenture,Organization,Accenture,https://www.naukri.com/hotjobs/images/v3/Accen_nov20.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Opportunity | Java/scala/python + Spark | Tavant India,"Dear candidate,  We found your profile suitable for our current opening, please go through the below JD for better understanding of the role,  Job Description : Role  : SSE/TL Exp  : 3 - 8 years Work Location  : Bangalore / Hyderabad /Noida/Kolkata/Pune Mode of work  : Hybrid (3 days WFO)   Required Skills : Design and develop Java-based applications that integrate with Big Data technologies such as Spark, Hadoop, and EMR. Implement and optimize data processing workflows on AWS cloud infrastructure. Develop and maintain scalable, distributed systems for data ingestion, transformation, and analysis. Collaborate with data scientists and analysts to understand data requirements and implement efficient solutions. Ensure code quality, performance, and scalability through unit testing, code reviews, and continuous integration. Troubleshoot and debug issues in production environments, providing timely resolutions. Stay updated on emerging technologies and industry trends related to Big Data and cloud computing. Skills and Qualifications Bachelor's degree in Computer Science, Engineering, or related field. 3+ years of professional experience in software development using Java/Scala/Python Strong proficiency in working with Big Data technologies such as Spark, Hadoop, and EMR. Hands-on experience with AWS cloud services including EC2, S3, Lambda, and EMR. Experience with distributed computing frameworks and parallel processing. Knowledge of containerization technologies such as Docker and Kubernetes is a plus. Excellent problem-solving skills and ability to work in a fast-paced environment. Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams. Please check below link for organisation details, https://www.tavant.com/ If interested, please drop your resume to dasari.gowri@tavant.com   Regards   Dasari Krishna Gowri Associate Manager | Talent Acquisition www.tavant.com",2.00E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Java, Spark, SCALA, EMR, Python",-,9am-6pm,"Full Time, Permanent",Tavant Technologies,Organization,Tavant Technologies,https://www.naukri.com/hotjobs/images/v3/tavtech123.gif,"Noida, Hyderabad/Secunderabad, Bangalore/Bengaluru","Noida, Hyderabad/Secunderabad, Bangalore/Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Trainee Big Data Consultant | Leading IT Co. | Faridabad,"- Access data stored in databases/warehouses/flat files. - Create/test/deploy intuitive & interactive, analytical dashboards - Create algorithms in Python/R for advanced analytics. - Create data governance policies like RLS to manage data security Require",2.70E+11,27-04-2024,26-07-2024,EducationalOccupationalCredential,1,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data Analytics, Big Data, Data Analytics, Big Data Consultant, Python, Data Transformation, Big Data Testing, Data Engineering, Power Bi, Sql Query Writing, Ms Power Bi, SQL Queries, Deployment, Data Science, RLS, Data Analysis, Big Data Engineer, WFH, Data Security, Data Modeling",-,9am-6pm,"Full Time, Permanent",Hindco Recruitment Consultants,Organization,Hindco Recruitment Consultants,-,"Panipat, Faridabad, Gurugram","Panipat, Faridabad, Gurugram",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Trainee Big Data Consultant | Leading IT Co. | Faridabad,"- Access data stored in databases/warehouses/flat files. - Create/test/deploy intuitive & interactive, analytical dashboards - Create algorithms in Python/R for advanced analytics. - Create data governance policies like RLS to manage data security Require",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,1,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data Analytics, Big Data, Data Analytics, Big Data Consultant, Python, Data Transformation, Big Data Testing, Data Engineering, Power Bi, Sql Query Writing, Ms Power Bi, SQL Queries, Deployment, Data Science, R, RLS, Data Analysis, Big Data Engineer, WFH, Data Security, Data Modeling",-,9am-6pm,"Full Time, Permanent",Hindco Recruitment Consultants,Organization,Hindco Recruitment Consultants,-,"Panipat, Faridabad, Gurugram","Panipat, Faridabad, Gurugram",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"Data Engineer (Kafka,Spark Streaming, Pyspark, AWS)","Responsible for designing, implementing, & maintaining large-scale data processing systems using cutting-edge technologies. Design, develop, & deploy scalable & fault-tolerant data processing pipelines using PySpark, Spark Streaming, Kafka. Required Candidate profile Hands-on exp with distributed computing frameworks such as PySpark, Spark Streaming, Kafka & AWS EMR. Certifications in big data. Understanding of big data concepts, data modeling, and ETL processes.",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Pyspark, Kafka, Big Data, Spark Streaming, Aws Emr, SQL, Python",-,9am-6pm,"Full Time, Permanent",Lagozon Technologies,Organization,Lagozon Technologies,-,"Noida, New Delhi, Faridabad","Noida, New Delhi, Faridabad",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Engineer - Big Data,"           American Express is looking for energetic, successful and highly skilled Engineers to help shape our technology and product roadmap       Our Software Engineers not only understand how technology works, but how that technology intersects with the people who count on it every day     Today, innovative ideas, insight and new points of view are at the core of how we create a more powerful, personal and fulfilling experience for our customers and colleagues, with Big Data and batch/real-time analytical solutions using ground-breaking technologies to deliver innovative solutions across multiple business units         This Engineering role is based in our Global Risk and Compliance Technology organization and will have a keen focus on platform modernization, bringing to life the latest technology stacks to support the ongoing needs of the business as well as     compliance against global regulatory requirements                                   Qualifications:                   Support the Compliance and Operations Risk big data delivery team in India to lead and assist in the design and actual development of applications.             Responsible for specific functional areas within the team, this involves project management and taking business specifications.             The individual should be able to independently run projects/tasks delegated to them.                               Technical Skills:                   Bachelor degree in Engineering or Computer Science or equivalent              2 to 5 years experience is required             Experience on Big Data (Spark Core and Hive) is good to have             Familiar with GCP offerings, experience building data pipelines on GCP a plus             Hadoop Architecture, having knowledge on Hadoop, Map Reduce, Hbase.             UNIX shell scripting experience is good to have             Creative problem solving (Innovative)             Benefits include:           Competitive base salaries          Bonus incentives          Support for financial-well-being and retirement          Comprehensive medical, dental, vision, life insurance, and disability benefits (depending on location)          Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need          Generous paid parental leave policies (depending on your location)          Free access to global on-site wellness centers staffed with nurses and doctors (depending on location)          Free and confidential counseling support through our Healthy Minds program          Career development and training opportunities     ",80524501967,08-05-2024,06-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Career development, GCP, Project management, Analytical, Finance, Open source, big data, Unix shell scripting",-,9am-6pm,"Full Time, Permanent",Resy,Organization,Resy,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Architect - Big Data,"     As a Solutions Architect (Analytics, AI, Big Data, Public Cloud), you will guide the technical evaluation phase in a hands-on environment throughout the sales process     You will be a technical advisor internally to the sales team, and work with the product team as an advocate of your customers in the field     You will help our customers to achieve tangible data-driven outcomes through the use of our Databricks Lakehouse Platform, helping data teams complete projects and integrate our platform into their enterprise Ecosystem     you'll grow as a leader in your field, while finding solutions to our customers biggest challenges in big data, analytics, data engineering and data science problems         The impact you will have:         You will be a Big Data Analytics expert on aspects of architecture and design     Lead your clients through evaluating and adopting Databricks including hands-on Spark programming and integration with the wider cloud ecosystem     Support your customers by authoring reference architectures, how-tos, and demo applications      Integrate Databricks with 3rd-party applications to support customer architectures     Engage with the technical community by leading workshops, seminars and meet-ups     Together with your Account Executive, you will form successful relationships with clients throughout your assigned territory to provide technical and business value         What we look for:         Consulting / customer facing experience , working with external clients across a variety of industry markets     Core strength in either data engineering or data science technologies     8+ years of experience demonstrating technical concepts, including demos, presenting and white-boarding      8+ years of experience designing architectures within a public cloud (AWS, Azure or GCP)     6+ years of experience with Big Data technologies, including Spark, AI, Data Science, Data Engineering, Hadoop, Cassandra, and others     Coding experience in Python, R, Java, Spark or Scala         Benefits         Private medical insurance     Accident coverage     Employees Provident Fund     Equity awards     Paid parental leave     Gym reimbursement     Annual personal development fund     Work headphones reimbursement     Business travel insurance     Mental wellness resources     ",2.00E+11,20-03-2024,18-06-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Consulting, Sales process, Coding, SAN, Medical insurance, Python, Travel insurance, big data, GCP",-,9am-6pm,"Full Time, Permanent",Databricks,Organization,Databricks,https://img.naukimg.com/logo_images/groups/v1/4128698.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"The Opportunity:   Looking for talented, motivated, and self-driven Big Data Engineers who specialized in data-driven projects and have an incredible experience to collect, transform and analyze data in order to improve the effectiveness of our products and create delightful experiences for customers. This is a great opportunity to work with a fast-growing US product-based company with a great work culture.  What You Will Do:    A Big Data Engineer is responsible for designing, building, and maintaining the infrastructure used to store and process large amounts of data.   They have to work with teams of data scientists, analysts, product, Support Engineers, and other teams to ensure that the data is easily accessible, reliable, and scalable format. If needed jump on production issues and try to resolve them ASAP.   This person should have a strong background in computer science and experience with big data technologies, such as S3, Data lakes, Spark, Glue, EMR, and Athena.   You will be an integral part of the development team examining requirements and designing optimal solutions.  Who You Are:    You are a 5-12 Years experienced professional having good working exposure in the following areas: a) RDBMS databases (Preferably MySQL having proficiency in Data Modeling and writing optimized SQL) b) Proficiency in at least one Programming Language (PHP, Scala, Python, Java) c) Data Engineering Stack (ETL tools & data pipelines, Apache Spark, Apache Airflow, NoSQL databases) d) AWS Services (AWS SDK, S3, Data Lakes, Aurora, Athena, EMR, Glue, Lambda, SQS)  Designing and implementing data pipelines to collect, process, and store large amounts of data from various sources.Building and maintaining a scalable, high-performance data platform with the capability of orchestrating various data pipelines with high configurability and data ingestion capabilities across multiple sources.   Developing algorithms and data models to clean, transform, and analyze data, and creating visualizations and reports to communicate findings.   Collaborating with data scientists and analysts to support their data needs and help them develop and test their models.   Troubleshooting and optimizing the data infrastructure to improve performance and reliability. To be successful in this role, a Big Data Engineer should have a strong understanding of computer science principles, such as algorithms, data structures, and software design.   They should also be familiar with big data technologies and have experience working with large datasets.   Good communication skills and the ability to work well in a team are also important.  Nice to Have:    Great learning attitude.   Eagerness to take ownership.   A global mindset.  Why Frequence?   Frequence is proud to be certified as a Great Place to Work and ranked as one of the USA's fastest-growing private companies by Inc. Magazine. Our people-first culture and distinctive mission set us apart from others in the industry.  As our company grows, so does our need to add sharp, ambitious, and talented people to our team. We're approachable, and inclusive, and encourage open dialogue within our community. This is a place where your voice matters. Join us and let's see what we can build together.  Some of the industry-leading benefits we offer include:   Competitive salary   Group Health Insurance Policy   Group Accident/Disability Insurance   Remote Work Options   Employee Referral Program   Food and Beverages   Stock option",30524006956,03-05-2024,01-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"RDBMS, MySQL, Airflow, Lambdas, Glue, Aws Aurora, Data Modeling, Spark, EMR, Athena",-,9am-6pm,"Full Time, Permanent",Frequence Software,Organization,Frequence Software,-,Pune,Pune,-,-,-,15-30 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Software Engineer/ Senior Software Engineer- Data Analytics,"  PubMatic Data platform is one of the biggest in tech industry with Peta Byte Scale data. Our cluster comprises of thousands of machines and multiple data centres spread across the globe . Given the super high data throughput and scale challenge many proven Big Data tools at times have failed for our use cases. With the help of our brilliant engineering team we have come up with many smart innovative ideas, sometimes employed anti patterns to build a highly scalable and robust data platform. As part of team expansion, were looking for strong Software Development Engineer (Data) to work with us to highly scalable data platforms and services.? Responsibilities: Build, design and, implement our highly scalable, fault-tolerant?big data platform to process?terabytes?of data?and provide customers with in depth analytics. Developing?Big Data?pipelines using modern technology stacks such as?Spark, Hadoop, Kafka, HBase, Hive, and Presto. Developing?analytics?applications using modern technology stacks such as?Java, Spring, Tomcat, Jenkins, REST APIs,?JDBC, Amazon Web Services, Hibernate. Building data pipeline?to?automate high volume collection and processing to provide?realtime data analytics. Customize reporting and analytics?platform?based on customer???requirements?from customers and deliver?scalable, production ready solutions. Lead multiple projects to?develop features for data processing and reporting platforms, collaborate with product managers, and cross functional teams. Collaborate with functional teams to build products to?deliver end-to-end products and features and fix bugs for better performance. Develop robust?& fault tolerant?systems?and?monitor implications of changes?on?data processing pipeline and performance. Leveraging?a broad range of data architecture strategies and proposing both data flows and storage solutions. Managing Hadoop map reduce and spark jobs?and?solving any ongoing issues with operating the cluster. Working closely with cross functional teams on?improving the availability and scalability of?large?data platforms and the functionality of PubMatic software. Participate in?Agile/Scrum processes?such as sprint planning, sprint retrospective, backlog grooming, user story management, and work item prioritization. Frequently discuss with product managers about the software features to include in PubMatic Data Analytics platform.? Support customer issues?over email?or?JIRA (bug tracking system),?provide updates, patches to customers to fix the issues. Perform code?and design reviews for code implemented by peers or as per the code review process. Requirements: 3 to 6 ?years coding experience in Java. Solid?computer science?fundamentals including data structure?and algorithm design, and creation of architectural specifications. Expertise in developing Implementation of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, documentation, build processes, automated testing, and operations. A passion for developing and maintaining a high-quality code and test base, and enabling contributions from engineers across the team. Expertise in big data technologies like Hadoop, Spark, Kafka, Snowflake etc would be an added advantage. Experience in developing and delivering large scale big data pipelines, real-time systems & data warehouses would be preferred. Demonstrated ability to achieve stretch goals in a very innovative and fast paced environment. Demonstrated ability to learn new technologies quickly and independently. Excellent verbal and written communication skills, especially in technical communications. Strong inter-personal skills and a desire to work collaboratively.",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,Software Product,"Java, SCALA, Hadoop, Spark, Spring, Snowflake",-,9am-6pm,"Full Time, Permanent",PubMatic,Organization,PubMatic,https://img.naukimg.com/logo_images/groups/v1/4658599.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Principal Software Engineer- Data Analytics," PubMatic (Nasdaq: PUBM) is an independent technology company maximizing customer value by delivering digital advertisings supply chain of the future.PubMatics sell-side platform empowers the worlds leading digital content creators across the open internet to control access to their inventory and increase monetization by enabling marketers to drive return on investment and reach addressable audiences across ad formats and devices. Since 2006, our infrastructure-driven approach has allowed for the efficient processing and utilization of data in real time. By delivering scalable and flexible programmatic innovation, we improve outcomes for our customers while championing a vibrant and transparent digital advertising supply chain. Position Description PubMatic Data platform is one of the biggest in tech industry with Peta Byte Scale data. Our cluster comprises of thousands of machines and multiple data centres spread across the globe . Given the super high data throughput and scale challenge many proven Big Data tools at times have failed for our use cases. With the help of our brilliant engineering team, we have come up with many smart innovative ideas, sometimes employed anti patterns to build a highly scalable and robust data platform. As part of team expansion, were looking for strong Software Development Engineer (Data) to work with us on highly scalable data platforms and services Responsibilities: Build, design and implement our highly scalable, fault-tolerant, highly available big data platform to process?terabytes?of data?and provide customers with in-depth analytics. Developing?Big Data?pipelines using modern technology stack such as?Spark, Hadoop, Kafka, HBase, Hive, Presto etc. Developing?analytics?application ground up using modern technology stack such as?Java, Spring, Tomcat, Jenkins, REST APIs,?JDBC, Amazon Web Services, Hibernate. Building data pipeline?to?automate high-volume?data?collection and processing to provide?real-time data analytics. Customize PubMatic???reporting and analytics?platform?based on customer???requirements?from customers and deliver?scalable, production-ready solutions. Lead multiple projects to?develop features for data processing and reporting platform, collaborate with product managers, cross-functional teams, other stakeholders and ensure successful delivery of projects. Use various mechanisms established to?fetch data from?different external data sources?and reconcile?them?with PubMatic???processed data. Collaborate with functional teams to build products to?deliver end-to-end products and features and fix bugs for better performance. Develop robust?& fault-tolerant?systems?and?monitor implications of changes?on?data processing pipeline and performance. Leveraging?a broad range of PubMatic?? data architecture strategies and proposing both data flows and storage solutions. Managing Hadoop map reduce and spark jobs?&?solving any ongoing issues with operating the cluster. Working closely with cross functional teams on?improving availability and scalability of?large?data platform and functionality of PubMatic software. Expertise?in developing?Implementation of professional?software?engineering best practices for the full?software?development life cycle, including coding standards, performing code reviews, committing to GitHub, preparing documents in Confluence, continuous delivery using Jenkins, automated testing, and operations. Participate in?Agile/Scrum processes?such as sprint planning, sprint retrospective, backlog grooming, user story management, work item prioritization, etc. Frequently discuss with product managers about the software features to include in PubMatic Data Analytics platform. Understand the technical aspects customer requirement from product managers. Keep in regular touch with?quality?engineering?team which?ensure?the quality of the platforms/products and performance SLAs?of java based micro services and spark-based data pipeline. Support customer issues?over email?or?JIRA (bug tracking system),?provide updates, patches to customers to fix the issues. Discuss with technical writing team?about the technical documents that are published?on documentation portal. Perform code?and design reviews for code implemented by peers or as per the code review process. Requirements: 5+ years of proven experience in designing, implementing, and delivering complex, scalable, and resilient platform and services. Experience in building high throughput big data platforms and systems. Hands-on experience in big data technologies (Spark/Kafka/Spark streaming) and other open-source data technologies. Experience in OLAP (Snowflake, Vertica or similar) would be an added advantage. Ability to understand vague business problems and convert into working solutions. Excellent spoken and written interpersonal skills with a collaborative approach. Dedication to developing high-quality software and products. Curiosity to explore and understand data is a strong plus. Deep understanding of Big-Data and distributed systems (MapReduce, Spark, Hive, Kafka, Oozie, Airflow)",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,Software Product,"Java, Hive, SCALA, Hadoop, Spark, Mapreduce, Hdfs, HBase",-,9am-6pm,"Full Time, Permanent",PubMatic,Organization,PubMatic,https://img.naukimg.com/logo_images/groups/v1/4658599.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Sr.Software Engineer - Java & Big Data,"   Must have 3 to 8 years of strong experience in Java J2EE technologies         Solid experience in fundamentals including Data structures, Algorithms object-oriented programming         Expertise in big data technology stacks such as Hadoop, Spark, Kafka, HBase , Hive         Expertise in building highly scalable big day platforms to process terabytes of data.         Expertise in building bigdata pipelines and data lakes         Experience in developing and delivering large scale big data pipelines, real-time systems & data warehouses would be preferred.   ",2.81E+11,28-07-2023,26-10-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"hive, spark, Hadoop, Manager Technology, Data structures, J2Ee, big data, Object oriented programming, HBase",-,9am-6pm,"Full Time, Permanent",Wissda Consulting,Organization,Wissda Consulting,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Software Scientist," 1 to 3 years of solid programming experience in Java, Scala or PythonSound knowledge of Design Principles and Object Oriented Programming.      Good knowledge of at least one NoSQL data store/ search engine HBase, Cassandra, ElasticSearch, MongoDB.Hands on experience with Angular JS, React JS, D3 JS would be a plus.Hands on experience with Hadoop/ Spark/ Spark Streaming, Kafka would be a plus.      Good communication skillsWhat good candidates can expect from this opportunityAn opportunity to work on the most cutting edge big data technologies - Spark, Hadoop, Kafka, HBase.An opportunity to learn from industry experts.  ",1.61E+11,16-09-2022,15-12-2022,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"NoSQL, cassandra, spark, SCALA, Hadoop, Javascript, MongoDB, big data, Object oriented programming, HBase",-,9am-6pm,"Full Time, Permanent",Valens Datalabs,Organization,Valens Datalabs,-,Ahmedabad,Ahmedabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Software Engineer - Saprk/Kafka," Deploy and configure Hadoop, Zookeeper, Kafka, and Spark components      Design and implement big data solutions that meet business requirements      Optimize and tune big data infrastructure for performance and scalability      Monitor and troubleshoot the big data infrastructure to ensure high availability      Collaborate with cross-functional teams to ensure data integrity and security      Develop and maintain documentation for the big data infrastructure and its components      Keep up-to-date with emerging trends and technologies in big data              Requirements:        Bachelors degree in Computer Science or a related field      5+ years of experience in deploying and configuring big data infrastructure components such as Hadoop, Zookeeper, Kafka, and Spark      Strong knowledge of Linux operating system and shell scripting      Experience in deploying and configuring big data infrastructure on cloud platforms such as AWS, Azure, or Google Cloud      Experience in using configuration management tools such as Ansible, Chef, or Puppet      Knowledge of data ingestion and processing frameworks such as NiFi, Flink, or Storm      Strong analytical and problem-solving skills      Excellent communication and collaboration skills          ",80423500585,08-04-2023,07-07-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Computer science, Linux, spark, Configuration management, Analytical, Shell scripting, Hadoop, Data processing, data integrity, big data",-,9am-6pm,"Full Time, Permanent",Quarks Technosoft,Organization,Quarks Technosoft,-,"Gurugram, Jaipur","Gurugram, Jaipur",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Urgent Opening For Big Data Engineer,"Key Responsibilities Selection & implementation of big data tools/ frameworks to implement the requirements. Developing and managing data pipelines to ingest, process, transform, and store data from diverse sources such as streams, logs, APIs etc. Designing and implementing data models and schemas optimized for specific use cases and analytical requirements. Implementing big data processing frameworks and technologies to perform batch and real-time data processing tasks. Optimizing data processing and query performance to ensure efficient data retrieval and analysis. Implementing data security measures, encryption, and access controls to protect sensitive data. Designing and implementing scalable and robust data architecture to handle large volumes of data efficiently. Monitoring data pipelines and infrastructure components to ensure reliability, availability, and performance. Troubleshooting issues and resolving performance bottlenecks. Collaborating with data scientists, analysts, and other stakeholders to understand data requirements and deliver scalable solutions that meet business needs. Model Deployment and cloud management Skills & Qualification Bachelor's degree in computer science or a related field. Proficiency in big data tools and related programming languages, preferably Python. Strong understanding and hands-on experience with big data technologies and framework Proficient with relational databases (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB, Couchbase). Experience on building data processing pipelines & model deployment in an offline, on-premises environment and extensible to an on-cloud platform (e.g., AWS, Azure, GCP). Experience of deploying Machine learning models. Knowledge of data governance principles, compliance regulations (e.g., GDPR, HIPAA), and best practices for data security and privacy. Strong problem-solving and communication skills Ability to work in a dynamic environment.",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data, Big Data Technologies, Python",-,9am-6pm,"Full Time, Permanent",Mapmyindia,Organization,Mapmyindia,https://img.naukimg.com/logo_images/groups/v1/529698.gif,Delhi / NCR,Delhi / NCR,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata Lead,"  We at Datametica Solutions Private Limited are looking for Bigdata Lead who have a passion for cloud with knowledge of different on-premise and cloud Data implementation in the field of Big Data and Analytic s including and not limiting to Teradata, Netezza, Exadata, Oracle, Cloudera, Hortonworks and alike. Ideal candidates should have technical experience in migrations and the ability to help customers get value from Datametica's tools and accelerators Job Description  Experience : 6 to 10 Years  Location : Pune 5+ years  of overall experience in developing, testing & implementing Big data projects using Hadoop, Spark, Hive.   Hands-on experience playing lead role in Big data projects, responsible for implementing one or more tracks within projects, identifying and assigning tasks within the team and providing technical guidance to team members.  Experience in setting up Hadoop services, implementing ETL/ELT pipelines, working with Terabytes of data ingestion & processing from varied systems  Experience working in onshore/offshore model, leading technical discussions with customers, mentoring and guiding teams on technology, preparing HDD & LDD documents   Required Skills and Abilities:  Mandatory Skills Spark, Scala/Pyspark, Hadoop ecosystem including Hive, Sqoop, Impala, Oozie, Hue, Java, Python, SQL, Flume, bash(shell scripting)  Secondary Skills Apache Kafka, Storm, Distributed systems, good understanding of networking, security(platform & data) concepts, Kerberos , Kubernetes  Understanding of Data Governance concepts and experience implementing metadata capture, lineage capture, business glossary  Experience implementing CICD pipelines and working experience with tools like SCM tools such as GIT, Bit bucket, etc  Ability to assign and manage tasks for team members, provide technical guidance, work with architects on HDD, LDD, POCs  Hands on experience in writing data ingestion pipelines, data processing pipelines using spark and sql, experience in implementing SCD type 1 & 2, auditing, exception handling mechanism   Data Warehousing projects implementation with either Java, or Scala based Hadoop programming background.  Proficient with various development methodologies like waterfall, agile/scrum.  Exceptional communication, organization, and time management skills  Collaborative approach to decision-making & Strong analytical skills  Good To Have - Certifications in any of GCP, AWS or Azure, Cloudera  Work on multiple Projects simultaneously, prioritizing appropriately Benefits we Provide! Working with Highly Technical and Passionate, mission-driven people Subsidized Meals & Snacks Flexible Schedule Approachable leadership Access to various learning tools and programs Pet Friendly Certification Reimbursement Policy   Check out more about us on our website below! www.datametica.com",2.31E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,Software Product,"Hive, spark, pyspark, hadoop, Hdfs, Sqoop, Mapreduce, Impala, HBase",-,9am-6pm,"Full Time, Permanent",Datametica,Organization,Datametica,https://www.naukri.com/hotjobs/images/v3/Dmetic_May21.gif,"Pune,Maharashtra","Pune,Maharashtra",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
We do have openings For H1B Visa holders in U.S. (Big data Engineer),"Job description of Big data engineer    Responsible for Hadoop development Implementation including loading from disparate data sets, preprocessing using Hive and Pig. Scope and deliver various Big Data solutions Ability to design solutions independently based on high-level architecture. Manage the technical communication between the survey vendor and internal systems Maintain the production systems (Kafka, Hadoop, Cassandra, Elasticsearch) Implementation including loading from disparate data sets, pre processing using Hive and Pig. Working proficiency in Analytics, OLAP technologies, and more.? Experience in agile development methodologies is a must",2.21E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data, hive, Maven, L1, Hadoop, Mongo Db Basics, Kafka, Flume, Hdfs, Apache Tomcat, HBase, Tomcat Server, Sqoop, Apache Pig, Ozzie, MongoDB, Oozie",-,9am-6pm,"Full Time, Permanent",Busitants Inc,Organization,Busitants Inc,-,United States (USA),United States (USA),-,-,-,3.5-7 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
We do have job opening in U.S. For Big Data Engineer (H1B  Visa),"JOB DESCRIPTION OF BIG DATA ENGINEER     Designing the architecture of a big data platform Maintaining data pipeline Customizing and managing integration tools, databases, warehouses, and analytical systems Managing and structuring data Setting up data-access tools for data scientists Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities. Implementing ETL process  Monitoring performance and advising any necessary infrastructure changes Defining data retention policies",2.01E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Hadoop, Java, Cdh, Kafka, Database Testing, Big Data, Flume, Apache Tomcat, Hdfs, Impala, Machine Learning, SQL, HBase, Hive, Cloud Computing, Apache Pig, Sqoop, SCALA, H1B, MongoDB, Oozie, ETL, Sqoop Scripting, Python",-,9am-6pm,"Full Time, Permanent",Busitants Inc,Organization,Busitants Inc,-,USA,USA,-,-,-,3.5-7.5 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Apache Spark Professional,Key Responsibilities: A: The resource will write and review complex SQL statements B: The resource will work on ETL preferably on OWB C: The resource will work on database related to SQL/PLSQL of Oracle only Technical Experience: A: Minimum 5-6 years of experience in designing and delivering Data Warehouse projects B: 5-6 years of working expertise in SQL/PLSQL Oracle C: Should have 3 years of experience in ETL testing and should have good analytical skills Professional Attributes: Should Have Good Communication and Analytical Skills Educational Qualification: 15 years of Full-Time education,1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,Emerging Technologies (Cloud),"Spark, Apache, Data Warehouse, PLSQL, ETL Testing, SQL",-,9am-6pm,"Full Time, Permanent",Visionyle Solutions,Organization,Visionyle Solutions,-,"Pune, Gurugram","Pune, Gurugram",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Assistant Manager,"                       Application development experience using Scala programming language in HDFS or Azure Databricks.                 Exceptional SQL programming skills                 Application development experience using python programming language.                 Ability to perform complex data analytics with large volumes of data                 Maintain a current understanding of industry and technology trends.                 Collaborate effectively with the team to deliver end to end, high-performing, highly reliable, scalable and operable solutions                 Contribute to the design and architecture of the project                 Analyze and improve efficiency, scalability, and stability of various system resources                           Knowledge, Skills and Abilities                                       Education                     Bachelors degree in Computer Science, Engineering, or related discipline                                 Experience                     5 years of experience with medium-to-large technology-based projects.                 5 years of experience developing, enhancing, and supporting application software                 3 years of hands-on experience     in core Hadoop technologies including HDFS, Hive, YARN,    Data transport,      Streaming     and Spark                                Knowledge and skills        (general and technical)                   At least 3 years application development experience using Scala programming language in HDFS or Azure Databricks                 Good working Knowledge of MS Azure and Azure notebook                 Experience designing, developing, and maintaining software frameworks on Spark, Hadoop MR, Kafka, using        Scala/Python etc utilizing distributed computing architecture                   Build scalable Spark data pipelines leveraging scheduler/executor framework                 Develop innovative engineering solutions core ETL Frameworks, Using Spark, other ETL tools                 Familiar with ETL process designing using Azure Data Factory (Azure pipeline development)                 Exceptional SQL programming skills                 Familiar with shell scripting                 Background in Java /Scala in Hadoop for data ingestion and data transformation.                 Familiarity with data access using HBase and Hive                  Should have experience in complete software development lifecycle using Agile and Waterfall methodologies                 Strong analytical and problem-solving skills                 Good communication skill                 Ability and desire to learn new skills and techniques is desired                 Proactive and flexible           ",1.61E+11,16-11-2022,14-02-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Insurance,"spark, Hadoop, SCALA, Programming, Application development, PID, Asset management, big data, SQL, Python",-,9am-6pm,"Full Time, Permanent",Metlife,Organization,Metlife,https://img.naukimg.com/logo_images/groups/v1/115024.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"Engineer- Big Data (Hadoop, Pyspark, Hive)","         Support the Compliance and Operations Risk big data delivery team in India to lead and assist in the design and actual development of applications.             Responsible for specific functional areas within the team, this involves project management and taking business specifications.             The individual should be able to independently run projects/tasks delegated to them.                             Technical Skills:                                 Bachelor degree in Engineering or Computer Science or equivalent             3 to 7 years experience is required             Experience on Big Data (Spark Core and Hive is must)             Familiar with GCP offerings, experience building data pipelines on GCP a plus             Expert on Hadoop Architecture having knowledge on Hadoop, Map Reduce, Hbase.             Experience in UNIX shell scripting is good to have             Demonstrated ability to develop and document technical and functional specifications and analyze software and system processing flows             Willingness to understand the business and participate in discussions about project requirements         ",70224500556,07-02-2024,07-05-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Computer science, Career development, GCP, Project management, Analytical, Finance, Wellness, Open source, big data, Unix shell scripting",-,9am-6pm,"Full Time, Permanent",Resy,Organization,Resy,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Unit Manager- Technology Services,"                       Position Summary                               One who has prior experience and knowledge on Big Data tools and Azure technologies and can work under minimal guidance.     One who has agility to keep himself updated on the relevant technologies and can      implement the same to decrease costs, increase performance and positively affect the bottom line                                 Job Responsibilities                                   Understand business requirement, participate in requirements analysis and provide inputs from Data perspective                 Application development experience using python programming language                 Build simple to complex pipelines and dataflows                 Application development experience using Scala programming language in HDFS or Azure Databricks                 Application development experience using python programming language                 Should be able to implement modules that has security and authorization frameworks                 Ability to perform complex data analytics with large volumes of data                 Maintain existing data pipelines with high quality work                 Maintain a current understanding of industry and technology trends                 Collaborate effectively with the team to deliver end to end, high-performing, highly reliable, scalable and operable solutions                 Contribute to the design and architecture of the project                       Knowledge, Skills and Abilities                                       Education                     Bachelors degree in Computer Science, Engineering, or related discipline                               Experience                   10 years of experience with medium-to-large technology-based projects.                 Expertise in core Hadoop technologies including HDFS, Hive, YARN,    Data transport, and Spark             Good exposure to Azure Databricks, ADF, ADLS and other Azure cloud components             Good exposure to Azure DevOps methodologies, create Pull Request (PR), Continuous Integration (CI) and Continuous Deployment (CD) of pipelines                                   Knowledge and skills        (general and technical)                 Background in Java /Scala/python in Hadoop for data ingestion and data transformation.             Familiarity with UNIX shell scripting             Strong understanding of Spark internals, RDD, Data frames             Familiarity with data access using HBase and Hive             Strong experience with ETL, specifically around data wrangling and transformation.               Experience with Open Source non-relational / NoSQL data repositories (incl. HBase, MongoDB, Cassandra, Neo4J etc.)               Experience designing and developing data ingestion and processing/transformation frameworks leveraging open-source tools such as NiFi, Sqoop, Airflow etc.             Experience designing solutions for Big Data environments             Should have experience in complete software development lifecycle using Agile methodologies             Ability and desire to learn new skills and techniques is desired                             Other Requirements            (licenses, certifications, specialized training - if required)                                               Working Relationships                                 Internal Contacts                   (and purpose of relationship):                         Business, App support team                  GOSC management, HR, Transport, Facilities                                    External Contacts                        (and purpose of relationship) - If Applicable                         Vendors                                        ",20822502702,02-08-2022,31-10-2022,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,Insurance,"Relationship, Software development life cycle, Programming, Application development, Asset management, Open source, big data, Unix shell scripting, Financial services, Python",-,9am-6pm,"Full Time, Permanent",Metlife,Organization,Metlife,https://img.naukimg.com/logo_images/groups/v1/115024.gif,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Lead Big Data Solution Engineer,"   Required          Proven hands-on experience on designing, developing and supporting Database projects for analysis in a demanding environment.      Proficient in database design techniques - relational and dimension designs      Experience and a strong understanding of business analysis techniques used.      High proficiency in the use of SQL or MDX queries.      Ability to manage multiple maintenance, enhancement and project related tasks.      Ability to work independently on multiple assignments and to work collaboratively within a team is required.      Strong communication skills with both internal team members and external business stakeholders          Added Advanatage          Hadoop ecosystem or AWS, Azure or GCP Cluster and processing      Experience working on Hive or Spark SQL or Redshift or Snowflake will be an added advantage.      Experience of working on Linux system      Experience of Tableau or Micro strategy or Power BI or any BI tools will be an added advantage.      Expertise of programming in Python, Java or Shell Script would be a plus          Roles & Responsibilities          Be frontend person of the worlds most scalable OLAP product company Kyvos Insights.      Interact with senior-most technical and business people of large enterprises to understand their big data strategy and their problem statements in that area.      Create, present, align customers with and implement solutions around Kyvos products for the most challenging enterprise BI/DW problems.      Be the Go-To person for customers regarding technical issues during the project.      Be instrumental in reading the pulse of the big data market and defining the roadmap of the product.      Lead a few small but highly efficient teams of Big data engineers      Efficient task status reporting to stakeholders and customer.      Good verbal & written communication skills      Be willing to work on off hours to meet timeline.      Be willing to travel or relocate as per project requirement    ",20224501568,02-02-2024,02-05-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Management Consulting,"Microstrategy, Linux, Business analysis, Database design, GCP, Shell scripting, OLAP, Data warehousing, SQL, Python",-,9am-6pm,"Full Time, Permanent",Kyvos,Organization,Kyvos,-,"Noida, Indore","Noida, Indore",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Architect,"   Quick Implementation    - We offer quick implementation for the new onboarding client.       Experienced Team    - We ve built an elite and diverse team that brings its unique blend of talent, expertise, and experience to make you more successful, ensuring our services are uniquely customized to your specific needs.       One Stop Solution    - Coders Brain provides end-to-end solutions for the businesses at an affordable price with uninterrupted and effortless services.       Ease of Use    - All of our products are user friendly and scalable across multiple platforms. Our dedicated team at Coders Brain implements keeping the interest of enterprise and users in mind.       Secure    - We understand and treat your security with utmost importance. Hence we blend security and scalability in our implementation considering long term impact on business benefit.       Skills-       Data Modeling     Pyspark       Additional:       Any ETL tool (Pentaho/datastage)     Any Visualization tool ( Kibana, Tableau) ",3.01E+11,30-08-2023,28-11-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"tableau, HR Executive, Scalability, Data modeling, Datastage, Manager Technology, Business solutions, Pentaho, big data architect, ETL tool",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Kolkata,Kolkata,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Hadoop specialist,"       NITYO INFOTECH is looking for Hadoop to join our dynamic team and embark on a rewarding career journey       Designing, implementing, and managing large scale Hadoop clusters     Configuring and tuning Hadoop components such as HDFS, MapReduce, YARN, and Hive     Developing custom applications and scripts to extract, transform, and load (ETL) data into the Hadoop ecosystem     Integrating Hadoop with other Big Data tools and technologies     Performing regular maintenance and upgrades to the Hadoop cluster, ensuring optimal performance and reliability     Monitoring and troubleshooting Hadoop-related issues and ensuring high availability of the cluster     Collaborating with data scientists and other stakeholders to understand their requirements and help them utilize Hadoop to meet their goals     Strong problem-solving skills, with the ability to troubleshoot and resolve complex technical issues     Excellent communication and collaboration skills                 ",2.81E+11,28-11-2023,26-02-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,Recruitment / Staffing,"hive, Hadoop, hdfs, Management, Troubleshooting, big data, YARN, Monitoring",-,9am-6pm,"Full Time, Permanent",Nityo Infotech,Organization,Nityo Infotech,https://img.naukimg.com/logo_images/groups/v1/76234.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata Developer_PB,"       2 to 5 years coding experience in Java.         Solid computer science fundamentals including data structure and algorithm design, and creation of architectural specifications.         Expertise in developing Implementation of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, documentation, build processes, automated testing, and operations.         A passion for developing and maintaining a high-quality code and test base, and enabling contributions from engineers across the team.         Expertise in big data technologies like Hadoop, Spark, Kafka, HBase etc would be an added advantage.         Experience in developing and delivering large scale big data pipelines, real-time systems & data warehouses would be preferred.         Demonstrated ability to achieve stretch goals in a very innovative and fast paced environment.         Demonstrated ability to learn new technologies quickly and independently.         Excellent verbal and written communication skills, especially in technical communications.         Strong inter-personal skills and a desire to work collaboratively.       ",2.81E+11,28-07-2023,26-10-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Computer science, Interpersonal skills, Automation testing, Architecture, Coding, spark, Hadoop, Software development life cycle, big data, HBase",-,9am-6pm,"Full Time, Permanent",Wissda Consulting,Organization,Wissda Consulting,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Lead,"   Solution Architecture: Designing and architecting scalable and efficient big data solutions that address business requirements and technical challenges      Technology Selection: Evaluating and selecting appropriate big data technologies, frameworks, and tools based on the organization's needs and goals      Data Ingestion: Overseeing the collection, extraction, and ingestion of large volumes of data from various sources into the big data platform      Data Processing: Designing data processing pipelines using technologies like Apache Hadoop, Apache Spark, or other distributed computing frameworks      Data Storage: Managing and optimizing data storage solutions, including Hadoop Distributed File System (HDFS), NoSQL databases, and cloud-based storage options      Data Transformation: Implementing data transformation and ETL (Extract, Transform, Load) processes to clean, enrich, and prepare data for analysis        ",2.41E+11,24-08-2023,22-11-2023,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Solution architecture, NoSQL, spark, Hadoop, Architectural design, Cloud, Data processing, hdfs, Management, big data",-,9am-6pm,"Full Time, Permanent",Vedity Software,Organization,Vedity Software,-,Ahmedabad,Ahmedabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"     Designing and implementing scalable data storage solutions, such as Hadoop and NoSQL databases     Developing and maintaining big data processing pipelines using tools such as Apache Spark and Apache Storm     Writing and testing data processing scripts using languages such as Python and Scala     Integrating big data solutions with other IT systems and data sources     Collaborating with data scientists and business stakeholders to understand data requirements and identify opportunities for data-driven decision making     Ensuring the security and privacy of sensitive data     Monitoring performance and optimizing big data systems to ensure they meet performance and availability requirements     Staying up-to-date with emerging technologies and trends in big data and data engineering     Mentoring junior team members and providing technical guidance as needed     Documenting and communicating technical designs, solutions, and best practices             Strong problem-solving and debugging skills     Excellent written and verbal communication skills   ",2.40E+11,24-02-2023,25-05-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"hive, MOB, C, spark, SCALA, hdfs, big data, Python",-,9am-6pm,"Full Time, Permanent",Vedity Software,Organization,Vedity Software,-,Ahmedabad,Ahmedabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Senior Big Data Engineer,"   Digit88 is looking for a Big Data Engineer who will work on building, and managing Big Data Pipelines for us to deal with the huge structured data sets that we use as an input to accurately generate analytics at scale for our valued Customers. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining,implementing, and monitoring them. You will also be responsible for integrating them with the architecture used across the company. Applicants must have a passion for engineering with accuracy and efficiency, be highly motivated and organized, able to work as part of a team, and also possess the ability to work independently with minimal supervision.                   To be successful in this role, you should possess       Collaborate closely with Product Management and Engineering leadership to devise and build the right solution.   Participate in Design discussions and brainstorming sessions to select, integrate, and maintain Big Data tools and frameworks required to solve Big Data problems at scale.   Design and implement systems to cleanse, process, and analyze large data sets using distributed processing tools like Akka and Spark.   Understanding and critically reviewing existing data pipelines, and coming up with ideas in collaboration with Technical Leaders and Architects to improve upon current bottlenecks   Take initiatives, and show the drive to pick up new stuff proactively, and work as a Senior Individual contributor on the multiple products and features we have.   7+ years of experience in developing highly scalable Big Data pipelines.   In-depth understanding of the Big Data ecosystem including processing frameworks like Spark, Akka, Storm, and Hadoop, and the file types they deal with.   Experience with ETL and Data pipeline tools like Apache NiFi, Airflow etc.   Excellent coding skills in Java or Scala, including the understanding to apply appropriate Design Patterns when required.   Experience with Git and build tools like Gradle/Maven/SBT.   Strong understanding of object-oriented design, data structures, algorithms, profiling, and optimization.   Have elegant, readable, maintainable and extensible code style.                       You are someone who would easily be able to       Work closely with the US and India engineering teams to help build the Java/Scala based data pipelines   Lead the India engineering team in technical excellence and ownership of critical modules; own the development of new modules and features   Troubleshoot live production server issues.   Handle client coordination and be able to work as a part of a team, be able to contribute independently and drive the team to exceptional contributions with minimal team supervision   Follow Agile methodology, JIRA for work planning, issue management/tracking       Additional Project/Soft Skills:       Should be able to work independently with India US based team members.   Strong verbal and written communication with ability to articulate problems and solutions over phone and emails.   Strong sense of urgency, with a passion for accuracy and timeliness.   Ability to work calmly in high pressure situations and manage multiple projects/tasks.   Ability to work independently and possess superior skills in issue resolution.   Should have the passion to learn and implement, analyse and troubleshoot issues               ",2.30E+11,23-02-2023,24-05-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,Film / Music / Entertainment,"Product management, Object oriented design, Maven, Coding, Consulting, Data structures, Apache, JIRA, Analytics, Monitoring",-,9am-6pm,"Full Time, Permanent",Digit88,Organization,Digit88,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"Responsibilities : Design, develop, and maintain large-scale data processing systems. Implement and optimise data pipelines for efficient data extraction, transformation, and loading (ETL) processes. Collaborate with cross-functional teams to understand business requirements and design solutions. Ensure the scalability, reliability, and performance of big data applications. Implement data security and privacy measures in accordance with industry best practices. Stay up-to-date with emerging trends and technologies in big data and analytics. Requirements : Bachelor's degree in Computer Science or Information Technology (B.E). Proven experience as a Big Data Engineer with 6-12 years of hands-on experience. Strong proficiency in big data technologies such as Hadoop, Spark, and Kafka. Experience with distributed computing and parallel processing. Proficiency in programming languages such as Java, Scala, or Python. Familiarity with data modelling and database design concepts. Excellent problem-solving and troubleshooting skills. Ability to work collaboratively in a team environment. Excellent written and verbal communication skills. Additional Requirements: Availability for onsite opportunities in the USA. Mandatory possession of a valid passport.",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data, database design, Java, Scala, data modelling, Hadoop, Kafka, Spark, ETL, Python",-,9am-6pm,"Full Time, Permanent",5D Solutions,Organization,5D Solutions,https://img.naukimg.com/logo_images/groups/v1/6757633.gif,United States (USA),United States (USA),-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Consultant,"Brief Role : Job Description  Consulting of our customers in the area of application and process design in complex system landscapes of Big Data / BI solutions Analysis & design of the customer requirements for Big Data systems as well as Big Data architecture definitions in combination with classical ERP systems Implementation of customer-specific solutions using Big Data technologies and current BI tools and databases Introduction / Development Hadoop solutions in combination with Analytics Application performance tuning and troubleshooting Ensure solutions developed adhere to security and data entitlement Additional Information  Salary: Not Disclosed by Recruiter Industry:IT-Software / Software Services Functional Area:IT Software - DBA , Datawarehousing Role Category:Programming & Design Role: Software Developer Desired Candidate Profile  3 - 5 yrs of exp. Strong CS fundamentals, including good working knowledge of classic algorithms and data structures Experience with Big Data Analytics: stream processing, batch processing optimization. Education  UG: B.Tech/B.E. - Any Specialization PG:MBA/PGDM - Any Specialization, M.Tech - Any Specialization, MCA - Computers, MS/M.Sc(Science) - Any Specialization",2.11E+11,21-07-2017,19-10-2017,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,Recruitment / Staffing,"Process design, Performance tuning, ERP, Algorithms, Consulting, Tools, Data structures, Troubleshooting, Software services, Data architecture",-,9am-6pm,"Full Time, Permanent",Right Step Consulting,Organization,Right Step Consulting,-,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer Intern,"           3+ years of experience in Big Data technologies         Experience in designing and implementing data pipelines         Hands on experience in Hadoop Ecosystem (Spark, Scala, Hive, PIG )         Flair for data, schema, data model, how to bring efficiency in big data related life cycle         Good understanding of streaming applications         Basic understanding of Azure Data bricks         Basic understanding of SQL or NoSQL DBs         Good communications skill     ",2.01E+11,20-10-2022,18-01-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"hive, NoSQL, Data modeling, spark, Schema, Hadoop, SCALA, big data, SQL",-,9am-6pm,"Full Time, Permanent",Veridic Solutions,Organization,Veridic Solutions,https://img.naukimg.com/logo_images/groups/v1/6169571.gif,remote,remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Professional,"   Designing and implementing big data architecture and infrastructure     Collecting and processing large volumes of structured and unstructured data from various sources     Cleaning, transforming, and integrating data to prepare it for analysisImplementing data processing and analysis tools, such as Hadoop, Spark, and NoSQL databases     Designing and implementing algorithms and models to analyze data and uncover insights     Collaborating with data scientists, business analysts, and stakeholders to define and prioritize big data projects     Presenting insights and findings to key stakeholders and making recommendations based on data analysis     Ensuring data security and privacy and following best practices for data management     Strong problem-solving skills and the ability to think creatively and critically     Excellent communication and collaboration skills, with the ability to work effectively with technical and non-technical stakeholders   ",1.81E+11,18-09-2023,17-12-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"hive, RF, Networking, spark, Manager Technology, Business solutions, big data, SQL",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Azure Data Factory,"         Experience with big data tools: Hadoop, Spark, Kafka, etc.             Experience with relational SQL and NoSQL databases.             Experience with data pipeline and workflow management tools: Azure ADF , Azkaban, Luigi, Airflow, etc.             Experience with native cloud services: Azure cloud native apps             Experience with stream-processing systems: Storm, Spark-Streaming, etc.             Experience with object-oriented/object function scripting languages: Python, Java, C , Scala, etc.         ",1.71E+11,17-08-2022,15-11-2022,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"C, NoSQL, spark, Cloud Services, Workflow management, SCALA, big data, SQL, Python, Scripting",-,9am-6pm,"Full Time, Permanent",eTeam Inc.,Organization,eTeam Inc.,https://img.naukimg.com/logo_images/groups/v1/228540.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"   As Lead Data Engineer, you will work with a variety of talented teammates and be a driving force for building first-class solutions for Enterprise Data Integration and Analytics. The ideal candidate will have programming experience, and big data skills, be comfortable with ambiguity, and will enjoy working in a fast-paced dynamic environment.       Job Requirements:        Bachelors or Master s degree in Computer Science, Information Technology, Engineering, or related field     6+ years of experience in relevant field with Apache spark knowledge.     Experience in software development with Python, Scala, Java SQL     Strong experience working with Big data technologies (Spark, Hive, Hadoop, Kafka)     Experience in building ETL Data Pipelines     Experience with batch processing data and/or creating real-time analysis systems     Experience with AWS services including S3, EC2, Redshift, EMR, RDS, and IAM highly desirable     Experience with workflow scheduling tools like Airflow     Good understanding of Data zlake concepts such as data ingestion and transformation     Demonstrate pro-activeness and task ownership     Ability to multi-task     Experience supporting and working with cross-functional teams in a dynamic environment     Excellent verbal and written communication skills   ",1.61E+11,16-12-2022,16-03-2023,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SCALA, Workflow, Scheduling, Real time analysis, big data, Information technology, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Sapper Software,Organization,Sapper Software,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Sr. Bigdata Engineer,"           We are looking for people who have the right attitude, aptitude, skills, empathy, compassion, and hunger for learning      Built products in the data analytics space, either frontend/backend/cloud      A passion for shipping high-quality products, interest in the data products space, curiosity about the bigger picture of building a company, product development, people, and product                Roles and Responsibilities                              We are looking for a savvy Data Engineering professional to join the newly formed Data Engineering team                        We are looking for Big Data specialists who have proven skills on working large scale data systems                        The hire will be responsible for building and optimizing data pipeline architectures, as well as optimizing data flow and collection for multiple source systems                        The ideal candidate should be experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up                        Have got strong ability to build robust and resilient data pipelines which are fault tolerant and reliable in terms of data movement                        Should have knowledge of experience on batch and stream data processing                        Create end to end data products and productionize them in cloud/in-house servers                        Technical Skills                        Minimum 6-8 years of progressive experience building solutions in Big Data environments                        Should have solid hands on experience on Big Data technologies like Hadoop, HBase, Hive, Pig, Oozie, MapReduce, Yarn, HDFS, Zookeeper                        Hands on knowledge of Apache Spark with Java/Scala for batch and stream processing will be highly preferred                        Knowledge of Apache Kafka will be added advantage but not mandatory                        Strong hands on capabilities on SQL and NoSQL technologies                        Should be able to build performant, fault tolerant, scalable solutions                        Excellent written and verbal communication skills                        Experience of interacting with stakeholders and end customers will be highly preferred                        Experience with cloud based technologies either AWS (EMR, Redshift, Lambda, Kinesis, Glue, S3, SQS) or Azure (Data Factory, Data Lake Gen 2, SQL data warehouse, Stream Analytics, Synapse Analytics) will be huge plus                        What is in it for you                    A clear career path with a hybrid of technical track and management track to grow in the company                      In the technical track, the candidate will be developing sophisticated actuarial and analytics skill                        Communicating with clients and manage the track, the candidate will develop skills sets in business strategy and build strong tech products                    You should have                          Excellent problem solving and analytical skills                        Fluency in written and communication skills in English                        Good time-management skills                        Should be a quick learner with ability to learn new tools/technology quickly                        Good analytical and problem-solving skills                        Intellectual and analytical curiosity                        Expertise in data analytics and providing data driven insights that help in taking business decisions                        The traits of a self-motivated, independent and detail-oriented team player              You will be responsible for optimizing data systems and building robust data pipelines for seamless data movement.      ",1.31E+11,13-10-2023,11-01-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Backend, Analytical, Cloud, Data processing, Actuarial, Data analytics, Business strategy, Apache, big data, SQL",-,9am-6pm,"Full Time, Permanent",Coditas Technologies,Organization,Coditas Technologies,https://img.naukimg.com/logo_images/groups/v1/4588051.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Architect," The Big Data Architect will be responsible for designing, developing, and implementing high-performance big data solutions. The key responsibilities include:          Designing and implementing scalable big data architectures      Developing data models and data integration strategies      Optimizing data storage, processing, and delivery      Collaborating with cross-functional teams to understand business requirements      Ensuring data security, integrity, and compliance            Candidate Qualifications:          The ideal candidate should possess the following qualifications:          8 to 15 years of experience in the IT industry with a focus on big data      Strong knowledge of Pyspark and data modeling      Excellent problem-solving and analytical skills      Good understanding of data security and compliance            Required Skills:            Pyspark      Data Modeling    ",1.31E+11,13-09-2023,12-12-2023,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Analytical skills, Compliance, Data modeling, data security, Focus, Manager Technology, Consultancy, Business solutions, big data, big data architect",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Kolkata,Kolkata,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data/ Hadoop professional,"                 Hands on experience in installation, configuration, supporting and managing Hadoop Clusters using Apache Hadoop, Cloudera distributions.         Experience in working with various Hadoop distributions like Cloudera,and Apache Hadoop.         Expertise in the installation of Hadoop cluster using Cloudera distribution from Scratch for different environments like Production, Development, Disaster Recovery on the infrastructure of AWS Cloud Environment.           Experienced in setting up pre requisites on servers for Hadoop clusters.           Installation of various Hadoop Ecosystems and Hadoop Daemons.          Experience in Configuring        HA (High Availability) of Namenode in aws availability zones.           Decommissioning and commissioning the DataNodes on running Hadoop cluster.            Involved in designing of Data lake Architecture.             Experienced in working with AWS services like IAM, EMR,EC2, S3, VPC, etc.           Good experience on Design, configure and manage the backup and disaster recovery for Hadoop data.            Monitor and actions on long running jobs on integration and production hadoop cluster, analysing delayed jobs affecting the cluster.             Increasing Priority for Job Sometimes there is a user request.            Hands on experience in analyzing Log files for Hadoop and eco system services and finding root cause.          As an admin involved in Cluster maintenance, trouble shooting, Monitoring and followed proper backup& Recovery strategies.          Experience in HDFS data storage and support for running jobs.          Installing and configuring Hadoop eco system like sqoop, pig, hive,Flume, Oozie, Zookeeper,Kafka, Spark.          Kafka Administraion using Streamsets.         ",1.21E+11,12-07-2022,10-10-2022,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"cloudera, oozie, Disaster recovery, Hadoop, flume, sqoop, Apache, Troubleshooting, big data, AWS",-,9am-6pm,"Full Time, Permanent",Diverse Lynx,Organization,Diverse Lynx,https://img.naukimg.com/logo_images/groups/v1/4554388.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer," Designing and implementing scalable data storage solutions, such as Hadoop and NoSQL databases.     Developing and maintaining big data processing pipelines using tools such as Apache Spark and Apache Storm.Writing and testing data processing scripts using languages such as Python and Scala.Integrating big data solutions with other IT systems and data sources.Collaborating with data scientists and business stakeholders to understand data requirements and identify opportunities for data-driven decision making.     Ensuring the security and privacy of sensitive data.Monitoring performance and optimizing big data systems to ensure they meet performance and availability requirements.Staying up-to-date with emerging technologies and trends in big data and data engineering.Mentoring junior team members and providing technical guidance as needed.Documenting and communicating technical designs, solutions, and best practices.Strong problem-solving and debugging skillsExcellent written and verbal communication skills ",1.01E+11,10-05-2023,08-08-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"big data, professional",-,9am-6pm,"Full Time, Permanent",Diverse Lynx,Organization,Diverse Lynx,https://img.naukimg.com/logo_images/groups/v1/4554388.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Sr. Data Engineers,"           5 years of Bachelor\s or Master\s degree with hands on experience in building data engineering pipelines in a distributed environment       Good understanding of Data warehousing principles       Expertise in working with      Big Data Technologies like Hadoop ,Hive, Spark(Scala/Java/Python), Sqoop, Oozie/Airflow       Good exposure to writing complex SQL ,shell scripting       Exposure to streaming data pipelines using      Spark, Kafka       Exposure to Performance optimization of batch pipelines written in Hive/Spark       Experience on CI/CD (Continuous Integration/Delivery) i.e. Jenkins, GIT/BitBucket       Familiarity with working in an agile software development framework                                           ",81222502074,08-12-2022,08-03-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,Management Consulting,"hive, continuous integration, GIT, spark, Shell scripting, Performance optimization, big data, Data warehousing, SQL, Python",-,9am-6pm,"Full Time, Permanent",Purview India Consulting and  Services  LLP,Organization,Purview India Consulting and  Services  LLP,-,remote,remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"   Designing and implementing scalable data storage solutions, such as Hadoop and NoSQL databases     Developing and maintaining big data processing pipelines using tools such as Apache Spark and Apache Storm     Writing and testing data processing scripts using languages such as Python and Scala     Integrating big data solutions with other IT systems and data sources     Collaborating with data scientists and business stakeholders to understand data requirements and identify opportunities for data-driven decision making     Ensuring the security and privacy of sensitive data     Monitoring performance and optimizing big data systems to ensure they meet performance and availability requirements     Staying up-to-date with emerging technologies and trends in big data and data engineering     Mentoring junior team members and providing technical guidance as needed     Documenting and communicating technical designs, solutions, and best practices     Strong problem-solving and debugging skills     Excellent written and verbal communication skills   ",60323501717,06-03-2023,04-06-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,big data,-,9am-6pm,"Full Time, Permanent",Diverse Lynx,Organization,Diverse Lynx,https://img.naukimg.com/logo_images/groups/v1/4554388.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata And Hadoop Developer,"Required Technical Skill Set- Hadoop, Python, PySpark, HIVE  Must-Have**   ??Hands-on experience of Hadoop, Python, PySpark, Hive, Big Data Eco System Tools.  ??Should be able to develop, tweak queries and work on performance enhancement.  ??Solid understanding of object-oriented programming and HDFS concepts  ??The candidate will be responsible for delivering code, setting up environment, connectivity, deploying the code in production after testing.  Good-to-Have   ??Preferable to have good DWH/ Data Lake knowledge.  ??Conceptual and creative problem-solving skills, ability to work with considerable ambiguity, ability to learn new and complex concepts quickly.  ??Experience in working with teams in a complex organization involving multiple reporting lines  ??The candidate should have good DevOps and Agile Development Framework knowledge. ",90524005776,09-05-2024,07-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Hive, Hadoop, Bigdata, Spark",-,9am-6pm,"Full Time, Permanent",Integrated Personnel Services,Organization,Integrated Personnel Services,https://img.naukimg.com/logo_images/groups/v1/4657959.gif,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,15-30 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"   Design, develop, and maintain big data solutions to meet business requirements and support data-driven decision making      Work with stakeholders to understand their data needs and determine how to best use big data technologies to meet those needs      Design and implement scalable, high-performance big data architectures, using technologies such as Hadoop, Spark, and NoSQL databases      Extract, transform, and load large data sets into a big data platform for analysis and reporting      Write complex SQL queries and develop custom scripts to process big data      Collaborate with data scientists, data analysts, and other stakeholders to develop predictive models and algorithms that drive insights and decision making        ",1.51E+11,15-05-2023,13-08-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,Recruitment / Staffing,"Coding, GCP, spark, Delta, oozie, Hadoop, Cloud, sqoop, big data, HBase",-,9am-6pm,"Full Time, Permanent",Gforce Consulting Solutions,Organization,Gforce Consulting Solutions,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"Tcs Hiring For Hadoop Data Engineer
",Role & responsibilities   Good Data background. Hadoop is the primary skill and PySpark is secondary experience With data transformation exp. ,10524007217,01-05-2024,30-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Data Engineering, Hadoop, Hadoop Development, Big Data, Hive, Sqoop, SCALA, Spark",-,9am-6pm,"Full Time, Permanent",Tata Consultancy Services (TCS),Organization,Tata Consultancy Services (TCS),https://www.naukri.com/hotjobs/images/v3/tcsl_nov13.gif,"Hyderabad, Pune, Bengaluru","Hyderabad, Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Spark Scala Developer,"Responsibilities A day in the life of an Infoscion As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.  You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain. You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews. You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes. You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional Requirements: Primary skills :Technology->Big Data - Data Processing->Spark,Technology->Functional Programming->Scala Preferred Skills:  Technology->Functional Programming->Scala Technology->Big Data - Data Processing->Spark  Additional Responsibilities: Knowledge of more than one technology Basics of Architecture and Design fundamentals Knowledge of Testing tools Knowledge of agile methodologies  Understanding of Project life cycle activities on development and maintenance projects Understanding of one or more Estimation methodologies, Knowledge of Quality processes Basics of business domain to understand the business requirements Analytical abilities, Strong Technical Skills, Good communication skills Good understanding of the technology and domain  Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods Awareness of latest technologies and trends Excellent problem solving, analytical and debugging skills  Educational Requirements Bachelor of Engineering  Service Line Data & Analytics Unit *  Location of posting is subject to business requirements ",60524906186,06-05-2024,04-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Spark, Scala, Big Data, agile methodologies, debugging, Data Processing",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://www.naukri.com/hotjobs/images/v3/infosys_nov13.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Spark-Scala Developer,"Responsibilities A day in the life of an Infoscion As part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain. You will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements. You will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers. You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional Requirements: Primary skills:Bigdata->Pyspark,Bigdata->Scala,Bigdata->Spark  Preferred Skills:  Technology->Functional Programming->Scala Technology->Big Data - Data Processing->Spark  Additional Responsibilities: Knowledge of design principles and fundamentals of architecture Understanding of performance engineering Knowledge of quality processes and estimation techniques Basic understanding of project domain Ability to translate functional / nonfunctional requirements to systems requirements Ability to design and code complex programs Ability to write test cases and scenarios based on the specifications Good understanding of SDLC and agile methodologies  Awareness of latest technologies and trends Logical thinking and problem solving skills along with an ability to collaborate  Educational Requirements Bachelor of Engineering  Service Line Data & Analytics Unit *  Location of posting is subject to business requirements ",60524905925,06-05-2024,04-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Spark, Scala, agile methodologies, test cases, performance engineering, SDLC",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://www.naukri.com/hotjobs/images/v3/infosys_nov13.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Spark Scala Developer,"  Responsibilities A day in the life of an Infoscion As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction. You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain. You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews. You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes. You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional Requirements: Primary skills:Technology->Big Data - Data Processing->Spark,Technology->Functional Programming->Scala Preferred Skills: Technology->Functional Programming->Scala Technology->Big Data - Data Processing->Spark Additional Responsibilities: Knowledge of more than one technology Basics of Architecture and Design fundamentals Knowledge of Testing tools Knowledge of agile methodologies Understanding of Project life cycle activities on development and maintenance projects Understanding of one or more Estimation methodologies, Knowledge of Quality processes Basics of business domain to understand the business requirements Analytical abilities, Strong Technical Skills, Good communication skills Good understanding of the technology and domain Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods Awareness of latest technologies and trends Excellent problem solving, analytical and debugging skills Educational Requirements Bachelor of Engineering Service Line Data & Analytics Unit: Spark Scala Role:  Big Data Engineer Industry Type:  IT Services & Consulting Department:  Engineering - Software & QA Employment Type:  Full Time, Permanent Role Category:  Software Development Education UG:  B.Tech/B.E. in Any Specialization PG:  Any Postgraduate * Location of posting is subject to business requirements",20424007140,13-05-2024,11-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Spark Core, Scala Programming, Big Data",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://www.naukri.com/hotjobs/images/v3/infosys_nov13.gif,"Bengaluru,Karnataka","Bengaluru,Karnataka",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata Developer(Scala),"Job Title - Big Data Engineer  Skills Required - Spark, Scala, Hadoop, Scala, Python, Java. Location - Pune Requirements:- You will create strategic solutions that consider the risk, benefits and conflicts of the situation Design elegant solutions to complex problems which satisfy user & non-functional requirements You will provide leadership and guidance to the delivery function, delivering with optimal functionality & efficiency Engineer solutions to minimise occurrence & prevent recurrence of production issues and to facilitate reduced mean      time to recovery Design & engineers solutions and services with embedded risk and security controls as mitigation against threats You will ensure a high quality solution, ensuring TDD/BDD standards are aligned to. Your skills and experience Skilled in building productive networks to drive collaboration, re-use and knowledge sharing Effectively communicates complex messages in a clear and concise manner Enables experimentation and fast learning approaches to creating business solutions Experience in solution design tools and techniques with ability to make design decision trade-offs Required Scala or Java Distributed Processing Spark Hadoop Technologies Hive Experience in Hadoop Big Data projects, HDFS file system Desirable Akka (including streaming, http, remoting) RESTful web services Python Linux Oracle Kafka",2.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,108,Engineering - Software & QA,Big Data Engineer,Investment Banking / Venture Capital / Private Equity,"bigdata, scala, spark",-,9am-6pm,"Full Time, Permanent",Deutsche Bank,Organization,Deutsche Bank,https://img.naukimg.com/logo_images/groups/v1/468918.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Sr Big Data Infrastructure Engineer (GCP),"       We are seeking a highly skilled and experienced Senior Big Data Infra Engineer to join our dynamic team     The ideal candidate will have a strong background in developing and scaling both stream and batch processing systems, and a solid understanding of public cloud technologies, especially GCP     This role involves working in a remote environment, requiring excellent communication skills and the ability to solve complex problems independently and creatively           What you will be doing:        Implementing automation/DevOps best practices for CI/CD, IaC, Containerization, etc to Build a reusable infra structure for stream and batch processing systems at scale.      Create automation, whether that is building DevOps pipelines, scripting or creating Infrastructure as Code in Terraform     Participating in work sessions with clients      Completing technical documentation                Requirements:        Experience in Developing and Scaling data Processing Systems This includes working with technologies like Pub/Sub, Kafka, Kinesis, DataFlow, Flink, Hadoop, Pig, Hive, and Spark.     Expertise in public cloud services, particularly in GCP.      Experience with GCP managed services and understanding of cloud-based messaging/stream processing systems are critical.     Experienced in Infrastructure and Applied DevOps principles in daily work. Utilize tools for continuous integration and continuous deployment (CI/CD), and Infrastructure as Code (IaC) like Terraform to automate and improve development and release processes.      Has knowledge in containerization technologies such as Docker and Kubernetes to enhance the scalability and efficiency of applications.     Worked effectively in a remote setting, maintaining strong written and verbal communication skills. Collaborate with team members and stakeholders, ensuring clear understanding of technical requirements and project goals.     Proven experience in engineering stream/batch processing systems at scale.     Strong programming abilities in Java and Python.     Hands-on experience in public cloud platforms, particularly GCP. Additional experience with other cloud technologies is advantageous.               Must Have:        Google Associate Cloud Engineer Certification or other Google Cloud Professional level certification     4+ years of experience in customer-facing software/technology or consulting     4+ years of experience with on-premises to cloud migrations or IT transformations     4+ years of experience building, and operating solutions built on GCP (ideally) or AWS/Azure     Technical degree: Computer Science, software engineering or related           ",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Automation, Managed services, GCP, Consulting, Cloud, Data processing, rackspace, Python, Technical documentation",-,9am-6pm,"Full Time, Permanent",Rackspace Technology,Organization,Rackspace Technology,https://img.naukimg.com/logo_images/groups/v1/3080116.gif,"Hyderabad, Gurugram, Bengaluru","Hyderabad, Gurugram, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Face 2 Face || Big Data Developer || Hyderabad,"Job Description: Key Responsibilities Creating complex, enterprise-transforming applications on diverse, high energy teams  Working with the latest tools and techniques  Hands-on coding, usually in a pair programming environment  Working in highly collaborative teams and building quality code  The candidate must exhibit a good understanding of model implementation, data structures, data manipulation, distributed processing, application development, and automation. The candidate must have a good understanding of consumer financial products, data systems and data environments, and processes that are necessary for the implementation of Risk and Finance models Essential Skills & Prerequisites Degree in computer science or a numerate subject (e.g. engineering, sciences, or mathematics) or Bachelor's/Master degree with 6 years of experience Hands-on Development experience with PySpark, ScalaSpark and Distributed computing. Development and implementation experience of applications for 4-6 years. 4 to 6 years' experience designing and developing in Python. 4 to 6 years' experience in Hadoop Platform (Hive, HDFS and Spark) 3 to 5 years' experience with Unix shell scripting 3 to 5 years' experience with SQL 2 to 3 years' experience with Spark programming. Knowledge of micro-services architecture and cloud will be added advantage. Knowledge of Java and Scala will be added advantage Desired A Bachelors degree or higher preferably in Computer Science or IT Additional experience on developing service-based application Excellent analytical skills: Proficient in MS Office and able to produce board-level documentation Fluency in written and spoken English; Good communication and interpersonal skills Self-starter who sets and meets challenging personal targets, Detailed person, with a big picture outlook Understanding of current technologies employed by Tier 1 Investment Banking Institutions Must be a team player Note:-   We are looking for Immediate to 30 days of Notice period",2.90E+11,29-04-2024,28-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Pyspark, Big Data developer, Hadoop developer, Hive, Scala, Hdfs, Hbase, Python",-,9am-6pm,"Full Time, Permanent",Wroots Global,Organization,Wroots Global,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata Developer,"Role & responsibilities: Design and implement scalable and      efficient data processing pipelines using Apache Spark and Java. Develop APIs and services for      data ingestion, transformation, and aggregation to support analytics and      machine learning models. Collaborate with data scientists,      analysts, and cross-functional teams to understand data requirements and      deliver innovative data solutions. Optimize Spark jobs for      performance and cost efficiency across large datasets. Ensure high data quality and      integrity by implementing robust testing and validation practices. Lead the adoption of best      practices in code quality, CI/CD, and DevOps within the data engineering      team. Preferred candidate profile  :   At least 5+ years of software      development experience Proficient in Java programming      language. Strong experience in designing      and implementing scalable data processing pipelines using Apache Spark and      Java. Solid understanding of big data      technologies and ecosystems (HDFS, YARN, Kafka, HBase, etc.). Experience with AWS cloud      platforms and understanding of cloud-native services for data processing. Familiarity with DevOps tools and      practices, including Docker, Kubernetes, Jenkins, and Git. Excellent problem-solving skills,      with the ability to tackle challenges in large-scale data environments. Strong communication and      collaboration skills, with the ability to work effectively in a team and      interface with technical and non-technical stakeholders.",1.60E+11,16-04-2024,15-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SCALA, Cloud, Spark, Java, Pyspark, SQL",-,9am-6pm,"Full Time, Permanent",Tavant Technologies,Organization,Tavant Technologies,https://www.naukri.com/hotjobs/images/v3/tavtech123.gif,"Kolkata, Pune, Bengaluru","Kolkata, Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Snowflake Developer,"Role & responsibilities  :   3 + years of programming experience in languages like Java, Scala, or Python  At least 3 years of experience in a Data Engineer role.  Strong hands-on programming skills in core technologies is a must.  Hands-on experience in designing scalable solutions.  Data engineering, integration, and data modeling experience  Can write scalable/performant pipelines, queries, and summaries of data.  Has worked with various data systems and tools.  Understands analytics and data science workflows and common use cases that leverage their work.  Snowflake experience (MUST)  Python (MUST)  SQL (MUST)  Datawarehouse experience  AWS experience (MUST)  Data QA / validation skills (to check their work)  Matillion, DBT, or other Data tech experience (good to have)",1.60E+11,16-04-2024,15-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Snowflake, Python, SQL",-,9am-6pm,"Full Time, Permanent",Tavant Technologies,Organization,Tavant Technologies,https://www.naukri.com/hotjobs/images/v3/tavtech123.gif,"Kolkata, Pune, Bengaluru","Kolkata, Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Python(spark)developer,   Strong experience with Python design     Ability to understand and perform rebasing     Experience with database integration     Knowledge of common data structures and algorithms and when to use them     Ability to configure and implement REST APIs     Good communication skills with a strong ability to explain your logic and challenge the logic of others     Required Skills      Python     Pyspark     SQL     Spark       ? ,2.61E+11,26-08-2023,24-11-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"spark, Database, Data structures, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"Resources must have at least 5 years of coding experience in Scala, Spark, Python and BigData/Hadoop. Required skills are Scala, Spark, Hadoop, Big Data, Python. Must be good in communication.",40524003631,04-05-2024,02-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Analytics / KPO / Research,"SCALA, Hadoop, Big Data, Spark, Python",-,9am-6pm,"Full Time, Permanent",Collabera Technologies,Organization,Collabera Technologies,https://img.naukri.com/logo_images/v3/395661.gif,"Pune, Delhi / NCR, Mumbai (All Areas)","Pune, Delhi / NCR, Mumbai (All Areas)",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Azure Big Data Architect,"   Design, Architect, Deploy and maintain solutions on the MS Azure using different Cloud & Big Data Technologies.     Manage the full life-cycle of a Data Lake / Big Data solutions from requirement gathering and analysis to platform selection, design of the architecture and deployment.     Be responsible for implementing solutions which can scale on Cloud.     Collaborate with a team of business domain experts, data scientists and application developers to develop BD solutions.     Explore and learn new technologies for creative business problem solving and mentor a team of Data Engineers.         Required Experience, Skills & Competencies:         Strong Hands-on experience in implementing Data Lake with technologies like - Data Factory (ADF), ADLS, Databricks, Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and Purview.     Experience of using big data technologies like Hadoop (CDH or HDP), Spark, Airflow, NiFi, Kafka, Hive, HBase or MongoDB, Neo4J, Elastic Search, Impala, Sqoop etc.     Strong programming & debugging skills either in Python and Scala/Java. Experience of building REST services is good to have.     Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner.     Good understanding and Experience of using CI/CD with Git, Jenkins / Azure DevOps.     Experience of setting up cloud-computing infrastructure solutions.     Hands on Experience/Exposure to NoSQL Databases and Data Modelling in Hive     9+ years of technical experience with at-least 2 years on MS Azure and 2 year on Hadoop (CDH/HDP).     B.Tech/B.E from reputed institute preferred.   ",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,108,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Cloud computing, Team management, GIT, Data Architect, Debugging, Consulting, MongoDB, Analytics, Python, HBase",-,9am-6pm,"Full Time, Permanent",Tiger Analytics,Organization,Tiger Analytics,https://img.naukimg.com/logo_images/groups/v1/642976.gif,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Azure Big Data Engineer- New,"   As a    Big Data Engineer (Azure)    , you will build and learn about a variety of analytics solutions & platforms, data lakes, modern data platforms, data fabric solutions, etc. using different Open Source, Big Data, and Cloud technologies on Microsoft Azure. On a typical day, you might      Design and build scalable & metadata-driven data ingestion pipelines (For Batch and Streaming Datasets)      Conceptualize and execute high-performance data processing for structured and unstructured data, and data    harmonization      Schedule, orchestrate, and validate pipelines      Design exception handling and log monitoring for debugging      Ideate with your peers to make tech stack and tools-related decisions      Interact and collaborate with multiple teams (Consulting/Data Science & App Dev) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.          What do we expect          4 to 9 years of total IT experience with 2+ years in big data engineering and Microsoft Azure      Experience in implementing Data Lake with technologies like Azure Data Factory (ADF), PySpark, Databricks, ADLS,    Azure SQL Database      A comprehensive foundation with working knowledge of Azure Synapse Analytics, Event Hub & Streaming    Analytics, Cosmos DB, and Purview      A passion for writing high-quality code and the code should be modular, scalable, and free of bugs (debugging    skills in SQL, Python, or Scala/Java).      Enthuse to collaborate with various stakeholders across the organization and take complete ownership of    deliverables.      Experience in using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic Search      Adept understanding of different file formats like Delta Lake, Avro, Parquet, JSON, and CSV      Good knowledge of building and designing REST APIs with real-time experience working on Data Lake or    Lakehouse projects.      Experience in supporting BI and Data Science teams in consuming the data in a secure and governed manner    ",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Debugging, Consulting, Agile, JSON, Unit testing, Open source, Analytics, Monitoring, SQL, Python",-,9am-6pm,"Full Time, Permanent",Tiger Analytics,Organization,Tiger Analytics,https://img.naukimg.com/logo_images/groups/v1/642976.gif,"Hyderabad, Chennai, Bengaluru","Hyderabad, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Data Engineer - Immd - Hyderabad,"  Altimetrik is looking for a strong Data Engineer professional who has experience into 1)Good coding skills required in either Java or python(numpy and pandas) and excelect ) 2)Working experience in big data environments like Cloudera or Mapr is required. 3)Having hands on experience in Spark, Hive, Presto(optional), NoSQL. 4)Ability to understand, optimize and finetune spark jobs. 5)Strong analysis skills is required where the developer will observe cluster level painpoints and provides optimization suggestion and implementation. 6)Working experience in presto is a plus. 7)Knowledge in Elastic Search , Grafana will be good to have.  Interested candidates kindly please share your cv's and below details to snidafazli@altimetrik.com   Name: Number: EmailID: Current CTC: Expected CTC: Notice period: If serving Notice period, LWD: Current Company: Current Location: Preferred Location: Total Experience: Relevant Experience: BigData: java: python: Spark: Hive: Presto: Elastic Search: Grafana:",30524004182,06-05-2024,04-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Hive, Presto, Big Data, Spark, Grafana, Java, Elastic Search, Python",-,9am-6pm,"Full Time, Permanent",Altimetrik,Organization,Altimetrik,https://www.naukri.com/hotjobs/images/v3/synovaadmin14.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"Python, XML, REST, SOAP, and JDBC plus big data  Spring Boot, Microservices, and other open-source technologies   AWS or Microsoft Azure SQL skills and developer experience on databases like MySQL, SQL Server, and Oracle. Required Candidate profile MIn 9yrs  Immediate joiners can apply",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,108,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data, Pyspark, SCALA, MySQL, MongoDB, Spark, Python",-,9am-6pm,"Full Time, Permanent",Reycruit,Organization,Reycruit,-,Hyderabad/Secunderabad,Hyderabad/Secunderabad,-,-,-,25-40 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata And Hadoop Developer,"  Role  Big Data Team at Xoriant empowers the future with latest technology, working across Service Delivery Life Cycle. We are looking for professionals who can work on client-server applications, designing and coding solutions  We focus on bringing the right candidate on-board and encouraging them to use the latest technologies for identifying key talents. We invite you to join our team and be part of our growth story.  You should have experience in the key matrix and performance areas given below:  Key Skills ??  What we offer in this role:  Best market exposure, internal visibility for growth  Design, develop, test, deploy, maintain, and enhance software solutions  Work on robust, secure and architectural Environment  Flawless blend of volume and niche to enrich your entire experience and add starts to your career.  Young fast-growing team  Mandatory Skills:  Should be 6-8 years of IT experience with a strong emphasis on analysis, design, coding, and unit testing  Well-versed in Big Data related skills such as Hadoop, Map Reduce, Hive, Yarn, Spark, Scala and Java  Extensive experience in handling the tools such as Eclipse IDE, IBM Rational, Apache Sqoop and Hue  Worked on Sqoop to import data to Hive  Responsible for creating Hive tables, loading data and writing Hive queries  Dealing with humongous amount of data, analyze them and derive insights using Spark SQL  Worked in teams and as individual contributor as well as trained several people in Hadoop and Spark  Experience in coding on Java  Java skills #Core Java, Java with Spark, Java programming experience #Good to have skills in Java - Spring boot, Micro services, Java 8 knowledge Other Mandatory Aspects  Quick to respond to management requests and solve issues taking ownerships  Strong debugging skills to turnover production issues quickly  Personal Skills  A positive thinker with excellent communication, interpersonal and problem-solving skills  Willing to learn and adapt new challenges and dedicated towards work Qualifications - External BE,ME.MCA",1.61E+11,07-05-2024,05-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Hive, Unix Scripting, sqoop, Spark, SQL, Java",-,9am-6pm,"Full Time, Permanent",Xoriant,Organization,Xoriant,https://www.naukri.com/hotjobs/images/v3/Xoriant_Dec23.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"Looking for big data engineer with strong technical skills in AWS, building data lakes , data warehousing. Strong knowledge on python /Scala.",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"python, Bigdata, aws, Hive, Scala, Hadoop, pyspark",-,9am-6pm,"Full Time, Permanent",Reycruit,Organization,Reycruit,-,Hyderabad,Hyderabad,-,-,-,25-40 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"Hands on development experience on Hadoop, Spark, Hive,Cicd,Scala, candidate should be good with advanced programming language (scala/python java) Or we can consider 3-5 year candidate with NIIT or IIT education background -Pan India -N.P- 15-21 Days",70524007479,07-05-2024,05-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Hive, Hadoop, Big Data, Ci/Cd, Spark, SCALA",-,9am-6pm,"Full Time, Permanent",IT SCIENT,Organization,IT SCIENT,https://img.naukimg.com/logo_images/groups/v1/115480.gif,"Kolkata, Hyderabad, Pune, Ahmedabad, Chennai, Bengaluru, Delhi / NCR, Mumbai (All Areas)","Kolkata, Hyderabad, Pune, Ahmedabad, Chennai, Bengaluru, Delhi / NCR, Mumbai (All Areas)",-,-,-,17-22.5 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"Python, R, SPARK, Py-SPARK Professional","       a. Azure working knowledge - 100% - required             b. Primary skills - Python, R, SPARK, Py-SPARK - one or more of these required                 c. Working experience in Azure cloud - required                 d. Hadoop knowledge - good to have                 e. Working knowledge - cloud era, Jupyter note book, data bricks workbenches - one of them is required                 f. Machine Learning models - optional                 Python, R, SPARK, Py-SPARK          Strong experience and Knowledge in Azure, Primary skills - Python, R, SPARK, Py-SPARK                 Closely work with client and support team to review and manage the deliverables.                     Good communication skills       ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"C, spark, Machine learning, Cloud, Hadoop, Management, Python",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"  Position Overview: As a Big Data Developer, you will be responsible for designing, developing, and maintaining scalable data pipelines and applications for processing and analyzing large volumes of data. You will work closely with cross-functional teams to understand data requirements and implement robust solutions to address business needs. Location:  [City, State/Country] Responsibilities: Design, develop, and maintain distributed data processing pipelines using Apache Spark and Scala. Implement data ingestion, transformation, and storage solutions for structured and unstructured data sources. Optimize performance and scalability of data processing applications to handle large-scale data sets efficiently. Collaborate with data scientists and analysts to understand data requirements and implement data-driven solutions. Troubleshoot and debug data processing issues, ensuring data integrity and reliability. Continuously research and evaluate new technologies and tools to enhance our data processing capabilities. Work in an Agile environment, participating in sprint planning, code reviews, and other Agile ceremonies. Document design decisions, APIs, and technical specifications.",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SCALA, Big Data, Hadoop",-,9am-6pm,"Full Time, Permanent",Artech,Organization,Artech,https://img.naukimg.com/logo_images/groups/v1/4655781.gif,"Pune, Chennai, Bengaluru","Pune, Chennai, Bengaluru",-,-,-,"50,000-90,000 P.A ",,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Lead,"       Data Lake implementation using Big Data technologies         Understand Data Lake requirements and transform those to design and build       Propose the new ideas/automation to reduce manual efforts       Handle Hadoop installation, configuration, and support       Write MapReduce coding for Hadoop clusters; help to build new Hadoop clusters       Convert hard and complex techniques as well as functional requirements into the detailed designs       Pre-processing of data using Pig, Hive, Spark Streaming       Lead and mentor team members       Work on technical resolution for incidents and identify technical root cause       Engage with key stakeholders (Business, Markets, Devs, Vendors)       Ensure team code is compliant with code quality and standards     ",1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Automation, Coding, Analytical, Social media, OLAP, HTML, Scheduling, Advertising, Analytics",-,9am-6pm,"Full Time, Permanent",Umbrella Infocare,Organization,Umbrella Infocare,https://img.naukimg.com/logo_images/groups/v1/4636547.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Architect,"   The decision-making power for data analysis and he/she should also possess the quality of architecting the massive data          Should have skills in big data tools and technologies; it includes technologies like the Hadoop, accumulo, MapReduce, Hive, HBase, panoply and redshift          Overall 10 years of experience as Principal Consultant, Subject Mater Expert working on Big Data Platforms (Hadoop, Map Reduce, NoSQL, Real Time Streaming, Search, Spark, Java ETL platforms)          Experience with managing and handling very large data repositories, delivering distributed and highly scalable applications          End-to-end design and build process of Near-Real Time and Batch Data Pipelines; expertise with SQL and Data modeling working in Agile development process          Ability to work with large data sets: Big Data involves large data sets, so applicants must be able to work with highly diverse data in multiple types and formats, and sources          Self-starter: They must be able to work with minimal supervision          Ability to quickly prototype, architect and build software using latest/greatest technologies          Good Customer facing, interpersonal and communication skills          Experience in addressing non-functional requirement s and large application deployment architectures and concerns such as scalability, performance, availability, reliability, security etc          Experience in any one or more of the following technologies        o Experience on Hadoop (Apache/Cloudera/Hortonworks) and/or other Map Reduce Platforms      o Experience on Hive, Pig, Sqoop, Flume and/or Mahout      o Experience on NO-SQL HBase, Cassandra, MongoDB (Any one NO SQL would do)      o Experience on Spark, Storm, Kafka (Spark and Kafka is a must)      o Experience around Search Platform Solr, Elastic      o Streaming Data load organizing in Hadoop,        Good background of Configuration Management/Ticketing systems like Maven/Ant/JIRA etc.          Strong in Shell Scripting programming, Java, EDW platforms          Knowledge around any Data Integration and/or EDW tools is plus          Extensive experience with Enterprise Messaging framework (Kafka or RabbitMQ)          Must be able to provide out of the box solutions when ever required.          Preferred Skills:          Data Integration, SQL, NO SQL, CLOUDERA , ADVANCED JAVA          Teamwork abilities: The big data architect must be able to work in a team-oriented environment with people of diverse skill sets          Analytical skills: It is important that big data architects can analyze complex problems using information provided understand customer requests, and provide the appropriate solution          Data warehousing        o Strong Data warehousing concepts      o Strong Data Modelling skills (analytical models), preferably with ERWin      o Must have implemented end-to-end DWH projects (discovery, requirements gathering,      HLD/LLD documentation, development, testing, SIT, UAT, deployment, pre and post prod checks etc)      o Must have worked with Teradata / Greenplum / Synapse / Big Query / Snowflake        Data Integration / Data Engineering        o Strong logical and analytics abilities in DI/DQ      o Very strong skills in writing logical code for data engineering, which has re-usable capabilities without rework      o Strong knowledge and technical expertise in ETL tools such as Talend / Informatica / Datastage / Pentaho etc.        Data Quality        o Strong understanding and experience in DQ frameworks      o Must have implemented atleast 2 projects involving end-to-end DQ frameworks      o Very strong skills in writing logical code for data quality, which has re-usable capabilities without rework        Good to Have Skills:          EDW Tools experience like Teradata, Netezza etc.          Qualifications and Education Requirements:          Any Degree    ",90823501863,09-08-2023,07-11-2023,EducationalOccupationalCredential,120,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Data modeling, Configuration management, Datastage, Shell scripting, HTML, Informatica, Apache, Teradata, Analytics, SQL",-,9am-6pm,"Full Time, Permanent",Edgematics,Organization,Edgematics,-,"Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
BIGDATA Professionals,"   IDESLABS PRIVATE LIMITED is looking for BIGDATA to join our dynamic team and embark on a rewarding career journeyData Processing: Developing and maintaining data processing applications that can handle large volumes of data efficiently      This often involves real-time and batch processing      Data Integration: Integrating data from various sources, including databases, data lakes, and external data streams      Data Transformation: Cleaning, transforming, and enriching raw data to make it suitable for analysis and reporting      Database Management: Working with NoSQL databases like Hadoop, Cassandra, or MongoDB, and traditional relational databases like SQL Server or Oracle      Big Data Frameworks: Utilizing big data frameworks and technologies such as Apache Hadoop, Apache Spark, Apache Kafka, and others to process and analyze data      Programming Languages: Proficiency in programming languages commonly used in big data development, such as Java, Python, Scala, or R      Data Storage: Setting up and maintaining data storage solutions, including distributed file systems and cloud-based storage services      Data Security: Implementing security measures to protect sensitive data, including encryption and access control    ",1.31E+11,13-12-2023,12-03-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"NoSQL, data security, cassandra, SCALA, Data processing, MongoDB, Oracle, big data, SQL, Python",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Hadoop Admin,"   Good understanding of SDLC and agile methodologiesInstallation and configuration of Hadoop clusters, including HDFS, MapReduce, Hive, Pig, HBase, and other related tools      Managing and monitoring Hadoop clusters to ensure high availability and performance      Planning and implementing data backup and disaster recovery strategies for Hadoop clusters      Proactively monitoring and tuning Hadoop cluster performance to optimize resource utilization and prevent bottlenecks      Providing technical support to developers and end-users as needed      Awareness of latest technologies and trends      Logical thinking and problem solving skills along with an ability to collaborate    ",1.21E+11,12-10-2023,10-01-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Resource utilization, Disaster recovery, Hadoop, Agile, hdfs, Management, Technical support, SDLC, Monitoring, HBase",-,9am-6pm,"Full Time, Permanent",Ipropal,Organization,Ipropal,-,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
BigData Professional,"   IDESLABS PRIVATE LIMITED is looking for BigData Professional to join our dynamic team and embark on a rewarding career journey     Data Processing: Developing and maintaining data processing applications that can handle large volumes of data efficiently     This often involves real-time and batch processing     Data Integration: Integrating data from various sources, including databases, data lakes, and external data streams     Data Transformation: Cleaning, transforming, and enriching raw data to make it suitable for analysis and reporting     Database Management: Working with NoSQL databases like Hadoop, Cassandra, or MongoDB, and traditional relational databases like SQL Server or Oracle     Big Data Frameworks: Utilizing big data frameworks and technologies such as Apache Hadoop, Apache Spark, Apache Kafka, and others to process and analyze data     Programming Languages: Proficiency in programming languages commonly used in big data development, such as Java, Python, Scala, or R     Data Storage: Setting up and maintaining data storage solutions, including distributed file systems and cloud-based storage services     Data Security: Implementing security measures to protect sensitive data, including encryption and access control   ",70324502342,07-03-2024,05-06-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"big data administration, python, oracle, scala, batch processing, relational databases, sql server, nosql, sql, java, encryption, cassandra, spark, kafka, integration, hadoop, data transformation, big data, mongodb, data integration, programming, data lake",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,"Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru","Kolkata, Mumbai, New Delhi, Hyderabad, Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Consultant,"We are currently hiring for below openings. Role - Big Data Engineer/Lead Experience - 6 to 14 years Domain - Banking Skills - Azure Databricks, Azure Data Factory, Pyspark, Python Positions open - 10 Location - Chennai/Bangalore Mode of work - 5 days from office Timings - 1pm to 10 pm IST   JD : Experience in Pyspark, SQL, Cloud data warehouse like Azure Synapse, databricks Experience working with structured and unstructured data Extensive knowledge of Data Warehousing concepts, strategies, methodologies Experience in ETL/Pipeline Development using tools such as Azure Databricks and Azure Data Factory with development expertise on batch and real-time data integration Experience in data ingestion, preparation, integration, and operationalization techniques in optimally addressing the data requirements Experience in relational data processing technology like MS SQL, Delta Lake, Spark SQL, SQL Server Experience with Orchestration tools, Azure DevOps, and GitHub Must be team oriented with strong communication, collaboration, prioritization, and adaptability skill Kindly share your updated resume on ""bhoomim@hexaware.com"" along with below details. Current CTC -  Expected CTC -  Notice Period -  Current Location -  Preferred Location -  PN: Kindly apply for this role only if you are comfortable with 5 Days WFO and Timings. Thanks, Bhoomi",90524004987,09-05-2024,07-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Databricks, Azure Data Factory, Pyspark, Python, SQL",-,9am-6pm,"Full Time, Permanent",Hexaware Technologies,Organization,Hexaware Technologies,https://img.naukimg.com/logo_images/groups/v1/12466.gif,"Bangalore Rural, Chennai","Bangalore Rural, Chennai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Scala / Spark Developer,"   Understand and implement tactical or strategic solutions for a given business problem.            Discussion with stakeholders on business needs and technology requirements.      Define and derive strategic solutions as well as identify tactical solutions when necessary.      Write technical design & other solution documents per Agile (SCRUM) standards.      Perform data analysis to aid development work and other business needs.      Perform unit testing of the developed code leveraging automated BDD test framework.              Role requirements      5+ years of experience as a Hands-on Scala/Spark developer.      Hands on SQL writing skills on RDBMS (DB2) databases.      Ability to perform code optimization for performance, Scalability and configurability.      Must have worked at least 2 years in a HDFS platform development project.      Proficiency in data analysis, data profiling, and data lineage.      Experience working in Agile projects.      ",60524500195,06-05-2024,04-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Recruitment / Staffing,"visualforce, algorithms, c++, python, project management, data analysis, rdbms, platform development, scala, unit testing, dbms, sql, apex, salesforce, sales force development, java, product management, spark, linux, scrum, data structures, agile, aws, data profiling",-,9am-6pm,"Full Time, Permanent",The Edge Partnership,Organization,The Edge Partnership,https://img.naukimg.com/logo_images/groups/v1/1043764.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Python Data Developer,"We are looking for a Remote Senior Python Developer to join our team. You will be responsible for developing and maintaining Python-based applications and solutions. You will work closely with the development team to ensure efficient and high-quality code.  Responsibilities: Develop and maintain Python-based applications and solutions. Collaborate with the development team to design and implement efficient code. Participate in code reviews and troubleshooting activities. Create and maintain technical documentation. Ensure the performance, quality, and responsiveness of applications. Requirements: Strong proficiency in Python programming language. Good understanding of software development principles and best practices. Excellent problem-solving and analytical skills. Ability to work independently as well as in a team. 5-10 years  of experience Perks & Benefits:  Insurance Coverage; Paid Leaves  including maternity, bereavement, paternity, and special COVID-19 leaves; EPAM Hobby Clubs; Certification trainings for technical and soft skills; Access to unlimited LinkedIn Learning platform;  Access to internal learning programs set up by world class trainers;",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Data Pipeline, SQL, Python, Algorithms, Data Structures",-,9am-6pm,"Full Time, Permanent",Epam Systems,Organization,Epam Systems,https://img.naukimg.com/logo_images/groups/v1/4592441.gif,India,India,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineers,"  Exceptional Spark Scala engineer with 5+ yrs experience who will be responsible for: Implementing the large-scale Spark applications and finetune at runtime Design and implement high volume and cardinality data using Iceberg Implement and optimize pipelines using Airflow Experience with SQL, CICD, Gradle build tool, docker Familiar with data -> rest web server integration, Salesforce experience is preferred Contribute to frameworks, while working to triage issues and and define and manage SLA for application Strong communication skills to collaborate effectively with technical and non-technical stakeholders Proficient in Python and PySpark",1.10E+11,11-04-2024,10-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Pyspark, Scala, Python, Jenkins, Airflow, Iceberg, Trino, Salesforce Apex",-,9am-6pm,"Full Time, Permanent",Nucleusteq Consulting,Organization,Nucleusteq Consulting,-,"Indore, Hyderabad, Bangalore Rural","Indore, Hyderabad, Bangalore Rural",-,-,-,30-45 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Java Bigdata / Pulsar,"Role & responsibilities     Software Engineering  Backend Development Big Data Java Python Flink Pulsar We need Pulsar Experts who will play a pivotal role in providing support to tickets related to designing, developing, and managing the operational aspects of the Apache Pulsar-based data streaming and messaging solutions. The ideal candidate should have a deep understanding of stream processing, data engineering, and the Apache Pulsar ecosystem. They will be responsible for architecting and implementing Pulsar applications, optimizing performance, and ensuring reliable operation. Responsibilities: The      engineer will be responsible for Documenting and Knowledge Sharing. Creating      runbooks. Troubleshooting      guidelines. Requirements: Proven      experience designing and developing Apache Pulsar applications in      production environments. Strong      programming skills in Java or Python, with a focus on distributed systems. Deep      understanding of Pulsar's architecture, including broker, bookie, and      tiered storage components. Proficiency      in cluster management tools such as Apache Bookkeeper, Kubernetes, or      Docker. Experience      with message queues, pub/sub systems, and stream processing frameworks. Excellent      problem-solving and debugging skills. Strong      communication and collaboration skills for cross-functional teamwork. A      commitment to staying current with Pulsar developments and industry best      practices. Good      to have Certification in Apache Pulsar. Experience      with cloud-based Pulsar deployments (e. g., Azure Event Hubs, AWS Pulsar      Service). Preferred candidate profile   Perks and benefits  ",1.60E+11,12-04-2024,11-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,Software Product,"Apache Pulsar, java, Apache Flink, Big Data, development & coding",-,9am-6pm,"Full Time, Permanent",Innominds Software,Organization,Innominds Software,https://www.naukri.com/hotjobs/images/v3/innominds1.gif,"Hyderabad,Telangana, Pune,Maharashtra, Bengaluru","Hyderabad,Telangana, Pune,Maharashtra, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
BigData - Hadoop / Scala Engineer,"Role Description  Senior Engineer is responsible for developing and delivering elements of engineering solutions to accomplish business goals. Awareness is expected of the important engineering principles of the Bank. Root cause analysis skills, developed through addressing enhancements and fixes to products. Build reliability and resiliency into solutions through early testing, peer reviews and automating the delivery lifecycle. Successful candidate should be able to work independently on medium to large sized projects with strict deadlines. Should be able to work in a cross-application, mixed-technical environment and must demonstrate solid hands-on development track record while working on an agile methodology. The role demands working along-side a geographically dispersed team. The position is required as part of the build out of AFC Tech internal development team in India. The overall team will primarily deliver improvements in anti-financial crime capabilities that are major components of the regulatory portfolio, addressing various regulatory commitments to mandated monitors. Your key responsibilities  Analysing data sets and designing and coding stable and scalable data ingestion workflows, also integrating into existing workflows. Working with team members and stakeholders to clarify requirements and provide the appropriate ETL solution. Work as a senior developer for developing analytics algorithm on top of ingested data. Work as a senior developer for various data sourcing in Hadoop, also GCP. Own unit-testing, UAT deployment, End-User sign-off & Prod Go-Live. Ensuring new code is tested, both at unit level and at system level. Design, develop and peer review new code/functionality. Operate as a team member of an Agile Scrum team. Your Skills & Experience  More than 12+ years of coding experience in reputed organizations. Hands on experience on BitBucket, Github and related CICD pipelines. Proficient in Hadoop, Python, Spark, SQL, Unix, Hive, Understanding of On-Prem, & GCP data security. Hands on development experience on large ETL/Big Data systems. Hands on experience on GCP Dataproc, GCP Composer, GCP Big Query, Cloud Storage, etc. Basic understanding of Data Quality dimensions like Consistency, Completeness, Accuracy, Lineage etc. Knowledge of data architectures such as Data Mesh, Data virtualization, Hybrid cloud environments, and latest industry trends including data observability. Basic understanding of CloudBuild, ArtifactRegistry, CloudDNS, CloudLoadBalancing etc. Hands on business and systems knowledge gained in a regulatory delivery environment. Banking experience, regulatory and cross-product knowledge. Passionate about test driven development. Good to have - Data visualisation experience in any reporting/ analytical tool like Tableau and/or GCP Looker, GCP data Studio GCP data certification would be a plus.",10524905406,01-05-2024,30-07-2024,EducationalOccupationalCredential,144,Engineering - Software & QA,Big Data Engineer,Investment Banking / Venture Capital / Private Equity,"BigData, Unix, unit-testing, Github, ArtifactRegistry, Scala, Hadoop, CloudBuild, test driven development, CloudDNS, SQL, coding, Hive, Spark, ETL, Data Mesh, Data virtualization, Python",-,9am-6pm,"Full Time, Permanent",Deutsche Bank,Organization,Deutsche Bank,https://img.naukimg.com/logo_images/groups/v1/468918.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata Engineer,"  Hi,       Hope you are looking for a job change. We have opening for  Data Engineer  for an MNC in  Hyderabad, I'm sharing JD with you. Please have a look and revert with below details and Updated Resume.  Apply only if you can join in 15 Days. Its 5-Days Work from Office  . Role Data Engineer Experience:   6- 12Years Mode: Permanent Work: Client Location:  Hyderabad Notice Period: immediate to 15 Days Mandatory Skills:    AWS      Data services Bigdata      Hadoop Spark SQL Databases mentioned in JD including DBA experience Full Name: Email ID: Mobile Number: Alternate No: Total Experience: Relevant experience: Current Organization: Working as Permanent Employee: Payroll Company: Experience in AWS: Experience in Spark: Experience in Python:  Which version in AWS-S3: Experience in Data Pipeline: Experience in Redshift : Experience in Apache Airflow : Experience in Lamda : Notice period: Current location: Preferred location: Current CTC: Exp CTC: Pan Card Number : Date of Birth : Any Offer in Hand LWD: Any Offer in hand: Serving Notice Period: Can you join Immediately:  Job Description: Mandatory Skills:    AWS      Data services Bigdata      Hadoop Spark SQL 5 DBA experience Data Engineering Lead  Role Overview: The Data Engineering Lead will be responsible for overseeing the design, development, and maintenance of our data infrastructure. This role involves leadership responsibilities, including managing a team of data engineers, setting technical direction, and collaborating with cross-functional teams to deliver robust data solutions for Old Mutual.  The role incumbent will have a strong technical background, leadership skills, and a proven track record of successfully leading data engineering projects. Key Responsibilities: Technical Leadership:  Lead the development and implementation of data architecture and engineering solutions. Provide technical direction and guidance to the data engineering team. Team Management:  Manage and mentor a team of data engineers, fostering a collaborative and innovative team culture. Conduct regular performance assessments, set goals, and provide constructive feedback. Project Management:  Oversee the planning, execution, and delivery of data engineering projects. Collaborate with stakeholders to define project requirements and ensure successful project outcomes. Data Strategy and Roadmap:  Contribute to the development of the company's data strategy and roadmap. Collaborate with senior management to align data initiatives with overall business goals. Cross-functional Collaboration:  Work closely with data scientists, analysts, software developers, and other teams to understand business needs and ensure effective data solutions. Act as a key liaison between the data engineering team and other business units. Quality Assurance and Best Practices:  Ensure the implementation of data governance policies, best practices, and industry standards. Lead efforts to maintain high data quality, integrity, and compliance with regulatory requirements. Innovation and Technology Adoption:  Stay abreast of emerging technologies and industry trends in data engineering. Evaluate and recommend new tools and technologies to enhance the data engineering capabilities of the team. Qualifications: Bachelor's or Master's degree in Computer Science, Information Technology, or a related field. 10+ years of experience in data engineering with a focus on leadership and project management. Proven experience managing and leading a team of data engineers. Expert proficiency in programming languages such as Python, SQL, and advanced scripting languages. In-depth knowledge of data warehousing concepts, big data technologies, and cloud platforms (e.g., AWS, Azure, Google Cloud). Strong analytical and problem-solving skills with a strategic mindset. Excellent communication and collaboration skills. Preferred Skills: Certifications in relevant data engineering, cloud, or leadership domains. Experience with real-time data processing and streaming technologies. Knowledge of containerization and orchestration tools (e.g., Docker, Kubernetes). Familiarity with data security and privacy considerations. Thanks & Regards, Rejeesh S Email :  rejeesh.s@jobworld.jobs Mobile : +91-9188336668",90524007268,09-05-2024,07-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"AWS data Services, Hadoop, Big Data, AWS, SQL, Dba Skills, Spark",-,9am-6pm,"Full Time, Permanent",Jobworld India,Organization,Jobworld India,-,Hyderabad,Hyderabad,-,-,-,16-27.5 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Manager,"  Job Title / Primary Skill:  Big Data Developer (Associate Manager) Management Level: G150 Years of Experience:  8 to 14 years Job Location:  Bangalore (Hybrid) Must Have Skills:  Big data, Spark, Scala, SQL, Snowflake Good To Have Skills:  HDFS, HIVE, IMPALA   Educational Qualification:  BE/BTech/ MTech/ MCA Job Description: We are looking for developers to have good knowledge of Scala programming skills and Knowledge of SQL Technical Skills: Scala, Python -> Scala is often used for Hadoop-based projects, while Python and Scala are choices for Apache Spark-based projects. SQL -> Knowledge of SQL (Structured Query Language) is important for querying and manipulating data  Shell Script -> Shell scripts are used for batch processing of data, it can be used for scheduling the jobs and shell scripts are often used for deploying applications Spark Scala -> Spark Scala allows you to write Spark applications using the Spark API in Scala Spark SQL -> It allows to work with structured data using SQL-like queries and Data Frame APIs. We can execute SQL queries against Data Frames, enabling easy data exploration, transformation, and analysis. Role and responsibilities: Data Ingestion: Collecting and importing data from various sources, such as databases, logs, APIs into the Big Data infrastructure. Data Processing: Designing data pipelines to clean, transform, and prepare raw data for analysis. This often involves using technologies like Apache Hadoop, Apache Spark. Data Storage: Selecting appropriate data storage technologies like Hadoop Distributed File System (HDFS), HIVE, IMPALA, or cloud-based storage solutions (Snowflake, Databricks). Data Analysis: Developing algorithms and implementing data processing techniques to extract meaningful insights, conduct statistical analysis( build machine learning models is advantage). Performance Optimization: Tuning and optimizing the performance of Big Data applications and infrastructure to ensure efficient data processing and reduced latency. Data Security: Implementing security measures to protect sensitive data and ensuring compliance with data protection regulations. Integration: Integrating Big Data solutions with existing enterprise systems and applications. Monitoring and Troubleshooting: Monitoring data pipelines and processes to identify and resolve issues or bottlenecks in the system. Collaboration: Collaborating with data scientists, data engineers, and other stakeholders to understand requirements and deliver valuable data-driven solutions. Continuous Learning: Staying updated with the latest Big Data technologies, tools, and industry trends to improve skills and enhance productivity. Professional Attributes:   Should have good communication skill.  Team player willing to collaborate throughout all phases of development, testing and deployment.  Ability to solve problems and meet the deadlines within minimal supervision.  Note: - Interested candidates can share his/her updated CV on raviranjankumar.singh@iqvia.com.",70523001302,19-04-2024,18-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SCALA, Big Data, Spark, SQL, Pyspark, Hadoop, Spark SQL, Azure Databricks, Python",-,9am-6pm,"Full Time, Permanent",IQVIA,Organization,IQVIA,https://img.naukimg.com/logo_images/groups/v1/3248522.gif,"Mysore/Mysuru, Bangalore/Bengaluru","Mysore/Mysuru, Bangalore/Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"Data Engineer (ETL, Big Data, Hadoop, Spark, GCP)","Senior Engineer is responsible for developing and delivering elements of engineering solutions to accomplish business goals. Awareness is expected of the important engineering principles of the Bank. Root cause analysis skills, developed through addressing enhancements and fixes to products. Build reliability and resiliency into solutions through early testing, peer reviews and automating the delivery lifecycle. Successful candidate should be able to work independently on medium to large sized projects with strict deadlines. Should be able to work in a cross-application, mixed-technical environment and must demonstrate solid hands-on development track record while working on an agile methodology. The role demands working along-side a geographically dispersed team. The position is required as part of the build out of AFC Tech internal development team in India. The overall team will primarily deliver improvements in anti-financial crime capabilities that are major components of the regulatory portfolio, addressing various regulatory commitments to mandated monitors. Your key responsibilities  Analysing data sets and designing and coding stable and scalable data ingestion workflows, also integrating into existing workflows. Working with team members and stakeholders to clarify requirements and provide the appropriate ETL solution. Work as a senior developer for developing analytics algorithm on top of ingested data. Work as a senior developer for various data sourcing in Hadoop, also GCP. Own unit-testing, UAT deployment, End-User sign-off & Prod Go-Live. Ensuring new code is tested, both at unit level and at system level. Design, develop and peer review new code/functionality. Operate as a team member of an Agile Scrum team. Skills & Experience  6+ years of coding experience in reputed organizations. Hands on experience on BitBucket, Github and related CICD pipelines. Proficient in Hadoop, Python, Spark, SQL, Unix, Hive, Understanding of On-Prem, & GCP data security. Hands on development experience on large ETL/Big Data systems. Hands on experience on GCP Dataproc, GCP Composer, GCP Big Query, Cloud Storage, etc. Basic understanding of Data Quality dimensions like Consistency, Completeness, Accuracy, Lineage etc. Basic understanding of data architectures such as Data Mesh, Data virtualization, Hybrid cloud environments, and latest industry trends including data observability. Basic understanding of CloudBuild, ArtifactRegistry, CloudDNS, CloudLoadBalancing etc. Hands on business and systems knowledge gained in a regulatory delivery environment. Banking experience, regulatory and cross-product knowledge. Passionate about test driven development. Good to have - Data visualisation experience in any reporting/ analytical tool like Tableau and/or GCP Looker, GCP data Studio GCP data certification would be a plus.",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,Investment Banking / Venture Capital / Private Equity,"Data Engineering, GCP, ArtifactRegistry, Hadoop, Big Data, CloudBuild, Spark, ETL, CloudDNS",-,9am-6pm,"Full Time, Permanent",Deutsche Bank,Organization,Deutsche Bank,https://img.naukimg.com/logo_images/groups/v1/468918.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"Tcs is Hiring_Bigdata Spark,Scala Developers_Bangalore/Kochi","TCS has always been in the spotlight for being adept in ""the next big technologies"" What we can offer you is a space to explore varied technologies and quench your techie soul. Role- Big Data Developer Location: Bangalore/Kochi Experience:7  12 Years Required Technical skills: Bigdata, spark, scala Must Have: 5+ years of experience in Big Data Sound Knowledge in big data technologies like Hadoop, Hive, PySpark, Scala Excellent knowledge in SQL and PL SQL ETL and Unix knowledge Scripting knowledge like shell scripting, python etc Scheduling tools like AutoSys Good experience in Agile projects Flexibility to work in complex work environment dealing with multiple stakeholders Good to Have:  Proactive in learning new technologies Responsibilities: Experience in Data migration Excellent knowledge Development Projects Experience in Agile methodology Minimum Qualification:  15 years of full-time education.  Minimum percentile of 50% in 10th, 12th, UG & PG (if applicable) Interested candidates can share your resume to kavya.dammala@tcs.com",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,84,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"bigdata, spark, scala",-,9am-6pm,"Full Time, Permanent",Tata Consultancy Services (TCS),Organization,Tata Consultancy Services (TCS),https://img.naukimg.com/logo_images/groups/v1/223346.gif,"Kochi, Bengaluru","Kochi, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"  Bigdata, Spark, Scala/Python  Bigdata: 5-6 years of Hands-on experience in Hadoop / Spark (Scala or Python), Hbase, Hive, Sqoop.  Knowledge on Database architectures of RDBMS & No-SQL. Good in writing optimized SQL queries which give good performance.  AWS: Working knowledge on Lambda, EC2, ECS, EMR, Athena, S3.  Working knowledge on Git/bitbucket.  AWS Debugging: Very good at debugging issues in AWS CloudWatch  Hands-on and good coding skills with Scala or Python. ",1.31E+11,13-05-2024,11-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Hive, NoSQL, SCALA, Spark, Python",-,9am-6pm,"Full Time, Permanent",Tata Consultancy Services (TCS),Organization,Tata Consultancy Services (TCS),https://img.naukimg.com/logo_images/groups/v1/223346.gif,Kolkata,Kolkata,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
BigData-Hadoop/Scala Engineer,"Role Description Engineer is responsible for managing or performing work across multiple areas of the bank's overall IT Platform/Infrastructure including analysis, development, and administration. It may also involve taking functional oversight of engineering delivery for specific departments. Work includes:  Planning and developing entire engineering solutions to accomplish business goals Building reliability and resiliency into solutions with appropriate testing and reviewing throughout the delivery lifecycle Ensuring maintainability and reusability of engineering solutions  Ensuring solutions are well architected and can be integrated successfully into the end-to-end business process flow  Reviewing engineering plans and quality to drive re-use and improve engineering capability  Participating in industry forums to drive adoption of innovative technologies, tools and solutions in the Bank. Your key responsibilities  This role is for Software Engineer responsible for Design and Developing software applications. The candidate is expected to work closely with Team Leads or Software Development Managers and other key stake holders to ensure good quality, maintainable, scalable and high performing software applications are delivered to users. Should be coming from a strong technological background. Should be hands on and be able to work independently requiring minimal technical/tool guidance. Should have good communication skill and strong positive outlook. Experience working in Agile and Scrum delivery. Should be able to contribute towards good software design. Participate in daily stand-up meetings. Analyze software defects and fix them in timely manner. Work closely with Functional Analysis and Quality Assurance teams Your skills and experience Extensive experience with Hadoop & Java/Scala related technologies such as Understanding of Hadoop ecosystem and HDFS file System. Understanding of different types of nodes in a Hadoop cluster. Experience with MapReduce framework. Experience with Hive, Pig, Impala, Spark SQL etc Experience with Apache Spark and Scala (preferably greater than version 2.0) Experience Spark job optimization Experience with File handling such as Avro/Parquet etc Good understanding of BEELINE command to connect to HIVE server Tuning and Troubleshooting, experience with profiling and monitoring tools Experience on Scala or Java preferably Spring Batch/Core/Boot Experience with Messaging and integration, Patterns, REST, SOA Experience with build and deployment tools like Maven/Artifactory/Teamcity or Jenkins Unix scripting and hands on experience Nice to have Experience with cloud technologies such as Docker, Kubernetes, Openshift, Azure, GCP Experience with UI frameworks like Angular or React RDBMSOracle design, development, tuning",80524904649,13-05-2024,11-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,Investment Banking / Venture Capital / Private Equity,"BigData, Azure, scala, Spark SQL, Openshift, Impala, Pig, Hive, java, mapreduce, Docker, spark, GCP, hdfs, hadoop, avro, Kubernetes",-,9am-6pm,"Full Time, Permanent",Deutsche Bank,Organization,Deutsche Bank,https://img.naukimg.com/logo_images/groups/v1/468918.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"Role & responsibilities  :-   First and most important  Sound understanding of data structures & SQL concepts and experience in writing complex SQLespecially around OLAP systems. Sound knowledge of the ETL tool like informatica, 5+ years of experience, Big Data technologies like Hadoop ecosystem, its various components, along with different tools including Spark, Hive, Sqoop,etc. In-depth knowledge of MPP/distributed systems. Preferred candidate profile  :-   The ability to write precise, scalable, and high-performance code. Knowledge/Exposure in data modeling with OLAP (Optional). 5 Plus years of experience into Scala Development. 5 Plus years of experience into GCP Development. Perks and benefits  ",80524010045,08-05-2024,06-08-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"GCP, Hadoop Development, SCALA, Coding",-,9am-6pm,"Full Time, Permanent",Cielo Talent India,Organization,Cielo Talent India,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"  Must Haves:[Candidate must have atleast 4 years of exp with these] Candidate must be very well versed ETL coding and Pipleline maintainance. Must have exp with SQL, python, pyspark Candidate must have exp working on Hadoop and Hive and must be aware of Spark Framework and design. Good to have : Knowledge on open source cloud platforms. Partial knowledge on automation testing Hands on? exp using? Linux commands and handling large data pipelines.",30424012732,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Etl Pipelines, Pyspark, Hive, Hadoop, Big Data, SQL Queries, Automation Testing, Spark, Python",-,9am-6pm,"Full Time, Permanent",Trigent Software,Organization,Trigent Software,https://img.naukimg.com/logo_images/groups/v1/125596.gif,Bengaluru,Bengaluru,-,-,-,15-22.5 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer - Hive/Spark,"- Solid Experience of software development experience and leading teams of engineers and scrum teams. - 3+ years of hands-on experience of working with Map-Reduce, Hive, Spark (core, SQL and Java). - Solid Data warehousing concepts. - Knowledge of Financi",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data, Unix, Java, Hive, Data Pipeline, Spark, Data Warehousing, SQL, MapReduce",-,9am-6pm,"Full Time, Permanent",Forward Eye Technologies,Organization,Forward Eye Technologies,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Java Spark Developer,"Job Title: Java + Spark Location: Pune/Mumbai/Chennai/Bangalore/Hyderabad Notice Period: Immediate to 90 Days  Interested Candidates can apply through given below link:   https://r.ripplehire.com/s/bLS7M OR can share your resume on  ram.tondwal@ltimindtree.com Job Description: Should have 3.5-8 years Experienced in Java, Spark SQL and worked in Unix/Linux Background. Should have Good Knowledge in Oracle Database SQL Queries and Views. Experienced to understand the Requirement and adapt it using SDLC/Agile Process. Experienced in Banking Domain. (Optional) Understand the Agile Process, Code Check-in Tools, Deployment tool, Scheduler Tools. (Optional) Should have worked in SQL Databases like Oracle. Need to implement ETL data model for applications. Ability to learn and contribute to automate the tasks using shell scripting. Need to contribute individually for developing the shell scripts. Ability to understand the framework scripts and contribute for any enhancement. Need to have experience in Agile process. Need to have experience in Bitbucket, Jenkins, Jira etc.., Skills Hadoop-Spark-Kafka-Nifi -Python / Scala / Java ",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Java, Spark, hadoop, Scala",-,9am-6pm,"Full Time, Permanent",Ltimindtree,Organization,Ltimindtree,https://img.naukimg.com/logo_images/groups/v1/7519247.gif,"Pune, Bangalore Rural, Chennai","Pune, Bangalore Rural, Chennai",-,-,-,10-20 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Spark / Scala Developer,Work Location:     Chennai/Pune/Bangalore/Delhi/Hyd     Experience: 4-8yrs Job Description: Hands on experience in Spark Experience in Scala Experience in Big data technologies Please share your updated resume to suganya@spstaffing.in or can reach me @ 7305441424.,2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Scala, Spark, Hadoop, pyspark, Mapreduce, Flume, Hdfs, Impala, Spark Streaming, YARN, HBase, Apache Storm, Hive, Sqoop, Hue, Oozie",-,9am-6pm,"Full Time, Permanent",SP Staffing,Organization,SP Staffing,-,"Pune, Chennai, Bengaluru","Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"Position Description We are looking for an Engineer with hands-on Big Data experience who will work on the internal and customer-based projects for Bridgenext, someone who cares about the quality of the code and who is passionate about providing the best solution to meet the client's needs and anticipates their future needs based on an understanding of the market. Must Have Skills:   4-6 years of overall experience. 3 years of technical solutions experience including 2+ years in a combination of relevant Big Data/Analytics areas including Hadoop and other industry Big Data frameworks, underlying infrastructure for Big Data solutions (clustered/distributed computing, storage, Data center networking). Deployment of a large distributed Big Data application. Experience building, and operating cloud services in IT, System integrator or service provider. High level of experience with Scala / Python. Ability to demonstrate micro/macro designing and familiar with unix commands and basic work experience in unix shell scripting. Technology expertise of solutioning in Hive, Spark (PySpark/Scala), SQL. Good to have AWS Cloud experience. Professional Skills: Solid written, verbal, and presentation communication skills. Strong team player, and individual contributor. Maintains composure during all types of situations and is collaborative by nature. High standards of professionalism, consistently producing high-quality results. Self-sufficient, independent requiring very little supervision or intervention. Demonstrate flexibility and openness to bring creative solutions to address issues.",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Pyspark, AWS, SQL, Python",-,9am-6pm,"Full Time, Permanent",Bridgenext India,Organization,Bridgenext India,https://img.naukri.com/logo_images/v3/5686648.gif,"Pune, Ahmedabad, Bengaluru","Pune, Ahmedabad, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Face 2 Face || Big Data Developer || Bengaluru,"Job Description:   Interact with business stake holders and designers to implement to understand business requirements. Hadoop development and implementation. Loading from disparate data sets. Good to have Teradata Knowledge Must have working experience in IntelliJ IDEA, AutoSys, WinSCP, Putty & GitHub. Designing, building, installing, configuring and supporting Hadoop. Transform data using spark & scala Translate complex functional and technical requirements into detailed design. Perform analysis of vast data stores and uncover insights. Maintain security and data privacy. Create scalable and high-performance web services for data tracking. High-speed querying. Managing and deploying HBase. Test prototypes and oversee handover to operational teams. Propose best practices/standards. Interested candidates share your CV to adithyan@wrootsglobal.in Note:-   We are looking for Immediate to 30 days of Notice period",1.60E+11,16-04-2024,15-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Pyspark, Big Data developer, Hadoop developer, Hive, Scala, Hdfs, Hbase, Python",-,9am-6pm,"Full Time, Permanent",Wroots Global,Organization,Wroots Global,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Spark Developer," . As a Spark developer you will manage the development of scalable distributed Architecture defined by the Architect or tech Lead in our team.      . Analyse, assemble large data sets to designed for the functional and non-functional requirements.      . You will develop ETL scripts for big data sources.      . Identify, design optimise data processing automate for reports and dashboards.      . You will be responsible for workflow optimizations, data optimizations and ETL optimization as per the requirements elucidated by the team.      . Work with stakeholders such as Product managers, Technical Leads Service Layer engineers to ensure end-to-end requirements are addressed.      . Strong team player to adhere to Software Development Life cycle (SDLC) and documentations needed to represent every stage of SDLC.      General Qualifications:      . BE/BTech/MCA/MTech/MSC with 3-5 years of experience with Apache Spark development or in alternative Data Engineering analytics frameworks.      . Overall experience in the field Java/Scala as a Backend developer could be between 3 years.      . Working experience in Bigdata is important.      Technical Skills      . Programming experience in Java/Scala is needed. Experience in Python also will have an advantage.      . Writing ETL Stack in scalable optimized fashion using Apache Spark/Hadoop/Kafka etc      . Should have working experience in writing distributed optimized Apache Spark jobs for various Machine Learning Algorithms.      . Experience building and optimizing data pipeline and data sets is essential.      . Should have worked with Kafka, at least one of No SQL databases (eg Cassandra, MongoDB, elastic DB) and at least one of RDBMS (eg MySQL)      . Working experience to use cache such as Redis, Apache Ignite, Hazelcast will advantage.      . Working knowledge of Docker and Kubernetes will be a plus.      Kindly find the below inputs from customer to get quality profiles     Include the criteria apart from Spark Experience in No sql databases (elastic search OR Cassandra OR MongoDB), Apache Kafka.     Look for Spark Streaming (people who have used Sqoop, likely doesn t know Spark Streaming).     Scala or Java based work experience using Spark is a MUST. Python usage with Spark is optional, not must.       Spark streaming ,Kafka experience is mandatory for this requirement.  ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Backend, RDBMS, MySQL, Machine learning, Workflow, Apache, SDLC, Analytics, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer," 4-8 Yrs. experience overall       Mandatory:     Programming Languages : Scala     Big Data Frameworks: Spark, Hadoop/HDFS/Hive/Sqoop/Oozie, Kafka, HBase or any other NoSQL Store             Good to Have   : Streaming with Spark/Kafka, Cloud knowledge (AWS/Azure)Scheduling and creating workflows with Apache AirFlow   Excellent communication skills                                                       ",1.00E+11,10-01-2023,10-04-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"NoSQL, spark, Cloud, Hadoop, Programming, Scheduling, sqoop, Apache, big data, AWS",-,9am-6pm,"Full Time, Permanent",Xpheno,Organization,Xpheno,https://img.naukimg.com/logo_images/groups/v1/3075830.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Urgent Hiring For Pyspark Developer || Banking Client,"  IMMEDIATE OPENING FOR Pyspark Developer (BANKING CLIENT) Relevant Experience :  3+ yrs. Location :  Navi Mumbai Mode of Work  : Work From Office Notice Period :  Upto 20 days. Job Description:   1) Experience required in Pyspark Developer  2) Should be excellent using Pyspark, Spark, Python and Hive 3) Good Communication Skills Benefits of working at Acxiom:   - Statutory Benefits - Paid Leaves - Phenomenal Career Growth - Exposure to Banking Domain  About Acxiom Technologies:   Acxiom Technologies is a leading software solutions services company that provides consulting services to global firms and has established itself as one of the most sought-after consulting organizations in the field of Data Management and Business Intelligence. Also here is our website address  https://www.acxtech.co.in/  to give you a detailed overview of our company. Interested Candidates can share their resumes on  careers@acxtech.co.in   Kindly revert with the below details over an email Total Experience: Relevant Experience in Pyspark and SQL:  Current CTC:  Expected CTC: Notice Period (Official & if further negotiable):  Current location:  Preferred location:  Reason for job change:  Availability for video interview:  Thank you. ",20524005544,02-05-2024,31-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Pyspark, Hive, Pyspark Developer, Spark, Python, SQL",-,9am-6pm,"Full Time, Permanent",Acxiom Technologies,Organization,Acxiom Technologies,https://img.naukimg.com/logo_images/groups/v1/4673963.gif,Navi Mumbai,Navi Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Engineer - Big Data,"           American Express is looking for energetic, successful and highly skilled Engineers to help shape our technology and product roadmap       Our Software Engineers not only understand how technology works, but how that technology intersects with the people who count on it every day     Today, innovative ideas, insight and new points of view are at the core of how we create a more powerful, personal and fulfilling experience for our customers and colleagues, with Big Data and batch/real-time analytical solutions using ground-breaking technologies to deliver innovative solutions across multiple business units         This Engineering role is based in our Global Risk and Compliance Technology organization and will have a keen focus on platform modernization, bringing to life the latest technology stacks to support the ongoing needs of the business as well as     compliance against global regulatory requirements                                   Qualifications:                   Support the Compliance and Operations Risk big data delivery team in India to lead and assist in the design and actual development of applications.             Responsible for specific functional areas within the team, this involves project management and taking business specifications.             The individual should be able to independently run projects/tasks delegated to them.                               Technical Skills:                   Bachelor degree in Engineering or Computer Science or equivalent              2 to 5 years experience is required             Experience on Big Data (Spark Core and Hive) is good to have             Familiar with GCP offerings, experience building data pipelines on GCP a plus             Hadoop Architecture, having knowledge on Hadoop, Map Reduce, Hbase.             UNIX shell scripting experience is good to have             Creative problem solving (Innovative)             Benefits include:           Competitive base salaries          Bonus incentives          Support for financial-well-being and retirement          Comprehensive medical, dental, vision, life insurance, and disability benefits (depending on location)          Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need          Generous paid parental leave policies (depending on your location)          Free access to global on-site wellness centers staffed with nurses and doctors (depending on location)          Free and confidential counseling support through our Healthy Minds program          Career development and training opportunities       ",80524501875,08-05-2024,06-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,Financial Services,"Career development, GCP, Project management, Analytical, Finance, Open source, big data, Unix shell scripting",-,9am-6pm,"Full Time, Permanent",AMERICAN EXPRESS,Organization,AMERICAN EXPRESS,https://img.naukimg.com/logo_images/groups/v1/82242.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"Looking for candidates  in Big data engineer having exp in  python, pyspark ,AWS, Data engineer, scala  Exp:4-6yrs Location: Hyderabad(WFO)",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Pyspark, Big Data, AWS, Python, SCALA, EMR, Aws Glue, Athena",-,9am-6pm,"Full Time, Permanent",Reycruit,Organization,Reycruit,-,Hyderabad,Hyderabad,-,-,-,15-25 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata Developer,"  We at Datametica Solutions Private Limited are looking for Bigdata Engineers who have a passion for cloud with knowledge of different on-premise and cloud Data implementation in the field of Big Data and Analytics including and not limiting to Teradata, Netezza, Exadata, Oracle, Cloudera, Hortonworks and alike. Ideal candidates should have technical experience in migrations and the ability to help customers get value from Datametica's tools and accelerators ?Job Description? Experience : 2 to 5 Years? Location : Pune 1. Hands on Experience in Hadoop and Spark Programming  2. Data Warehousing projects with either Java, Python or Scala based Hadoop programming background. 3. Data Analysis, Code development experience, ideally in Big Data Spark, Hive, Hadoop, Java, Python, PySpark 4. Hands on Experience in Shell Scripting 5. Problem solving skills",1.70E+11,08-05-2024,06-08-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,Software Product,"Hive, Hadoop, Spark, SQL",-,9am-6pm,"Full Time, Permanent",Datametica,Organization,Datametica,https://www.naukri.com/hotjobs/images/v3/Dmetic_May21.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Senior Associate - Data Management Analyst - Big Data,"         Work with internal business groups on implementation opportunities, challenges, and convert gathered requirements into robust implementation by reducing the level of technical debt of the backend codebase         Stewarding technical standards and quality.          Use existing and emerging technology platforms to design, develop and document technically detailed applications. Analyze organizational needs and goals to develop and implement application systems.         Provide application software development services or technical support for backed features and components.          Scaling the backend architecture and codebase to support growth and improvise application efficiencies.         Provide application software development services or technical support in situations of moderate complexity.         provide ongoing maintenance, support and enhancements in existing systems and platforms. Provide recommendations for continuous improvement.         Advocate for innovative, creative technology solutions. Mentor junior level engineers                 To be successful in this role, we're seeking the following:              Self-motivated and can work quickly with minimal supervision in a fast-paced, deadline oriented agile environment.         5+ years of overall experience with a minimum of 3+ years of hands-on experience in developing ELT (Extract, Load and Transform) using big data stack. Everyone from top down is hands-on.         Ability to quickly convert requirements into robust and performance agnostic design and implement by developing optimized and resource efficient ELT pipelines with code readability and reviewing skills.         Help triage bugs, track software defects, and ensure their timely resolution - Stewarding technical standards and quality - Interface with product and other functional teams and their leadership         Expertise in Cloudera Data Platform 7.x, Apache Spark 3.x, Apache Airflow, Hive, Impala, SQOOP, Informatica, Python, Java         Experience in trouble shooting and performance optimizations in Airflow, Cloudera Hadoop Distribution components.         Good understanding on the importance of tests in a sophisticated CI/CD environment incorporating the latest DevOps technologies and best practices.         Excellent analytical, problem solving, troubleshooting and communication skills         Experience in Snowflake or AKS (Azure Kubernetes Service) for Spark is a plus.     ",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Banking,"Backend, Bloomberg, Analytical, Agile, data management analyst, Asset management, Informatica, Continuous improvement, Technical support, Python",-,9am-6pm,"Full Time, Permanent",BNY Mellon,Organization,BNY Mellon,https://img.naukimg.com/logo_images/groups/v1/293492.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Manager & Deputy Manager Pyspark Developer,"         1. Having experience 4-5 years                      2. Having experience 2-3 years - Strong expertise in Big Data ecosystem like Spark, Hive, Sqoop, HDFS, Map Reduce, Oozie, Yarn, HBase, Nifi.    - Design, develop, and maintain PySpark data processing pipelines to process large volumes of structured and unstructured data.    - Collaborate with data engineers and data scientists to understand data requirements and design efficient data models and transformations.    - Optimize and tune PySpark jobs for performance, scalability, and reliability.    - Implement data quality checks, error handling, and monitoring mechanisms to ensure data accuracy and pipeline robustness.    -Develop and maintain documentation for PySpark code, data pipelines, and data workflows.    -Developed production ready Spark applications using Spark RDD apis, Data frames, Datasets, Spark SQL and Spark Streaming.    -Strong experience of HIVE Bucketing and Partitioning.    -Well versed in writing complex hive queries using analytical functions.    -Knowledge in writing custom UDFs in Hive to support custom business requirements.      Job Description :      - We are looking for a skilled PySpark Developer having 4-5 or 2-3 years of experience to join our team.    - As a PySpark Developer, you will be responsible for developing and    - maintaining data processing pipelines using PySpark, Apache Sparks Python API.    - You will work closely with data engineers, data scientists, and other stakeholders to design and implement scalable and efficient data processing solutions.          ",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,Recruitment / Staffing,"Computer science, Analytical, UDF, Data processing, Data quality, YARN, Monitoring, SQL, Python, HBase",-,9am-6pm,"Full Time, Permanent",Central Depository Services,Organization,Central Depository Services,https://img.naukimg.com/logo_images/groups/v1/4686673.gif,Mumbai,Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Urgent Hiring For Pyspark Developer || Banking Client,"  IMMEDIATE OPENING FOR Pyspark Developer (BANKING CLIENT) Relevant Experience :  3+ yrs. Location :  Navi Mumbai Mode of Work  : Work From Office   Notice Period :  Upto 20 days. Job Description :   1) Experience required in Pyspark Developer 2) Should be excellent using Pyspark, Spark Python and Hive 3) Good Communication Skills Benefits of working at Acxiom :   - Statutory Benefits  - Paid Leaves  - Phenomenal Career Growth  - Exposure to Banking Domain  About Acxiom Technologies :  Acxiom Technologies is a leading software solutions services company that provides consulting services to global firms and has established itself as one of the most sought-after consulting organizations in the field of Data Management and Business Intelligence. Also here is our website address  https://www.acxtech.co.in/  to give you a detailed overview of our company.  Interested Candidates can share their resumes on  careers@acxtech.co.in   Kindly revert with the below details over an email:   Total Experience:  Relevant Experience in Pyspark and SQL:  Current CTC:  Expected CTC:  Notice Period (Official & if further negotiable):  Current location:  Preferred location:  Reason for job change:  Availability for video interview:    Thank you. ",2.20E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Pyspark, Hive, Pyspark Developer, Spark, Python, SQL",-,9am-6pm,"Full Time, Permanent",Acxiom Technologies,Organization,Acxiom Technologies,https://img.naukimg.com/logo_images/groups/v1/4673963.gif,Navi Mumbai,Navi Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Spark Big Data Developer,"   Design, develop, and maintain big data solutions to meet business requirements and support data-driven decision making      Work with stakeholders to understand their data needs and determine how to best use big data technologies to meet those needs      Design and implement scalable, high-performance big data architectures, using technologies such as Hadoop, Spark, and NoSQL databases      Extract, transform, and load large data sets into a big data platform for analysis and reporting      Write complex SQL queries and develop custom scripts to process big data      Collaborate with data scientists, data analysts, and other stakeholders to develop predictive models and algorithms that drive insights and decision making    ",31023500438,03-10-2023,01-01-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SQL queries, Usage, NoSQL, spark, Hadoop, big data, Quest",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Navi Mumbai,Navi Mumbai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Kafka Professionals,"BONbLOC Technologies Private Limited is looking for Kafka to join our dynamic team and embark on a rewarding career journey      you will be responsible for designing, implementing, and maintaining Kafka-based solutions to support our organization's data processing needs        Responsibilities:        Design, develop, and deploy Kafka-based solutions to meet business requirements      Configure and manage Kafka clusters for optimal performance and reliability      Develop Kafka Connectors for integrating Kafka with various data sources and sinks      Implement Kafka Streams for real-time data processing and analytics      Monitor Kafka infrastructure and troubleshoot issues to ensure system stability      Collaborate with cross-functional teams to design and implement end-to-end data pipelines      Implement security measures to protect data in transit and at rest within Kafka clusters      Optimize Kafka configurations for performance, scalability, and resource utilization      Conduct performance tuning and capacity planning for Kafka clusters      Document technical designs, deployment procedures, and best practices    ",1.40E+11,14-02-2024,14-05-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,Software Product,"rest, visualforce, project management, performance tuning, web services, configuration, customization, sfdc, triggers, javascript, apex, salesforce, sales force development, salesforce crm, technical design, data loader, java, capacity planning, kafka",-,9am-6pm,"Full Time, Permanent",Bonbloc Technologies,Organization,Bonbloc Technologies,-,"Hyderabad, Chennai","Hyderabad, Chennai",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"Looking for Bigdata Engineer with Strong technical skills in AWS, building Data Lakes, Data Warehousing, and hands-on with Python/Scala Experience in building Data warehousing Interested can share cv with Radhika@reycruit.com",30524008123,03-05-2024,01-08-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SCALA, Big Data, Data Warehousing, AWS, Python, Pyspark, Glue, EMR, Athena",-,9am-6pm,"Full Time, Permanent",Reycruit,Organization,Reycruit,-,Hyderabad,Hyderabad,-,-,-,20-35 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"HIVE, Big Data, Spark, Unix, SQL","   ?     Strong knowledge of HIVE, Spark, and able work independently.   Should have knowledge on cloud technologies like Azure   GitHub knowledge is an advantage,   Should be able to do code debugging.   Strong SQL knowledge will help in data reconciliation.   Should have experience in working in Agile environment   Well organized and able to manage multiple projects in a fast-paced demanding environment.   Attention to detail and quality; excellent problem solving and communication skills.   Ability and willingness to learn new tools and applications.   Effective written and verbal communication skills with an ability to convey complex technical concepts to business users and management Role & Band Senior Systems Engineer    ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Unix, hive, github, spark, Debugging, Reconciliation, Agile, Management, big data, SQL",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Kafka Admin,"               technical skills Apache/Confluent Kafka, Big Data technologies, Spark.     5+ years of Experience working as Kafka Admin.     Expertise and hands on experience working on Kafka connect using schema registry in a very high volume environment.     Expertise in Kafka brokers, zookeepers, KSQL, KStream and Kafka Control center.     Expertise and hands on experience working on JsonConverters, and StringConverters.     Working knowledge on Kafka Rest proxy     Expertise and hands on experience on custom connectors using the Kafka core concepts and API.       "", ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Administration, spark, Schema, Apache, big data",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Kafka Admin,"       technical skills Apache/Confluent Kafka, Big Data technologies, Spark.      5+ years of Experience working as Kafka Admin.      Expertise and hands on experience working on Kafka connect using schema registry in a very high volume environment.      Expertise in Kafka brokers, zookeepers, KSQL, KStream and Kafka Control center.      Expertise and hands on experience working on JsonConverters, and StringConverters.      Working knowledge on Kafka Rest proxy          Expertise and hands on experience on custom connectors using the Kafka core concepts and API            ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Administration, spark, Schema, Apache, big data",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Scala/ Spark,"             Overall experience of 7+ years in IT delivery      Must have 5+ years of practical experience in Apache Spark((Pyspark/Scala/Sqoop) Development.      work experience in building batch workloads in a distributed computing environment (on-premise or cloud).      Knowledge in data processing pipelines Design      work experience in Integration with AWS/Hadoop/HDFS, Real-Time Systems, Data Warehouses, and Analytics solutions.      Knowledge of Healthcare industry.      Experience in Agile methodologies      Anticipate, understand and respond to the needs of internal and external clients      Good Client Interfacing skills excellent communication      ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"spark, SCALA, Hadoop, Agile, Healthcare, Data processing, hdfs, Client interfacing, Analytics, it delivery",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Data Engineers,"               Overall experience of 7+ years in IT delivery     Must have 5+ years of practical experience in Apache Spark((Pyspark/Scala/Sqoop) Development.         Must have been created an SBT-based project for Apache Spark using the Scala language and deployed it to a standalone cluster         work experience in building batch workloads in a distributed computing environment (on-premise or cloud).     Knowledge in data processing pipelines Design     work experience in Integration with AWS/Hadoop/HDFS, Real-Time Systems, Data Warehouses, and Analytics solutions.     Knowledge of Healthcare industry.     Experience in Agile methodologies     Anticipate, understand and respond to the needs of internal and external clients     Good Client Interfacing skills excellent communication       Additional Responsibilities:     Knowledge of design principles and fundamentals of architecture   Understanding of performance engineering   Knowledge of quality processes and estimation techniques   Basic understanding of project domain   Ability to translate functional / nonfunctional requirements to systems requirements     ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"spark, Performance engineering, Billing, SCALA, Agile, Healthcare, Data processing, Client interfacing, Analytics, it delivery",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data subcon profiles Amex.,"             Must have at least 3 to 5 years of Spark programming experience     Must have working experience with Big data toolsets       Must have programming knowledge in Java/Scala       Good to have experience in Python     Working Knowledge of Agile methodologies is added advantage     Good communication skill     Must be flexible based on project need     3 major responsibilities are Analyze, design and build.     ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"spark, SCALA, Agile, Programming, big data, Python",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Hadoop,"     Interpret written business requirements, functional requirements and technical specification documents to design and develop technical solutions that meet business needs     Collaborate with IT and Business partners to design, develop, and troubleshoot end to end technical solutions     Perform system design and specification development, program logic and flow-charting that meets the stated project objectives     Perform coding to written technical specifications           Mandatory skills  Big Data, Hadoop    Domain  ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Service tax, Coding, Billing, Hadoop, Banking, System design, Vendor, microsoft, big data, Troubleshooting",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data + Azure,"       3-6 Years of experience in working on Hadoop Distribution, good understanding of core concepts and best practices     Participate in design and implementation of the Data Lake, big data migration using Azure cloud as platform     Working knowledge of ETL tool     Understanding of Big data, Data Lake Data Warehousing concepts     Must have experience with Java,Scala,Big data,Hive,Spark and Microsoft Azure Synapse Cloud.     Ability to build Azure data solutions and provide a technical perspective on storage, big data platform services Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.     Create and maintain optimal data pipeline architecture, including optimal extraction, transformation and loading of data, Apache Spark (SQL, Python, Scala, Java) and other       ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Data migration, spark, SCALA, Cloud, microsoft azure, big data, Data warehousing, ETL tool, SQL, Python",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data,"     Must have at least 3 to 5 years of Spark programming experience     Must have working experience with Big data toolsets       Must have programming knowledge in Java/Scala       Good to have experience in Python     Working Knowledge of Agile methodologies is added advantage     Good communication skill     Must be flexible based on project need     3 major responsibilities are Analyze, design and build.     ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"spark, SCALA, Agile, Programming, big data, Python",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data,"           Must have at least 5+ of Spark programming experience     Must have working experience with Big data toolsets     Good to have experience in Python     Working Knowledge of Agile methodologies is added advantage     Good communication skill     Must be flexible based on project need     3 major responsibilities are Analyze, design and build       ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"spark, Agile, Programming, big data, Python",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Apache Spark Professional,"               Overall experience of 7+ years in IT delivery      Must have 5+ years of practical experience in Apache Spark((Pyspark/Scala/Sqoop) Development.      work experience in building batch workloads in a distributed computing environment (on-premise or cloud).      Knowledge in data processing pipelines Design      work experience in Integration with AWS/Hadoop/HDFS, Real-Time Systems, Data Warehouses, and Analytics solutions.      Knowledge of Healthcare industry.      Experience in Agile methodologies      Anticipate, understand and respond to the needs of internal and external clients          Good Client Interfacing skills excellent communication       ",2.80E+11,28-02-2023,29-05-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"spark, Hadoop, Agile, Healthcare, Data processing, hdfs, Client interfacing, AWS, Analytics, it delivery",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Hadoop Admin,"     IDESLABS PRIVATE LIMITED is looking for Hadoop Admin to join our dynamic team and embark on a rewarding career journey       Good understanding of SDLC and agile methodologiesInstallation and configuration of Hadoop clusters, including HDFS, MapReduce, Hive, Pig, HBase, and other related tools     Managing and monitoring Hadoop clusters to ensure high availability and performance     Planning and implementing data backup and disaster recovery strategies for Hadoop clusters     Proactively monitoring and tuning Hadoop cluster performance to optimize resource utilization and prevent bottlenecks     Providing technical support to developers and end-users as needed     Awareness of latest technologies and trends     Logical thinking and problem solving skills along with an ability to collaborate       ",2.70E+11,27-03-2024,25-06-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"hive, cloudera, hadoop cluster, apache pig, kerberos, emr, apache zookeeper, apache, spark, ambari, flume, linux, shell scripting, hadoop, big data, hbase, oozie, cdh, impala, nosql, high availability, mapreduce, kafka, hadoop administration, agile, sqoop, yarn, aws, sdlc",-,9am-6pm,"Full Time, Permanent",IDESLABS,Organization,IDESLABS,https://img.naukimg.com/logo_images/groups/v1/4601085.gif,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Sr.Big Data Engineer,"         Must have -             Atleast 4 to 7 years of relevant experience as Big Data Engineer     Hands-on experience into Scala or Python     Hands on experience on major components in Hadoop Ecosystem likeHDFS, Map Reduce, Hive, Impala.     Strong programming experience of building applications / platformsusing Scala or Python.     Experienced in implementing Spark RDD Transformations, actions toimplement business analysis.         Good to have -         An efficient interpersonal communicator with sound analytical problemsolving skills and management capabilities.     Strive to keep the slope of the learning curve high and able to quickly adapt to new environments and technologies.     Good knowledge on agile methodology of Software development.         ",1.91E+11,19-10-2022,17-01-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,Management Consulting,"PDF, Business analysis, spark, Analytical, SCALA, Programming, Agile methodology, Management, big data, Python",-,9am-6pm,"Full Time, Permanent",Daya Consultancy Services,Organization,Daya Consultancy Services,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
SRE Manager - Big Data,"   Role & Responsibilities       We are seeking a hands-on Manager who has experience leading large Big Data environments spread across thousands of nodes and petabytes of data. We look forward to a dynamic manager with a background & experience that looks like this:       Grown into leadership roles after proving technical skills in individual contributor roles but still enjoys hands on work when the situation calls for it.         You have designed and built large data environments for availability, security and reliability.         You keep yourself informed about the choices and trade off as the new technology evolves in big data landscape.         You have an eye for talent and hire and grow your engineers by mentoring and challenging them.         You will collaborate across many teams to deliver on projects related to big data platform and data pipeline and provide SRE support for reliability of these managed services.         You will have significant opportunity to influence and shape our big data platform strategy and data products as we work on the next generation of our architecture, platform and processes.         Education Requirements         Minimum Bachelor s degree in Computer Science, Engineering, Business Information Systems, or related field.       Required Skills       5+ Years of Management experience leading team of engineers         Hands on manager who likes troubleshooting complex performance and scale problems         Excellent problem solving, critical thinking, and communication skills Lead by example to motivate and challenge the team to deliver their best.         8+ years of experience in Hadoop based technologies Hive, Pig, Map Reduce, Spark-SQL, Spark-Streaming, Python & PySpark, Kafka, Flume and Scala         Very good experience in GCP Big Query, Bigtable         Good experience in APM tools like Dynatrace and log aggregation tools like Splunk         Good experience in workflow management tools like Apache Airflow         Familiarity with incident life cycle management tools like ServiceNow, Jira, Confluence, etc         Zoom in and zoom out to clear out ambiguity and set a clear path forward         Experience managing Hadoop clusters with hundreds of nodes and petabytes of data running thousands of jobs         Have a passion for automation by creating tools to eliminate the toil         Strong expertise in troubleshooting complex production issues.         The candidate should be adapt at prioritizing multiple issues in a high pressure environment         Should be able to understand complex architectures and be comfortable working with multiple teams         Ability to conduct performance analysis and troubleshoot large scale distributed systems         Should be highly proactive with a keen focus on improving uptime and availability of mission-critical services         Comfortable working in a fast paced environment while continuously evaluating emerging technologies         Excellent communication, client engagement and client management skills are strongly preferred.   ",1.71E+11,17-12-2020,17-03-2021,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Recruitment / Staffing,"Computer science, Automation, Managed services, GCP, Troubleshooting, Apache, big data, Client management, SQL, Python",-,9am-6pm,"Full Time, Permanent",AugmatrixGo,Organization,AugmatrixGo,-,"Hyderabad, Gurugram","Hyderabad, Gurugram",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Hadoop Administration,"   ?     ?     1. Number of openings 3      ?     2. Job Title* Hadoop Admin      ?     3. Client Name* (Not disclose to supplier at the time of releasing the request) GST      4. Client rate (For internal tracking) NA      ?     5. Vendor Rate* / Budget rate to vendor (please do not mention client billing rate). Please refer market condition and revert accordingly.      ?     6. Contract duration (in months)* 24      ?     7. BGV Check (post or pre-onboarding) Post     ?8. Detailed JD (Not less than 10 lines) including skills, tools and versions if any. Experience in working on Big Data components like HDFS, YARN, MapReduce, Hive, Hue, Kafka, Streamsets , Sqoop, Flume, Zookeeper, Spark, etc. in production clusters.       Configure and manage Cloudera Platform & Services like CM and CDH, Production Ready Features Like HA for HDFS for YARN for HIVEWorkflow, Configure and Administrating cluster to cloud BDR in Production Environment with Alerts (Backup and Disaster Recovery).   Experience in deploying, configuring, maintaining, monitoring, troubleshooting & performance tuning of Spark on Cloudera.   Big Data Technologies: Apache Hadoop & Distributions, Map-Reduce, HDFS.       Enterprise Hadoop: Apache Hadoop, Cloudera CDH.   Eco System: HDFS, Flume, Hive, Sqoop, Zookeeper, Kafka, Yarn, hue, Spark.   3-4 years relevant experience.      9. Any specific mandatory points related candidate skills (tools, versions, certifications etc), experience, any specific industry etc., 10. Candidates required from any specific industry      11. Other suggestions if any related to Candidate profiles     ?12. Work Location Gurgaon/Bangalore      13. Work from home/ Office/ Client location (Pl provide Client location) Office    "", ",40323500843,04-03-2023,02-06-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Performance tuning, Client billing, Disaster recovery, flume, sqoop, Apache, Troubleshooting, big data, YARN, Monitoring",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Hadoop Admin,"       Experience in working on Big Data components like HDFS, YARN, MapReduce, Hive, Hue, Kafka, Streamsets , Sqoop, Flume, Zookeeper, Spark, etc. in production clusters.               Configure and manage Cloudera Platform & Services like CM and CDH, Production Ready Features Like HA for HDFS for YARN for HIVEWorkflow, Configure and Administrating cluster to cloud BDR in Production Environment with Alerts (Backup and Disaster Recovery).   Experience in deploying, configuring, maintaining, monitoring, troubleshooting & performance tuning of Spark on Cloudera.   Big Data Technologies: Apache Hadoop & Distributions, Map-Reduce, HDFS.               Enterprise Hadoop: Apache Hadoop, Cloudera CDH.   Eco System: HDFS, Flume, Hive, Sqoop, Zookeeper, Kafka, Yarn, hue, Spark.       "", ",40323500842,04-03-2023,02-06-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Performance tuning, Disaster recovery, flume, Hadoop, sqoop, Apache, Troubleshooting, big data, YARN, Monitoring",-,9am-6pm,"Full Time, Permanent",Fusion Plus Solutions Inc,Organization,Fusion Plus Solutions Inc,-,Hyderabad,Hyderabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Senior Software Engineer,"Role & responsibilities     This role is for Software      Engineer responsible for Design and Developing software applications. The candidate is expected      to work closely with Team Leads or Software Development Managers and other      key stake holders to ensure good quality, maintainable, scalable and high      performing software applications are delivered to users. He/she should be coming      from a strong technological background. Should be hands on and be      able to work independently requiring minimal technical/tool guidance. Should have good      communication skill and strong positive outlook.  Experience working in Agile      and Scrum delivery. Should be able to      contribute towards good software design. Participate in daily      stand-up meetings. Analyze software defects      and fix them in timely manner. Work closely with      Functional Analysis and Quality Assurance teams Preferred candidate profile     Extensive experience with      Hadoop & Java/Scala related technologies such as Understanding of Hadoop      ecosystem and HDFS file System. Understanding of different      types of nodes in a Hadoop cluster. Experience with MapReduce      framework. Experience with Hive, Pig,      Impala, Spark SQL etc Experience with Apache      Spark and Scala (preferably greater than version 2.0) Experience Spark job      optimization Experience with File      handling such as Avro/Parquet etc Good understanding of      BEELINE command to connect to HIVE server Tuning and Troubleshooting,      experience with profiling and monitoring tools Experience on Scala or Java      preferably Spring Batch/Core/Boot Experience with Messaging      and integration, Patterns, REST, SOA Experience with build and      deployment tools like Maven/Artifactory/Teamcity or      Jenkins Unix scripting and hands on experience Perks and benefits     Training and development to help you excel in your career Coaching and support from experts in your team A culture of continuous learning to aid progression A range of flexible benefits that you can tailor to suit your needs",1.90E+11,19-04-2024,18-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,Investment Banking / Venture Capital / Private Equity,"Hadoop, Bigdata, Data Engineer, Parquet, Java, Hive, Oracle RDBMS, Oracle SQL, Scala, Impala, Avro, Apache Spark",-,9am-6pm,"Full Time, Permanent",Deutsche Bank,Organization,Deutsche Bank,https://img.naukimg.com/logo_images/groups/v1/468918.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"Framework:  Apache Spark Languages: Java / Scala DB Layer: MySQL Testing: ScalaTest Build & Deployment:  CI/ CD toolset, Code Build/Deploy Cloud Env: AWS Cloud and Services",90524008804,09-05-2024,07-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SCALA, Big Data, Spark, SQL, Java, AWS",-,9am-6pm,"Full Time, Permanent",Talentiqo Workforce And Rpo,Organization,Talentiqo Workforce And Rpo,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"Required Skills: ?Solid experience in Apache Spark and SQL . Good Knowledge of Azure cloud, Airflow, and Databricks. Basic Knowledge of Python and shell scripting is required Good to have Exposure to setting up CI/CD pipelines. Understanding of Agile methodology, using Jira",30524005192,03-05-2024,01-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Azure, Ci/Cd, Apache spark, SQL, Python",-,9am-6pm,"Full Time, Permanent",ERM Placement Services (p) Ltd.,Organization,ERM Placement Services (p) Ltd.,-,Noida,Noida,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Hadoop Developer,"Strong architectural experience with Cloudera Hadoop distributions on Appliance based and on-premise clusters.   Expertise in providing technical solutions for data lakes design and data ingestion in Hadoop. Required Candidate profile Proficiency in Cloudera Manager architecture, cloudera cluster environment and cloudera manager.- Expertise in understanding Java , REST API concepts and troubleshooting java based services",2.50E+11,25-04-2024,24-07-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Cloudera Hadoop, Cloudera, Hadoop, Cassandra, Big Data, Data Modeling, HBase",-,9am-6pm,"Full Time, Permanent",Manek Consulting,Organization,Manek Consulting,-,"Mumbai Suburban, Navi Mumbai, Mumbai (All Areas)","Mumbai Suburban, Navi Mumbai, Mumbai (All Areas)",-,-,-,25-30 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"Looking for Bigdata Engineer with Strong technical skills in AWS, building Data Lakes, Data Warehousing, and hands-on with Python/Scala Experience in building Data warehousing Interested can share cv with sudhachanduri@reycruit.com",2.70E+11,27-04-2024,26-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SCALA, Big Data, Data Warehousing, AWS, Python",-,9am-6pm,"Full Time, Permanent",Reycruit,Organization,Reycruit,-,Hyderabad,Hyderabad,-,-,-,17-32 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"Big Data Senior Developer: (Experience Range: 8+yrs e Looking for Sr Data Engineer with Strong technical skills in AWS, building Data Lake  Hands on with Python/Scala. & Experience in building Data warehousing",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"python, Data Warehousing",-,9am-6pm,"Full Time, Permanent",Reycruit,Organization,Reycruit,-,Hyderabad,Hyderabad,-,-,-,30-45 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"Role & responsibilities     Mandatory Skills Description: - BigData experience (4 years+); - Scala, Spark, Hive - SQL Databases; UNIX Shell; - BigData (understand MapReduce), HDFS, Yarn; Nice-to-Have Skills: - Java - Apache Hadoop, Spark, Hive, Impala, Yarn, Talend, Hue - Spark Calculators based on business logic/rules - Basic performance tuning and troubleshooting knowledge - Experience with all aspects of the SDLC - Experience with complex deployment infrastructures - Knowledge in software architecture, design and testing - Data flow automation (Apache NiFi, Airflow etc) - Understanding of difference between OOP and Functional design approach - Understanding of an event driven architecture - Spring, Maven, GIT, uDeploy; Preferred candidate profile   Perks and benefits  ",2.60E+11,26-04-2024,25-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"hive, spark, Hadoop, SCALA, Big Data, Mapreduce, SQL",-,9am-6pm,"Full Time, Permanent",Luxoft,Organization,Luxoft,https://img.naukimg.com/logo_images/groups/v1/1744734.gif,Pune,Pune,-,-,-,15-20 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Scala Developer,Hiring Scala Developers!!! Looking for candidates who have Min 5 years of experience in Scala. Candidates who are willing to take the onsite opportunity to UK can apply. Interested candidates share resume to sahana.n@primesourcellp.com,2.00E+11,20-04-2024,19-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Software Product,"SCALA, Scala Programming, Scala Technologies",-,9am-6pm,"Full Time, Permanent",Primesource Consulting Llp,Organization,Primesource Consulting Llp,-,United Kingdom (UK),United Kingdom (UK),-,-,-,20-35 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
AWS Big data developer,"   Mandatory      :         Minimum of 4 to 6 years of relevant work experience as AWS Developer         Good knowledge of      Hadoop Architecture     and its ecosystem and possess experience in data    storage HDFS, writing queries HQL or Spark SQL, data processing and analysis using Pyspark     Strong hands on Experience in    AWS Big data tools     EMR, Glue, Athena, MSK/Kinesis, IAM, EC2, S3       Strong hands on Experience in    AWS Databases     -RDS, Dynamo DB, Redshift/Spectrum,        ?     Strong hands on    Python     script programming using Jupiter notebook       Experience of version control tools like Git, TFS/Bit Bucket     Knowledge on AWS Aurora, Neptune, SNS, SQS, Redis, Cloud formation, Lambda, VPC, Glacier, EBS, EFS, Cloudwatch     Knowledge of CICD, Docker, Terraform, RabbitMQ/Apache Kafka     Working Knowledge of Presto DB, Apache Spark, Apache Hive, Apache Hudi and Delta tables     Certified in AWS Solution Architect .             Good To Have :         Nice to have knowledge of AWS Data lake formation         Hive, Presto, Flink connectors.         Knowledge of Medical domain ( Dicom,HL7, FHIR)         ",2.81E+11,28-08-2023,26-11-2023,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"hive, Dicom, Version control, GIT, Data processing, big data, AWS, Solution Architect, SQL, Python",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big data Spark Scala Developers,"Diverse Lynx  is looking for Big data Spark Scala Developers  to join our dynamic team and embark on a rewarding career journey    We are seeking a talented and experienced Big Data Spark Scala Developer to join our dynamic team     The successful candidate will be responsible for designing, developing, and implementing complex big data solutions using Apache Spark and Scala     As a key member of our technical team, you will contribute to the development and maintenance of our big data infrastructure, ensuring scalability, performance, and reliability       Responsibilities:       Big Data Development:Design, develop, and implement scalable and high-performance big data solutions using Apache Spark and Scala     Collaborate with cross-functional teams to gather and analyze requirements for data processing     Data Processing and Analysis:Develop and optimize Spark jobs for large-scale data processing, transformation, and analysis     Implement data pipelines and ETL processes to support business intelligence and analytics requirements     Performance Optimization:Fine-tune and optimize Spark applications for maximum performance and efficiency     Troubleshoot and resolve performance issues, bottlenecks, and data processing errors     Data Integration:Integrate big data solutions with existing systems and data warehouses     Ensure seamless data flow and compatibility between different data sources     Code Quality and Testing:Write clean, modular, and maintainable code following best practices     Conduct unit testing and participate in code reviews to ensure code quality and reliability     Documentation:Document design decisions, code, and data architecture for future reference     Create and maintain technical documentation for developed solutions   ",2.71E+11,27-12-2023,26-03-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"spark, SCALA, Developer, Data processing, Unit testing, Business intelligence, big data, Analytics, Technical documentation, Data architecture",-,9am-6pm,"Full Time, Permanent",Diverse Lynx,Organization,Diverse Lynx,https://img.naukimg.com/logo_images/groups/v1/4554388.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata Developer,"   Data Processing: Developing and maintaining data processing applications that can handle large volumes of data efficiently     This often involves real-time and batch processing     Data Integration: Integrating data from various sources, including databases, data lakes, and external data streams     Data Transformation: Cleaning, transforming, and enriching raw data to make it suitable for analysis and reporting     Database Management: Working with NoSQL databases like Hadoop, Cassandra, or MongoDB, and traditional relational databases like SQL Server or Oracle     Big Data Frameworks: Utilizing big data frameworks and technologies such as Apache Hadoop, Apache Spark, Apache Kafka, and others to process and analyze data     Programming Languages: Proficiency in programming languages commonly used in big data development, such as Java, Python, Scala, or R     Data Storage: Setting up and maintaining data storage solutions, including distributed file systems and cloud-based storage services     Data Security: Implementing security measures to protect sensitive data, including encryption and access control   ",1.81E+11,18-10-2023,16-01-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"NoSQL, data security, cassandra, SCALA, Data processing, MongoDB, Oracle, big data, SQL, Python",-,9am-6pm,"Full Time, Permanent",Diverse Lynx,Organization,Diverse Lynx,https://img.naukimg.com/logo_images/groups/v1/4554388.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Developer,"We are seeking an experienced Senior Big Data Developer to join our dynamic team. The ideal candidate will have a strong background in big data technologies, with a minimum of 5 years of experience. You will be responsible for designing, developing, and maintaining large-scale data processing systems, leveraging technologies such as Hadoop, Hive, Google Cloud Platform (GCP), BigQuery, and DataProc.    Key Responsibilities: Design, develop, and implement scalable and reliable big data solutions using Hadoop ecosystem technologies. Develop and optimize data pipelines for ingestion, transformation, and analysis of large datasets. Collaborate with cross-functional teams to understand data requirements and translate them into technical solutions. Optimize performance and efficiency of big data systems and processes. Troubleshoot and resolve issues related to data processing and performance. Stay updated with emerging technologies and industry trends in big data and cloud computing. Requirements: Bachelor's degree in Computer Science, Engineering, or related field. Minimum of 5 years of experience in big data development. Strong proficiency in Hadoop ecosystem technologies such as HDFS, MapReduce, Hive, and HBase. Hands-on experience with Google Cloud Platform (GCP) services, including BigQuery and DataProc. Proficiency in programming languages such as Java, Scala, or Python. Experience with data modeling, ETL processes, and data warehousing concepts. Strong analytical and problem-solving skills. Excellent communication and collaboration skills. Ability to work independently and as part of a team in a fast-paced environment. Experience with containerization technologies like Docker and orchestration tools like Kubernetes (desired).",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Hive, Hadoop, Big Data, GCP Big Query, Dataproc",-,9am-6pm,"Full Time, Permanent",IntraEdge Technology,Organization,IntraEdge Technology,https://img.naukimg.com/logo_images/groups/v1/4706559.gif,Gurugram,Gurugram,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Hadoop Developer,"   5 years of Bachelor\s or Master\s degree with hands on experience in building data engineering pipelines in a distributed environment         Good understanding of Data warehousing principles       Expertise in working with Big Data Technologies like    Hive, Spark(Scala/Java/Python), Sqoop, Oozie/Airflow         Good exposure to writing complex SQL ,shell scripting       Exposure to streaming data pipelines using Spark,    Kafka         Exposure to Performance optimization of batch pipelines    written in Hive/Spark         Experience on CI/CD (Continuous Integration/Delivery) i.e. Jenkins, GIT/BitBucket       Exposure to building    microservice    API using spring is highly preferred       Familiarity with working in an agile software development framework             ?     ",60123501048,06-01-2023,06-04-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Management Consulting,"hive, continuous integration, GIT, spark, Shell scripting, Agile, Performance optimization, big data, SQL, Python",-,9am-6pm,"Full Time, Permanent",Purview India Consulting and  Services  LLP,Organization,Purview India Consulting and  Services  LLP,-,remote,remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Hadoop Developer,"   A Hadoop Developer is responsible for designing, developing, and maintaining big data solutions using Apache Hadoop.          Key responsibilities include:          Designing and developing scalable, efficient, and reliable data processing pipelines using Hadoop and related technologies such as MapReduce and Hive.      Writing and executing MapReduce jobs to process large datasets stored in Hadoop Distributed File System (HDFS).      Collaborating with stakeholders to understand their data processing requirements and develop solutions that meet their needs.      Integrating Hadoop with other data storage and processing technologies, such as NoSQL databases and data warehouses.      Developing and maintaining data processing workflows using Apache Oozie or similar workflow management tools.      Debugging and optimizing Hadoop applications to improve performance and scalability.      Ensuring data quality and security through proper data validation, cleansing, and encryption.      Writing and maintaining technical documentation for Hadoop solutions.      Staying up-to-date with the latest developments in big data processing and related technologies.      Performing other tasks as assigned by management.Qualifications:      Strong programming skills in Java and/or Python.      Experience with MapReduce programming, Hive and Pig scripting, and Hadoop Distributed File System (HDFS).      Knowledge of NoSQL databases, such as HBase, Cassandra, and MongoDB.      Experience with data warehousing, ETL, and data integration technologies.      Strong analytical and problem-solving skills, with the ability to debug and optimize complex data processing pipelines.      Good communication and collaboration skills, with the ability to work effectively with cross-functional teams.      Familiarity with Agile software development methodologies.      Experience with cloud computing platforms, such as Amazon Web Services (AWS) and Microsoft Azure, is an advantage.    ",31023500473,03-10-2023,01-01-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Cloud computing, NoSQL, Analytical, Agile, Data processing, Data quality, MongoDB, Python, Technical documentation, HBase",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Ahmedabad,Ahmedabad,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Data Engineer / Sr. Data Engineer - Azure,"Data Engineer / Sr. Data Engineer - Azure As a Big Data Engineer - Azure, you will build and learn about a variety of analytics solutions & platform, data lake, modern data platforms, data fabric solutions etc. using different Open Source, Big Data and Cloud technologies on Microsoft Azure. More specifically, you will: ??Design and build scalable & metadata driven data ingestion pipelines (For Batch and Streaming Datasets) ??Execute high-performance data processing for structured and unstructured data, and data harmonization ??Schedule, orchestrate, and validate pipelines ??Design exception handling and log monitoring for debugging ??Make tech stack and tools related decisions ??Collaborate with business consultants, data scientists, and application developers to develop analytics solutions Required Experience, Skills & Competencies: ??4 to 8 years of total IT experience with 2+ years on big data engineering and Microsoft Azure. ??Strong Hands-on experience in implementing Data Lake with technologies like: Mandatory: Azure Data Factory (ADF), PySpark, Databricks, ADLS, Azure SQL Database Optional: Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and Purview. ??Strong programming, unit testing & debugging skills in SQL, Python or Scala/Java. ??Some experience of using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic Search. ??Good Understanding of different file formats like Delta Lake, Avro, Parquet, JSON and CSV. ??Experience of working in Agile projects and following DevOps processes with technologies like Git, Jenkins & Azure DevOps. ??Good to have: Experience of working on Data Lake & Lakehouse projects Experience of building REST services and implementing service-oriented architectures. Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner. Certifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE) Designation will be commensurate with expertise/experience",2.30E+11,23-04-2024,22-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Analytics / KPO / Research,"Azure Data Factory, Pyspark, Azure Databricks, Big Data Analytics, Big Data, Azure Data Lake, API, SQL Azure, SQL",-,9am-6pm,"Full Time, Permanent",Tiger Analytics,Organization,Tiger Analytics,https://www.naukri.com/hotjobs/images/v3/tigermar16.gif,"Pune, Chennai, Bengaluru","Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata Testing,"  JOB DESCRIPTION   Snapshot Synchronoss Technologies (Nasdaq: SNCR) builds software that empowers companies around the world to connect with their subscribers in trusted and meaningful ways. The companys collection of products helps streamline networks, simplify onboarding, and engage subscribers to unleash new revenue streams, reduce costs and increase speed to market. Hundreds of millions of subscribers trust Synchronoss products to stay in sync with the people, services, and content they love. That?? why more than 1,500 talented Synchronoss employees worldwide strive each day to reimagine a world in sync.   How you will help:   Reviews and refines software      requirements. Drafts and executes test plans      and test cases. Identifies and drafts steps to      reproduce defects utilizing our defect tracking system, JIRA. Enters and tracks defects to      closure. Utilizes testing tools to      increase the effectiveness of testing. Operate within the SAFe Agile      methodology. Who we have in mind: Software Test Engineer with 3-5 years of experience in      Bigdata Testing. Must have experience in any big data tool such as      Spark, Kafka, Document DB Good experience in writing SQL queries. Hands on experience working in Unix-Shell scripting. Should have worked in AWS cloud. Hands on experience in Docker and K8s   It would be great if you had:   Automation Test Experience What we offer: Synchronoss is proud to be an equal opportunity employer. As a global company, we value and celebrate diversity and are committed to a workplace free from discrimination and harassment. We take pride in fostering an inclusive environment based on mutual respect and merit. We are at our best when our workforce is dynamic in thought, experience, skill set, race, age, gender, sexual orientation, sexual expression, national origin and beyond.",1.10E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Unix, Airflow, Big Data Testing, AWS, SQL, Docker, Python, Kubernetes",-,9am-6pm,"Full Time, Permanent",Synchronoss,Organization,Synchronoss,https://img.naukimg.com/logo_images/groups/v1/370270.gif,Bangalore Rural,Bangalore Rural,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Data Engineer,"   ?   Skills Needed-      At least 4+ years of data engineering experience in handling large data volume.         Solid experience in Apache Spark, Python, SQL         Experience in data engineering solutions on the cloud. Knowledge of Azure cloud, Airflow, and databricks is preferable.         Good to have         Exposure to setting up CI/CD pipelines         Understanding of Agile methodology, using Jira.     ",30622500562,03-06-2022,01-09-2022,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,Engineering & Construction,"Procurement, Telecom, Hospitality, Supply chain management, Consulting, Healthcare, Market research, CRM, SQL, Logistics",-,9am-6pm,"Full Time, Permanent",Absolutdata,Organization,Absolutdata,https://img.naukimg.com/logo_images/groups/v1/4664821.gif,"Noida, Bengaluru","Noida, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Snowflake Developer,"Immediate Opening!!! Role: Spark Developer Exp: 6  years Location: Chennai, Pune Notice Period: Immediate-30 days Mode: contract",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,Snowflake,-,9am-6pm,"Full Time, Temporary/Contractual",SRS Infoway,Organization,SRS Infoway,https://img.naukimg.com/logo_images/groups/v1/1142018.gif,"Pune, Chennai","Pune, Chennai",-,-,-,7-14 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Spark Developer,"Immediate Opening!!! Role: Spark Developer Exp: 6  years Location: Chennai, Pune Notice Period: Immediate-30 days Mode: contract",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,Spark,-,9am-6pm,"Full Time, Temporary/Contractual",SRS Infoway,Organization,SRS Infoway,https://img.naukimg.com/logo_images/groups/v1/1142018.gif,"Pune, Chennai","Pune, Chennai",-,-,-,7-14 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data  -Spark Scala / F2F Interview,"  Warm Greetings from SP Staffing Services Private Limited!! We have an urgent opening with one of our Client for the below position, Big Data Engineer, if you are interested Please send your update profile if you are interested.  Relevant Experience: 5- 8 Yrs Job Location : chennai,Bangalore and Pune Required skill set :    Big Data on Cloud, Hadoop and frameworks such as Spark and Cloudera Good to have (Not Mandatory) : Devops, Python, Java, Scala Detailed Job Description: Overall 7+ Years in database development and 2 - 3 Years experience in space of Big Data on Cloud as well on premise environments ( preferably SQL ). Experience on Data Analytics services on AWS e.g. AWS EMR, Athena, Kinesis, Redshift Proficient at implementing data processing workflows using Hadoop and frameworks such as Spark and Cloudera Hands-on experience on broadcasting tools like Kafka, Event Hub, AWS Kinesis etc. GOOD TO HAVE Understanding of Devops. Experience in building scalable end-to-end data ingestion and processing solutions Experience with object-oriented and/or functional programming languages, such as Python, Java and Scala.   If interested , Please forward your updated cv with following Details to  preetha.m@gmail.com or 7358295265 Over All Exp : Relevant Exp :  Current CTC : Expected CTC : Notice Period :",1.01E+11,10-05-2024,08-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data on Cloud, Airflow, Kinesis, Cloudera, Event Hub, SCALA, Hadoop, Kafka, Spark, Spark Streaming, Framework, Akka",-,9am-6pm,"Full Time, Permanent",SP Staffing,Organization,SP Staffing,-,"Pune, Chennai, Bengaluru","Pune, Chennai, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data (K8+AWS),"             Min 5-8 years of experience in Hadoop/big data technologies. Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)                         Hands-on experience with Python/Pyspark/Scala.                          3+ years of experience in spark.                         1+ experience in Snowflake, and AWS cloud developing data solutions. Certifications preferred.                         Experience with Open shift containerization and related technologies (e.g. Docker, Kubernetes)                         Experience with all aspects of DevOps (source control, continuous integration, deployments, etc.)                         Knowledge of agile(scrum) development methodology is a plus                         Strong development/automation skills             ",2.81E+11,28-11-2023,26-02-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"continuous integration, Automation, Agile scrum, spark, devops, Cloud, SCALA, big data, AWS, Python",-,9am-6pm,"Full Time, Permanent",Newt Global,Organization,Newt Global,https://img.naukri.com/logo_images/v3/1405722.gif,Chennai,Chennai,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Staff Engineer - Big Data,"       Be part of 10-12 Platform Engineers that are the crux for developing and maintaining Big Data (Data Lake, Data Warehouse and Data Integration) and advanced analytics platforms at Seagate   Apply hands-on subject matter expertise in the Architecture of and administration of Big Data platforms - Data Warehouse Appliances , Open Data Lakes (AWS EMR, HortonWorks), Data Lake Technologies (AWS S3/Databricks/Other) and experience with ML and Data Science platforms (Spark ML , H2O , KNIME)   Develop and manage SPARK ETL Frameworks, Data orchestration with Airflow and support building PRESTO/Trino queries for the key stakeholders   Design, scale and deploy Machine Learning pipelines   Collaborate with Application Architects and Business SMEs to design and develop end-to-end data pipelines and supporting infrastructure   Establish and maintain productive relationships with peer organizations, partners, and software vendors   Work with customers and business stakeholders to understand the needs and deliver on their highest priority features and requirements incrementally   Work with PMO and effectively communicate with management team on the platform charter and scope   Manage budget responsibilities, development of staff, resource planning and goal setting for the team   Provide guidance, career development, and mentoring to technical team members, and help achieve maximum potential         About you:                                                   You re a passionate professional who is up to the challenge of blending the fast-changing technology landscape of Big Data analytics with the complex and high-impact space of HiTech and Manufacturing analytics   As a motivated self-starter, you have the experience working in a dynamic environment   Exceptional data engineering experience in building large, high-scale Data platforms and applications using cloud and big data technologies like Hadoop ecosystem and Spark   Strong appetite for constant learning, thinking out of the box, questioning the problems & solutions with the intent to understand and solve better   As well, you re uncompromisingly detail oriented, well organized with solid time management skills, and you have solid, effective verbal and written communications abilities as well   Excellent interpersonal skills to develop relationships with different teams and peers in the organization         Your experience includes:                                   Excellent technical skills with a proven and successful history working with data at scale and empowering organizations through data   Big data processing frameworks: Spark, Hadoop, Hive, Kafka, EMR   Architecting and developing big data solutions on cloud (AWS or Other)   Advanced experience and hands-on architecture and administration experience on big data platforms   Data Warehouse Appliances, Hadoop (AWS EMR), Data Lake Technologies (AWS S3/GCS/Other) and experience with ML and Data Science platforms (Spark ML , H2O , KNIME )   Python, Java, Scala   DevOps, Continuous Delivery, and Agile development   Creating a culture of technical excellence by leading code and design reviews, promoting mentorship, and identifying and promoting educational opportunities for engineers   Strong understanding of Micro-services and container-based development using Docker and Kubernetes ecosystem is a plus   Experience working in a Software Product Development environment is a plus                           ",2.30E+11,23-01-2024,22-04-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,Electronic Components / Semiconductors,"Career development, RSS, orchestration, Appliances, Machine learning, SCALA, Data processing, big data, Data warehousing, Python",-,9am-6pm,"Full Time, Permanent",Seagate,Organization,Seagate,https://img.naukimg.com/logo_images/groups/v1/1293088.gif,Pune,Pune,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Sr Big Data Infrastructure Engineer (GCP)," We are seeking a highly skilled and experienced Senior Big Data Infra Engineer to join our dynamic team. The ideal candidate will have a strong background in developing and scaling both stream and batch processing systems, and a solid understanding of public cloud technologies, especially GCP. This role involves working in a remote environment, requiring excellent communication skills and the ability to solve complex problems independently and creatively.       ?       What you will be doing:         Implementing automation/DevOps best practices for CI/CD, IaC, Containerization, etc to Build a reusable infra structure for stream and batch processing systems at scale.      Create automation, whether that is building DevOps pipelines, scripting or creating Infrastructure as Code in Terraform     Participating in work sessions with clients      Completing technical documentation            Requirements:         Experience in Developing and Scaling data Processing Systems This includes working with technologies like Pub/Sub, Kafka, Kinesis, DataFlow, Flink, Hadoop, Pig, Hive, and Spark.     Expertise in public cloud services, particularly in GCP.      Experience with GCP managed services and understanding of cloud-based messaging/stream processing systems are critical.     Experienced in Infrastructure and Applied DevOps principles in daily work. Utilize tools for continuous integration and continuous deployment (CI/CD), and Infrastructure as Code (IaC) like Terraform to automate and improve development and release processes.      Has knowledge in containerization technologies such as Docker and Kubernetes to enhance the scalability and efficiency of applications.     Worked effectively in a remote setting, maintaining strong written and verbal communication skills. Collaborate with team members and stakeholders, ensuring clear understanding of technical requirements and project goals.     Proven experience in engineering stream/batch processing systems at scale.     Strong programming abilities in Java and Python.     Hands-on experience in public cloud platforms, particularly GCP. Additional experience with other cloud technologies is advantageous.         Must Have:         Google Associate Cloud Engineer Certification or other Google Cloud Professional level certification     4+ years of experience in customer-facing software/technology or consulting     4+ years of experience with on-premises to cloud migrations or IT transformations     4+ years of experience building, and operating solutions built on GCP (ideally) or AWS/Azure     Technical degree: Computer Science, software engineering or related   ",1.60E+11,16-04-2024,15-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Automation, Managed services, GCP, Consulting, Cloud, Data processing, rackspace, Python, Technical documentation",-,9am-6pm,"Full Time, Permanent",Rackspace Technology,Organization,Rackspace Technology,https://img.naukimg.com/logo_images/groups/v1/3080116.gif,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Intern - Big Data,"         Be a part of 10-12 Platform Engineers that are the crux for developing and maintaining Big Data (Data Lake, Data Warehouse and Data Integration) and advanced analytics platforms at SeaGate                     Apply your hands-on subject matter expertise in the Architecture of and administration of Big Data platforms - Data Warehouse Appliances , Open Data Lakes (AWS EMR, HortonWorks), Data Lake Technologies (AWS S3/Databricks/Other) and experience with ML and Data Science platforms (Spark ML , H2O , KNIME)     Develop and manage SPARK ETL Frameworks, Data orchestration with Airflow and support building PRESTO/Trino queries for the key stakeholders     Design, scale and deploy Machine Learning pipelines.     Collaborate with Application Architects and Business SMEs to design and develop end-to-end data pipelines and supporting infrastructure.     Establish and maintain productive relationships with peer organizations, partners, and software vendors           About you:                                   Excellent coding skills in any language with deep desire to learn new skills and technologies.     You re a passionate professional who is up to the challenge of blending the fast-changing technology landscape of Big Data analytics with the complex and high-impact space of HiTech and Manufacturing analytics     As a motivated self-starter, you have the experience working in a dynamic environment     Exceptional data engineering skills in large, high-scale Data platforms and applications using cloud and big data technologies like Hadoop ecosystem and Spark     Strong appetite for constant learning, thinking out of the box, questioning the problems & solutions with the intent to understand and solve better     Excellent interpersonal skills to develop relationships with different teams and peers in the organization                         Your experience includes:                               Big data processing frameworks knowledge: Spark, Hadoop, Hive, Kafka, EMR     Big data solutions on cloud (AWS or Other)     Advanced experience and hands-on architecture and administration experience on big data platforms     Data Warehouse Appliances, Hadoop (AWS EMR), Data Lake Technologies (AWS S3/GCS/Other) and experience with ML and Data Science platforms (Spark ML , H2O , KNIME )     Python, Java, Scala     DevOps, Continuous Delivery, and Agile development     Creating a culture of technical excellence by leading code and design reviews, promoting mentorship, and identifying and promoting educational opportunities for engineers       Strong understanding of Micro-services and container-based development using Docker and Kubernetes ecosystem is a BIG plus         Experience working in a Software Product Development environment is a BIG plus                     ",10224501040,01-02-2024,01-05-2024,EducationalOccupationalCredential,12,Engineering - Software & QA,Big Data Engineer,Electronic Components / Semiconductors,"RSS, orchestration, Appliances, Coding, Machine learning, SCALA, Data processing, big data, Data warehousing, Python",-,9am-6pm,"Full Time, Permanent",Seagate,Organization,Seagate,https://img.naukimg.com/logo_images/groups/v1/1293088.gif,Pune,Pune,-,-,-,Unpaid P.M ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
BigData Engineer,"       The BigData Engineers have expertise in building horizontally scalable applications using distributed technologies like NoSQL Dbs/Hadoop/Spark and others and we execute projects on-premise and cloud based systems.          The AI-Engineers and MLOps Engineers work on scaling AI-systems and in building end-to-end productionized MLOps pipelines.                RESPONSIBILITIES:          Our Big Data capability team needs hands-on developers who can produce beautiful functional code to solve complex analytics problems. If you are an exceptional developer with an aptitude to learn and implement using new technologies, and who loves to push the boundaries to solve complex business problems innovatively, then we would like to talk with you.      You would be responsible for evaluating, developing, maintaining, and testing big data solutions for advanced analytics projects.      The role would involve big data pre-processing reporting workflows including collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into business insights.      The role would also involve testing various machine learning models on Big Data and deploying learned models for ongoing scoring and prediction. An appreciation of the mechanics of complex machine learning algorithms would be a strong advantage.              QUALIFICATIONS:              Demonstrable experience designing technological solutions to complex data problems, developing testing modular, reusable, efficient and scalable code to implement those solutions.      Ideally, this would include work on the following technologies:      Expert-level proficiency in at-least one of Java, C++, or Python (preferred). Scala knowledge a strong advantage      Strong understanding and experience in distributed computing frameworks, particularly Apache Hadoop 2.0 (YARN; MR HDFS) and associated technologies -- one or more of Hive, Sqoop, Avro, Flume, Oozie, Zookeeper, etc.      Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage.      Operating knowledge of cloud computing platforms (AWS, especially EMR, EC2, S3, SWF services and the AWS CLI)      Experience working within a Linux computing environment, and use of command line tools including knowledge of shell/Python scripting for automating common tasks.      Ability to work in a team in an agile setting, familiarity with JIRA and clear understanding of how Git works.      Linux environment and shell scripting      ",1.50E+11,15-03-2024,13-06-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Cloud computing, C++, GIT, Linux, Shell scripting, Machine learning, Agile, JIRA, SQL, Python",-,9am-6pm,"Full Time, Permanent",Neal Analytics,Organization,Neal Analytics,https://img.naukimg.com/logo_images/groups/v1/2694638.gif,"Mumbai, Pune, Chennai, Gurugram, Bengaluru","Mumbai, Pune, Chennai, Gurugram, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"Apache Spark, Big Data Experts","   Experience with big data tools: Hadoop, Spark, Kafka, etc.     Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.     Experience with data pipeline and workflow management tools: Luigi, Airflow, etc.     Experience with stream-processing systems: Storm, Spark-Streaming, etc.     Experience with object-oriented/object function scripting languages: Python, Pyspark, Scala. ",1.20E+11,12-04-2023,11-07-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"NoSQL, spark, cassandra, Workflow management, SCALA, Hadoop, big data, SQL, Python, Scripting",-,9am-6pm,"Full Time, Permanent",Abylle Solutions,Organization,Abylle Solutions,-,"Mumbai, Gurugram, Bengaluru","Mumbai, Gurugram, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Senior AWS Data Engineer,"   Design and build large-scale enterprise data solutions and applications using one or more AWS data and analytics services using EMR, Lambda, Glue, DynamoDB, RedShift, Kinesis.          Analyze, re-architect, and re-platform on-premise data warehouses to data platforms on AWS cloud using AWS or 3rd party services.          Must have experience in design and build production data pipelines from ingestion to consumption within a big data architecture, using Java, Python, Scala.          MUST have HANDS-ON experience on Hadoop tools/technologies like Spark (Strong in Spark), Map Reduce, Hive, HDFS.          HANDS-ON expertise and excellent understanding of big data toolsets such as Sqoop, Spark[1]streaming, Kafka, NiFi.          Good working knowledge in NoSQL DB (Mongo, HBase, Casandra).          Implemented complex projects dealing with the considerable data size (TB/ PB) and with high complexity in the production environment. Hortonworks (HDPCA/HDPCD/HDPCD-Spark) or Cloudera certification is an added advantage.      Required Skills      Bachelor s degree or higher in a quantitative /technical field (e.g., Computer Science, Statistics, Engineering) and software development experience with proven hands-on experience in Big Data technologies.          Experience in developing/architecting environments in the Hadoop ecosystem using HDP and HDF.          Demonstrated strength in data modelling, ETL development, and Data.          Experience in designing and implementing an enterprise data lake.          Experience in Big Data Management and Big Data Governance.          Some experience with Kubernetes, Docker containers, etc    ",30423501229,03-04-2023,02-07-2023,EducationalOccupationalCredential,84,Engineering - Software & QA,Big Data Engineer,Analytics / KPO / Research,"Computer science, Data management, Data modeling, spark, data governance, big data, AWS, digital transformation, Analytics, Python",-,9am-6pm,"Full Time, Permanent",Biz-metric India,Organization,Biz-metric India,-,"Mumbai, Navi Mumbai, Pune, Gurugram, Bengaluru","Mumbai, Navi Mumbai, Pune, Gurugram, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Lead Data Engineer / AWS/ Azure/ GCP,"Experience working with distributed technology tools for?developing?Batch and Streaming?pipelines using.? SQL,?Spark, Python [3+ years], Scala [4+ years].Preferable ??Experience in Cloud Computing, e.g., AWS, GCP, Azure, etc. ??Able to quickly pick up new programming languages, technologies, and frameworks. ??Strong skills building positive relationships across Product and Engineering. ??Able to influence and communicate effectively, both verbally and written, with team members and business stakeholders",3.00E+11,30-04-2024,29-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Data Engineering, SCALA, Spark, Python, SQL, Azure Databricks, AWS, Gcp Cloud",-,9am-6pm,"Full Time, Permanent",Csquare Consultants Llp,Organization,Csquare Consultants Llp,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Etl Engineer,"Job responsibilities   ??Design, develop, and maintain highly scalable data processing applications   ??Write efficient, reusable and well documented code   ??Deliver big data projects using Spark, Scala, Python, SQL, HQL, Hive   ??Maintain and tune existing Hadoop/Hbase applications   ??Work closely with QA, Operations and various teams to deliver error free software on time   ??Actively participate in daily agile / scrum meetings  Skill set   ??5+ years of software development experience with Hadoop framework components(HDFS, Spark,  Sqoop, Hive, HQL, Spark, Scala, PySpark)   ??5+ years of experience using Scala/Java/Python, SQL and shell scripting  ??Experience in developing and tuning spark applications   ??Excellent understanding of spark architecture, data frames and tuning spark  ??Strong knowledge of database concepts, systems architecture, and data structures is a must  Process oriented with strong analytical and problem solving skills  ??Excellent written and verbal communication skills Bachelor's degree in Computer Science or  related field  ",30524007513,03-05-2024,01-08-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,Software Product,"Pyspark, Python, SQL",-,9am-6pm,"Full Time, Permanent",Circana,Organization,Circana,-,"Pune, Bengaluru","Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data - GCP (Immediate Joiner),"Immediate Joiner Preferred  Exp: 5+ Years We at Onix- Datametica Solutions Private Limited are looking for GCP Data Engineers/Leads who have a passion for cloud with knowledge and working experience of different GCP Data related services like Big Query and Composer. Required Past Experience: Hands on and deep experience working with Google Data Products (e.g. BigQuery, Dataflow, Dataproc ,Composer..) Good Understanding of Data Structures. Worked with large datasets and solving difficult analytical problems. Project Level Experience in Shell Scripting Experience working with GIT for Source Code Management Worked with Structured and unstructured data Required Skills and Abilities: Mandatory Skills -  Hands-on and deep experience working with  BigQuery, Composer ,  Python . ??Data Skill required: Hive, Cloudera, Scala, HQL, Shell, Python, Scala, Oozie, BigQuery, Dataflow, Compiler works (BQ data warehouse modernization) , DLP, Dataform, Terraform, Artifact Registry, Cloud Build Secondary Skills -Strong in Dataflow, Dataproc, Pubsub, Cloud Function and Shell Scripting  Data Analysis to identify the gaps and issues in data pipelines.  Good to have - Understanding of ETL Tools or SQL  Good To Have - Certifications in any of the following: GCP Professional Data Engineer",1.80E+11,18-04-2024,17-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"java, spark, GCP, Scala, Bigtable, python, dataflow, Bigquery, Dataproc",-,9am-6pm,"Full Time, Permanent",Onix- Datametica,Organization,Onix- Datametica,https://img.naukimg.com/logo_images/groups/v1/4659981.gif,"Pune, Bangalore Rural, Delhi / NCR","Pune, Bangalore Rural, Delhi / NCR",-,-,-,30-45 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Azure Big Data Engineer,"  Job Description Total Exp in years  5+ Skills: Azure SQL/T-SQL, Azure Data Factory, Azure Function apps, Azure Blob Responsibilities: Overall 4 ??6 years of relevant IT experience Primary Skills : 1. Experience in implementing at least 2 end to end Hadoop projects 2. Min 4 year experience on MS-Azure with HDInsights / Databricks 3. Min 4 years of experience on SparkCore, PySpark, Scala, Hive, Hbase, SparkSQL Secondary Skills: 1. Min 2+ years of experience with PostgreSQL 2. Experience with Azure Data Factory (ADF), Blob, ADLS. 3. Experience with NoSQL databases such as HBase, CosmosDB 4. Excellent communication Qualifications : BE,MCA,M.Tech or MS Additional information : BE/MCA with 4+ years of total experience with end to end Hadoop Project execution and Excellent communication Azure SQL/T-SQL, Azure Data Factory, Azure Function apps, Azure Blob ",2.40E+11,24-04-2024,23-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Azure Data Factory, Azure SQL/T-SQL, Azure Blob, Azure Function apps",-,9am-6pm,"Full Time, Permanent",Elorce Industries,Organization,Elorce Industries,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Senior Big Data Engineer,"  Senior Big Data Engineer:  Responsible for implementing the Jobrapido platform architecture for On Premise and Cloud systems. This role works closely with the development and product teams across different regions primarily in Europe and is part of the Scrum Team in an Agile Pod. The role includes Research and Continuous Development of new Products based on new Technologies and good understanding of data models and data warehouse concepts.  Qualifications:  Bachelors degree in Computer Science or an IT related field  5-7 years of experience working across different product domains in a product development/data management/ engineering role.  Good communication skills necessary to manage business requests and work with different teams across different geographies and time-zones; experience working with remote and distributed teams will be an added advantage.   Hands-on working knowledge and experience is  required  in: a. ELT Processes (Airflow, Nifi, Luigi, Spark, etc.) b. Relational Databases (PostgreSQL, MySQL, SQL, PLSQL, etc.) c. NoSQL Databases (Redis, Elastic Search, Mongo etc.) d. Big Data Solutions like Vertica, Big Query based Solutions e. Lambda/Kappa/Delta Architecture f. Programming Languages (Java, Scala, Python, etc.)  g. Streaming Technologies (Kafka, Pub/Sub, Apache Beam, etc.)  h. Agile Methodologies (Scrum, TDD, BDD, etc.) i. Cloud Platforms (AWS, Azure, GCP), preferably GCP j. Development of Complex Big Data Solutions like Spark, Dataflow, Big Query ML, Dataproc, Dataproc Hub   Experience with several of the following tools/technologies is  desirable : a. GIT/Bit Bucket, Jira, Maven/Gradle, Jenkins, SharePoint, Visual Studio Code, Jupiter Notebooks. b. Data Structures and Machine Learning Models    Knowledge of the following technologies is  a plus : a. Big Data ML toolkits, such as TensorFlow, SparkML, or H2O  b. Unix/Linux environments c. Continuous Integration and Continuous Delivery Tools like Jenkins, Git, etc.  d. Google Analytics  e. Containerization Technologies (Docker) ",2.20E+11,22-04-2024,21-07-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Airflow, NoSQL, Postgresql, GCP Data Engineer, Apache Nifi",-,9am-6pm,"Full Time, Permanent",STG Labs,Organization,STG Labs,-,Bengaluru,Bengaluru,-,-,-,25-30 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"- Should have 6 years of experience in Big data, spark, Scala, AWS/GCP.",1.80E+11,07-05-2024,05-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SCALA, Big Data, Spark",-,9am-6pm,"Full Time, Temporary/Contractual",Callesto,Organization,Callesto,-,Bengaluru,Bengaluru,-,-,-,10-16 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"Experience : 3.5 to 7yrs Mandatory Skills Sound knowledge on SPARK/SCALA, NIFI, KAFKA, PIG, HIVE & HQL About the Team/Project This role is within the HR Data team under the Human & Other Resources (H2R) unit of the Business Solution Centre (BSC). The H2R Big Data services center is involved in the BSC transformation by creating an agile data centric IT architecture for business such as: HR, Sourcing, Real Estate. The project team will be spread between Paris and Bangalore. So, the candidate with an experience of 1-7 years is expected to work and coordinate on daily basis with the remote teams. Ability to learn new technology / framework / methodology. Hands-on individual responsible for producing excellent quality of code, adhering to expected coding standards and industry best practices. Must have strong knowledge and working experience on Big DATA ecosystem. Must have strong experience in SPARK/SCALA, NIFI, KAFKA, HIVE, PIG. Must have Strong knowledge and experience working on HQL (hive Query Language) Must have strong strong expertise in Debugging and Fixing Production Issues on BIG DATA eco System. Knowledge on code version management using Git & Jenkins, Nexus. High levels of ownership and commitment on deliverables. Strong and Adaptive Communication Skills; Should be comfortable interacting with Paris counterparts to probe a technical problem or clarify requirement specifications. Behavioral Competencies : Communication- verbal and written Accountability Active Listening Conflict Management Decision Making Interpersonal skills Problem Solving Networking",1.70E+11,17-04-2024,16-07-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data, Jenkins, Hive, HQL, Git, Nexus, Scala, Kafka, NIFI, Spark, PIG",-,9am-6pm,"Full Time, Permanent",Societe Generale Global Solution Centre ,Organization,Societe Generale Global Solution Centre ,https://www.naukri.com/hotjobs/images/v3/societe_nov13.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Etl Engineer,"Job responsibilities   ??Design, develop, and maintain highly scalable data processing applications   ??Write efficient, reusable and well documented code   ??Deliver big data projects using Spark, Scala, Python, SQL, HQL, Hive   ??Maintain and tune existing Hadoop/Hbase applications   ??Work closely with QA, Operations and various teams to deliver error free software on time   ??Actively participate in daily agile / scrum meetings  Skill set  ??5+ years of software development experience with Hadoop framework components(HDFS, Spark,  Sqoop, Hive, HQL, Spark, Scala, PySpark)   ??5+ years of experience using Scala/Java/Python, SQL and shell scripting   ??Experience in developing and tuning spark applications   ??Excellent understanding of spark architecture, data frames and tuning spark   ??Strong knowledge of database concepts, systems architecture, and data structures is a must  Process oriented with strong analytical and problem solving skills   ??Excellent written and verbal communication skills Bachelor's degree in Computer Science or  related field  ",1.50E+11,15-04-2024,14-07-2024,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,Software Product,"Spark, Python, SQL",-,9am-6pm,"Full Time, Permanent",Circana,Organization,Circana,-,"Pune, Bengaluru","Pune, Bengaluru",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Kafka Professionals," Global Pharma Tek is looking for Kafka to join our dynamic team and embark on a rewarding career journey        you will be responsible for designing, implementing, and maintaining Kafka-based solutions to support our organization's data processing needs      Responsibilities:Design, develop, and deploy Kafka-based solutions to meet business requirements      Configure and manage Kafka clusters for optimal performance and reliability      Develop Kafka Connectors for integrating Kafka with various data sources and sinks      Implement Kafka Streams for real-time data processing and analytics      Monitor Kafka infrastructure and troubleshoot issues to ensure system stability      Collaborate with cross-functional teams to design and implement end-to-end data pipelines      Implement security measures to protect data in transit and at rest within Kafka clusters      Optimize Kafka configurations for performance, scalability, and resource utilization      Conduct performance tuning and capacity planning for Kafka clusters      Document technical designs, deployment procedures, and best practices    ",1.30E+11,13-02-2024,13-05-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,Pharmaceutical & Life Sciences,"rest, visualforce, project management, performance tuning, web services, configuration, customization, sfdc, triggers, javascript, apex, salesforce, sales force development, salesforce crm, technical design, data loader, java, capacity planning, kafka",-,9am-6pm,"Full Time, Permanent",Global Pharma Tek,Organization,Global Pharma Tek,https://img.naukimg.com/logo_images/groups/v1/1012164.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"         Experience: 5 to 8 years             Overview             Working within a cross functional, agile development team, this Big Data Developer will be responsible for hands on development and technical delivery of innovative components used in data driven solutions       The candidate will have a sound working knowledge of spark, Scala, NoSQL, Bigdata SQL, Hadoop, SQL      The candidate should be self-motivated and be able to drive through rapidly changing requirements typically found in innovative business units           Responsibilities             1+ Years of experience with Big Query SQL             4+ years of experience with Scala or SPARK             Very strong in SQL             Experience with Cloud liek AWS, GCP etc          BE / B.Tech (CSE), ME/M.Tech. (ECE/EEE/IT) or any equivalent qualification.   ",2.71E+11,27-06-2022,25-09-2022,EducationalOccupationalCredential,48,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"NoSQL, PDF, GCP, spark, Agile development, SCALA, Cloud, MS Word, big data, SQL",-,9am-6pm,"Full Time, Permanent",Tech Highbrows Software Solutions,Organization,Tech Highbrows Software Solutions,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Scala /Spark Professional,"     3 years of experience working in software development building modern and scalable big data applications      Expertise with functional programming, preferably Scala      Fluency in Hadoop technologies and big data processing technologies, e.g. Spark, YARN, HDFS, Oozie, Hive      Experience implementing RESTful web services in Java, Scala, Python, or similar languages.      Experience with NoSQL and SQL databases      Experience in information retrieval and machine learning      Familiarity with GeoSpatial concepts is a plus      Past Apple experience is a plus      ",2.51E+11,25-11-2022,23-02-2023,EducationalOccupationalCredential,12,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"NoSQL, Web services, Machine learning, SCALA, Data processing, Information retrieval, big data, YARN, SQL, Python",-,9am-6pm,"Full Time, Permanent",eTeam Inc.,Organization,eTeam Inc.,https://img.naukimg.com/logo_images/groups/v1/228540.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Senior Big Data Engineer,"     To be successful in this role, you should possess                   Collaborate closely with Product Management and Engineering leadership to devise and build the right solution.   Participate in Design discussions and brainstorming sessions to select, integrate, and maintain Big Data tools and frameworks required to solve Big Data problems at scale.   Design and implement systems to cleanse, process, and analyze large data sets using distributed processing tools like Akka and Spark.   Understanding and critically reviewing existing data pipelines, and coming up with ideas in collaboration with Technical Leaders and Architects to improve upon current bottlenecks   Take initiatives, and show the drive to pick up new stuff proactively, and work as a Senior Individual contributor on the multiple products and features we have.   7+ years of experience in developing highly scalable Big Data pipelines.   In-depth understanding of the Big Data ecosystem including processing frameworks like Spark, Akka, Storm, and Hadoop, and the file types they deal with.   Experience with ETL and Data pipeline tools like Apache NiFi, Airflow etc.   Excellent coding skills in Java or Scala, including the understanding to apply appropriate Design Patterns when required.   Experience with Git and build tools like Gradle/Maven/SBT.   Strong understanding of object-oriented design, data structures, algorithms, profiling, and optimization.   Have elegant, readable, maintainable and extensible code style.                       You are someone who would easily be able to       Work closely with the US and India engineering teams to help build the Java/Scala based data pipelines   Lead the India engineering team in technical excellence and ownership of critical modules; own the development of new modules and features   Troubleshoot live production server issues.   Handle client coordination and be able to work as a part of a team, be able to contribute independently and drive the team to exceptional contributions with minimal team supervision   Follow Agile methodology, JIRA for work planning, issue management/tracking       Additional Project/Soft Skills:       Should be able to work independently with India & US based team members.   Strong verbal and written communication with ability to articulate problems and solutions over phone and emails.   Strong sense of urgency, with a passion for accuracy and timeliness.   Ability to work calmly in high pressure situations and manage multiple projects/tasks.   Ability to work independently and possess superior skills in issue resolution.   Should have the passion to learn and implement, analyse and troubleshoot issues               ",2.31E+11,23-09-2022,22-12-2022,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,Film / Music / Entertainment,"Product management, Object oriented design, Maven, Coding, Consulting, Data structures, Apache, JIRA, Analytics, Monitoring",-,9am-6pm,"Full Time, Permanent",Digit88,Organization,Digit88,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Admin,   We are seeking      Big Data Admin     candidates who have experience in      hadoop     and      spark     . You will have the opportunity to work with a talented team. Apply now and be part of our innovative and dynamic organization.               This position is responsible for-               Installing Hadoop in Linux environment.               Deployment in a Hadoop cluster and its maintenance.               Health check of a Hadoop cluster monitoring whether it is up and running all the time.               Analyse the storage data volume and allocating the space in HDFS.               Resource management in a cluster environment. This involves new node creation and removal of unused ones.               Configuring NameNode to ensure its high availability               Implementing and administering Hadoop infrastructure on an ongoing basis.               Required hardware and software deployment in Hadoop environment. Furthermore to expanding of existing environments.               Software installation and its configuration.               Backup and recovery operation of Database.               Checking database connectivity and its security measurements.               Performance monitoring and fine tuning on actual basis.               Managing and optimizing disk space for handling data               Installing patches and upgrading software as and when needed.               Automate manual tasks for faster performance.               A Hadoop administrator loads a large volume of data   ,2.31E+11,23-08-2023,21-11-2023,EducationalOccupationalCredential,84,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Administration, Software installation, Linux, Database, Deployment, Manual, hadoop administrator, Resource management, big data, Performance monitoring",-,9am-6pm,"Full Time, Permanent",Intuitive Apps,Organization,Intuitive Apps,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
ETL with Big Data,"         Should have good knowledge in SQL and Unix commands.     Should have good knowledge in ETL process     Should possess at least 3+ years experience with Bigdata testing, Hadoop, HDFS and Hive.     Good to have Shell scripting knowledge       ",2.00E+11,20-03-2023,18-06-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,Recruitment / Staffing,"Unix, hive, Shell scripting, Hadoop, hdfs, big data, SQL, Testing",-,9am-6pm,"Full Time, Permanent",Capleo Global,Organization,Capleo Global,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Develpoers,"   We are actively seeking skilled and experienced Big Data Developers to join our dynamic technology team      As a Big Data Developer, you will play a key role in designing, developing, and maintaining scalable and high-performance data processing solutions      If you have a strong background in Big Data technologies, a passion for data-driven solutions, and a desire to work on cutting-edge projects, we encourage you to apply        Responsibilities:        Big Data Architecture: Design and implement robust and scalable Big Data solutions, considering factors such as performance, reliability, and security      Data Processing: Develop efficient and high-performance data processing pipelines using Big Data technologies such as Apache Hadoop, Apache Spark, or Apache Flink      Data Modeling: Design and implement data models for efficient storage and retrieval of large-scale datasets      Optimize data structures for performance and scalability      Integration: Collaborate with cross-functional teams, including data scientists, business analysts, and software engineers, to integrate Big Data solutions into end-to-end systems      Performance Optimization: Identify and address performance bottlenecks in data processing and storage      Implement optimizations to enhance overall system performance      Real-time Data Processing: Work on real-time data processing solutions, leveraging technologies like Apache Kafka or other stream processing frameworks      Data Security: Implement and adhere to security best practices in Big Data development, ensuring the confidentiality and integrity of sensitive data      Troubleshooting and Debugging: Identify and resolve issues related to data processing, ensuring the reliability and accuracy of the data      Documentation: Create and maintain comprehensive documentation for Big Data solutions, including architecture, design, and implementation details      Continuous Learning: Stay updated on emerging trends and advancements in Big Data technologies      Evaluate and recommend new tools and frameworks that can enhance our data processing capabilities    ",1.90E+11,19-02-2024,19-05-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"big data administration, hive, python, scala, oozie, big data technologies, data architecture, apache flink, scalability, sql, apache, java, data modeling, mapreduce, spark, kafka, debugging, data structures, hadoop, sqoop, big data, aws, real-time data processing, hbase",-,9am-6pm,"Full Time, Permanent",Andor Tech,Organization,Andor Tech,https://img.naukimg.com/logo_images/groups/v1/4690465.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Azure Data Factory,"         Experience with big data tools: Hadoop, Spark, Kafka, etc.             Experience with relational SQL and NoSQL databases.             Experience with data pipeline and workflow management tools: Azure ADF , Azkaban, Luigi, Airflow, etc.             Experience with native cloud services: Azure cloud native apps             Experience with stream-processing systems: Storm, Spark-Streaming, etc.             Experience with object-oriented/object function scripting languages: Python, Java, C , Scala, etc.         ",1.71E+11,17-08-2022,15-11-2022,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"C, NoSQL, spark, Cloud Services, Workflow management, SCALA, big data, SQL, Python, Scripting",-,9am-6pm,"Full Time, Permanent",eTeam Inc.,Organization,eTeam Inc.,https://img.naukimg.com/logo_images/groups/v1/228540.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"   Resource and Project Timeline Estimation      Defining project and program management process Prioritizing task within project     : Task allocation across resources - capable of evaluating resources by their skills and improvement areas - prioritizes resource development      Contributes to org-level activities such as sales collaterals, designing proposals, guiding teams through Proof of Concepts and training      Recruit, train, develop and supervise team of Associates Consultants      Expertise in requirement gathering, technical design and functional documents.      Should have proven experience in delivering projects using Agile frameworks such as Scrum, Kanban      Experience in leading conversations with clients and onsite counterparts      Strong analytical and logical skills.      Must be able to comfortably tackle new challenges and learn.      Must have strong verbal and written communication skills.   ",1.41E+11,14-09-2023,13-12-2023,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Supervisor, HR Executive, Technical design, Analytical, Manager Program Management, Agile, Scrum, big data, Software solutions, Recruitment",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"     Experience           6+ years experience in Big Data Hadoop, MapReduce, Hive, Sqoop and Spark with hands-on expertise in design and implementation of high data volume solutions (ETL & Streaming).         Experience with building stream-processing systems, using solutions such as Storm, Spark-Streaming, Kafka streams         Extensive experience in working with Big Data tools like Pig, Hive, Athena, Glue, Snowflake and EMR         Experience with NoSQL databases, such as HBase, Cassandra, MongoDB         Knowledge of various ETL techniques and frameworks, such as Flume         Experience with various messaging systems, such as Kafka or RabbitMQ         Good understanding of Lambda Architecture, along with its advantages and drawbacks   ",1.41E+11,14-07-2022,12-10-2022,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,Advertising & Marketing,"NoSQL, Architecture, cassandra, flume, MongoDB, sqoop, big data, Monitoring, Data architecture, HBase",-,9am-6pm,"Full Time, Permanent",Maiden Marketing,Organization,Maiden Marketing,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer,"Hi  Hiring Big Data Engineer for US based IT MNC Big Data Engineer Salary : 10 - 35 LPA Experience : 5 - 20 Years Job Type : Full Time Location :  Remote or Bengaluru, Hyderabad, Mumbai, Pune, Chennai, Delhi/NCR Key Skills :  Big Data, Big Data Engineering Note :  Applicant must be graduate with 5 years of experience in big data engineering in any skills specialisation.",1.40E+11,14-03-2024,12-06-2024,EducationalOccupationalCredential,120,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Big Data, Big Data Analytics, Data Engineering",-,9am-6pm,"Full Time, Permanent",Big Data Engineer,Organization,Big Data Engineer,-,"Bengaluru,Karnataka","Bengaluru,Karnataka",-,-,-,12-22 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Production Support Engineer,"                                                     1. Production Support Leadership: Lead and manage a team of Production Support Engineers, providing guidance, mentorship, and technical expertise. Foster a collaborative and high-performing environment, ensuring the team is equipped with the necessary skills and knowledge to handle production incidents effectively.   2. Incident Management: Monitor the Big Data infrastructure and applications, anticipating and promptly resolving production incidents. Investigate and troubleshoot issues, minimizing impact and downtime. Develop and implement effective incident management processes and procedures to ensure timely resolution.   3. Change Management: Establish and enforce change management processes to ensure smooth deployments and minimize risks. Collaborate with development teams to review and approve changes, assess potential impacts, and coordinate with stakeholders. Ensure proper documentation and communication throughout the change process.   4. Performance Monitoring and Optimization: Implement monitoring and alerting mechanisms to proactively identify performance bottlenecks and optimize the Big Data infrastructure. Analyze system performance metrics, diagnose issues, and implement enhancements to ensure optimal performance and scalability.   5. Collaboration and Coordination: Collaborate with cross-functional teams, including data engineering, infrastructure, and application development teams, to resolve complex issues and ensure seamless integration of the Big Data platform. Coordinate with stakeholders to understand requirements, provide updates, and ensure effective governance.   6. Automation and Process Improvement: Identify opportunities for automation and process improvement within the Big Data production support function. Streamline manual tasks, automate monitoring and deployment processes, and implement tools and frameworks to enhance efficiency and reliability.   7. Documentation and Knowledge Sharing: Create and maintain comprehensive documentation, including standard operating procedures, troubleshooting guides, and knowledge base articles. Promote knowledge sharing within the team and across the organization to enhance overall productivity and reduce resolution times.                     Profile required             ?     Proficiency in Big Data technologies such as Hadoop, Spark, Hive, HBase, Kafka, and related ecosystem tools.   Experience with cluster management and resource allocation frameworks like YARN or Mesos.   Expertise in Linux/Unix operating systems and shell scripting (e.g., Bash).   Strong understanding of distributed computing principles and performance optimization techniques.   Experience with configuration management and automation tools like Ansible or Puppet.   Familiarity with version control systems like Git and collaborative development platforms like GitHub.   Knowledge of SQL and scripting languages like Python for data manipulation and analysis.   Understanding of monitoring and logging tools such as ELK stack (Elasticsearch, Logstash, Kibana) or Splunk.   Experience with job scheduling and orchestration tools like Control-M, Oozie, or Airflow.           ",1.10E+11,11-03-2024,09-06-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Unix, Change management, Production support, Linux, Configuration management, Shell scripting, Application development, Information technology, SQL, Python",-,9am-6pm,"Full Time, Permanent",Societe Generale Global Solution Centre ,Organization,Societe Generale Global Solution Centre ,https://img.naukri.com/logo_images/v3/1496734.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Fresher,"   Good hands-on experience in Scala, Spark, and HIVE is MUST.      1. Experience in handling transformation in Hadoop using Scala and Spark SQL and Core Good knowledge of the HDP and Cloudera ecosystem.      2. Hands-on in Oozie is preferred but not MUST HAVE.      3. Extensively worked on Performance improvements.      4. Good debugging and Data analytical skills.      5. Drive out features via appropriate test frameworks, like Junit, Scala tests.      6. Knowledge of Kafka or similar messaging systems will be an add-on.      7. Good knowledge of CI/CD tools like Jenkins, Version control Tools like GIT.        Qualification:    Any Graduate    ",1.01E+11,10-11-2022,08-02-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,Software Product,"Analytical skills, GIT, Version control, spark, oozie, Debugging, SCALA, jenkins, SQL, Testing",-,9am-6pm,"Full Time, Permanent",Brisa Technologies,Organization,Brisa Technologies,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Data Engineer-Big Data," Design and implement scalable data processing systems that can handle large volumes of structured and unstructured data.Work with tools such as Hadoop, Spark, and NoSQL databases to create and maintain data pipelines.Ensure the performance, scalability, and reliability of the data processing systems.Write efficient and high-quality code using programming languages.     Collaborate with cross-functional teams, including data scientists, business analysts, and product managers, to identify new opportunities for data-driven innovation.Continuously monitor and optimize the data processing systems to ensure optimal performance and efficiency.Excellent problem-solving skills and the ability to think creatively.Good communication and collaboration skills, with the ability to work effectively in a team environment. ",1.00E+11,10-03-2023,08-06-2023,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,Recruitment / Staffing,"spark, Hadoop, SCALA, big data",-,9am-6pm,"Full Time, Permanent",Triangle Global,Organization,Triangle Global,https://img.naukri.com/logo_images/v3/4728742.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Big Data Engineer - Lead,"   Experience in advanced Apache Spark processing framework, spark programming languages such as Scala/Python/Advanced Java with sound knowledge in shell scripting.     Experience in working with Core Spark, Spark Streaming, Data Frame API, Data set API, RDD APIs & Spark SQL programming dealing with processing terabytes of data. Specifically, this experience must be in writing ""Big Data"" data engineering jobs for large scale data integration in AWS.     Advanced SQL experience using Hive/Impala framework including SQL performance tuning     Experience in writing spark streaming jobs integrating with streaming frameworks such as Apache Kafka or AWS Kinesis.     Experience working in Key/Value data store such as HBase     Experience in AWS services such as EMR, Glue (server less architecture), S3, Athena, IAM, Lambda and Cloud watch is required.           Key Skills :       Bigdata      Apache      Spark      Scala/python/advanced Java                            ",81222503223,08-12-2022,08-03-2023,EducationalOccupationalCredential,84,Engineering - Software & QA,Big Data Engineer,Software Product,"Performance tuning, Advanced Java, Shell scripting, SCALA, Apache, big data, AWS, SQL, Python, HBase",-,9am-6pm,"Full Time, Permanent",MM STAFFING CAREER CONSULTANTS PRIVATE LIMITED,Organization,MM STAFFING CAREER CONSULTANTS PRIVATE LIMITED,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Azure Big Data Specialist,"       Overall 8+ years of experience in IT Industry      Min 3+ year experience working on Data Engineering using Databricks,Synapse, ADF      Atleast 4 Project experience in Building and maintaining ETL / ELT pipelines for large data sets , complex data and feature engineering processesMust Have skills : Azure Data factory (ADF), Azure Databricks (ADB), Azure Synapse Analytics (SQL DW), Azure Analysis Services (AAS), Azure Data Lake Storage (ADLS)      Azure SQL Database (SQL DB), SQLExperience with NoSQL databases such as cosmosdb, Cassandra, CosmosDB, MongoDB      Experience in Real-Time Data Processing using Eventhub,IoT Hub,Apache Kafka ,Structured Streaming and Stream analytics      Experience with SparkCore, Spark Streaming, PySpark, Hive, Impala, SparkSQL, CosmosDB, Kafka, Azure Data Factory (ADF), Blob, ADLS, EventHub      Sound Knowledge on Azure DevOps and CI/CD tools like Jira, Confluence, Bamboo, Bitbucket      Hands on experience in RDBMS like MSSQL,Oracle,Mysql ,Teradata              Qualifications        BE, MS, M.Tech or MCA    ",60224500545,06-02-2024,06-05-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,Pharmaceutical & Life Sciences,"RDBMS, MySQL, Data processing, MongoDB, Oracle, Apache, Teradata, JIRA, Business solutions, Analytics",-,9am-6pm,"Full Time, Permanent",Global Pharma Tek,Organization,Global Pharma Tek,https://img.naukimg.com/logo_images/groups/v1/1012164.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"Big Data engineer(scala,Hadoop,Kafka,elastic search )","   Designing and implementing fine-tuned production ready data/ML pipelines in Hadoop platform.     Driving optimization, testing and tooling to improve quality.     Reviewing and approving high level detailed design to ensure that the solution delivers to the business needs and align to the data analytics architecture principles and roadmap.     Understanding business requirement and solution design to develop and implement solutions that adhere to big data architectural guidelines and address business requirements.     Following proper SDLC (Code review, sprint process).     Identifying, designing, and implementing internal process improvements: automating manual processes, optimizing data delivery, etc.     Building robust and scalable data infrastructure (both batch processing and real-time) to support needs from internal and external users     Understanding various data security standards and using secure data security tools to apply and adhere to the required data controls for user access in Hadoop platform.     Supporting and contributing to development guidelines and standards for data ingestion     Working with data scientist and business analytics team to assist in data ingestion and data related technical issues.     Designing and documenting the development deployment flow.       Requirements       Experience in developing rest API services using one of the Scala frameworks     Ability to troubleshoot and optimize complex queries on the Spark platform     Expert in building and optimizing big data data/ML pipelines, architectures and data sets     Knowledge in modelling unstructured to structured data design.     Experience in Big Data access and storage techniques.     Experience in doing cost estimation based on the design and development.     Excellent debugging skills for the technical stack mentioned above which even includes analyzing server logs and application logs.     Highly organized, self-motivated, proactive, and ability to propose best design solutions.     Good time management and multitasking skills to work to deadlines by working independently and as a part of a team.     Ability to analyse and understand complex problems.     Ability to explain technical information in business terms.     Ability to communicate clearly and effectively, both verbally and in writing.     Strong in user requirements gathering, maintenance and support     Excellent understanding of Agile Methodology.     Good experience in Data Architecture, Data Modelling, Data Security.       Experience -Must have:     a) Scala: Minimum 2 years of experience     b) Spark: Minimum 2 years of experience     c) Hadoop: Minimum 2 years of experience (Security, Spark on yarn, Architectural     knowledge)     d) Hbase: Minimum 2 years of experience     e) Hive - Minimum 2 years of experience     f) RDBMS (MySql / Postgres / Maria) - Minimum 2 years of experience     g) CI/CD Minimum 1 year of experience     Experience (Good to have):     a) Kafka     b) Spark Streaming     c) Apache Phoenix     d) Caching layer (Memcache / Redis)     e) Spark ML     f) FP (Scala cats / scalaz)     Qualifications       Bachelors degree in IT, Computer Science, Software Engineering, Business Analytics or equivalent with at-least 2 years of experience in big data systems such as Hadoop as well as cloud-based solutions.   ",51223501271,05-12-2023,04-03-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"RDBMS, MySQL, Debugging, Machine learning, Agile methodology, Apache, Business solutions, SDLC, Data architecture",-,9am-6pm,"Full Time, Permanent",Coders Brain Pvt Ltd,Organization,Coders Brain Pvt Ltd,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Senior Big Data Engineer,"       5+ years of experience working with Java and Spring technologies             At least 3 years of programming experience working with Spark on big data; including experience with data profiling and building transformations             Knowledge on microservices architecture is plus             Experience with any of NoSQL databases such HBase, MongoDB, or Cassandra             Experience with Kafka or any streaming tools             Knowledge of Scala would be preferable             Experience with agile application development             Exposure of any Cloud Technologies including containers and Kubernetes             Demonstrated experience of performing DevOps for platforms             Strong Skillsets in Data Structures & Algorithm in using efficient way of code complexity             Exposure to Graph databases             Passion for learning new technologies and the ability to do so quickly             A Bachelor s degree in a computer related field or equivalent professional experience is required   ",30423501879,03-04-2023,02-07-2023,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"NoSQL, PDF, cassandra, SCALA, Agile, Data structures, Application development, MongoDB, big data, HBase",-,9am-6pm,"Full Time, Permanent",Banyan Data Services,Organization,Banyan Data Services,-,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Bigdata Architect,"Responsibilities:  Reporting into the CTO,  the Technical Solutions Architect will be engaged in detailed in- depth technology assessments to recommend and validate technology and architecture are the appropriate solutions for our clients.  	Experience in normal solutions architecture before making the move to big data solutions.   	3 years of total IT experience and needs to have experience with the major big data solutions like Hadoop,  MapReduce,  Hive,  HBase,  MongoDB,  Cassandra.   	Should have experience in designing solutions for multiple large data warehouses with a good understanding of cluster and parallel architecture as well as high- scale or distributed RDBMS and/ or knowledge on NoSQL platforms.   	Should have experience with one of the large cloud- computing infrastructure solutions like Amazon Web Services or Elastic MapReduce.   	Skilled with cross- industry,  cross- functional and cross- domain know- how.   	Sketching the big data solution architecture,  then monitors and governs the implementation.",10717505443,30-03-2023,28-06-2023,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Technical product configuration, c++, C, design, MySQL, JavaScript, integration PHP, HTML",-,9am-6pm,"Full Time, Permanent",Entropik Tech,Organization,Entropik Tech,https://img.naukimg.com/logo_images/groups/v1/3389178.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Spark Scala Developer,"  Responsibilities A day in the life of an Infoscion As part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction. You will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain. You will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews. You will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes. You would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you! Technical and Professional Requirements: Primary skills:Technology->Big Data - Data Processing->Spark,Technology->Functional Programming->Scala Preferred Skills: Technology->Functional Programming->Scala Technology->Big Data - Data Processing->Spark Additional Responsibilities: Knowledge of more than one technology Basics of Architecture and Design fundamentals Knowledge of Testing tools Knowledge of agile methodologies Understanding of Project life cycle activities on development and maintenance projects Understanding of one or more Estimation methodologies, Knowledge of Quality processes Basics of business domain to understand the business requirements Analytical abilities, Strong Technical Skills, Good communication skills Good understanding of the technology and domain Ability to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods Awareness of latest technologies and trends Excellent problem solving, analytical and debugging skills Educational Requirements Bachelor of Engineering Service Line Data & Analytics Unit: Spark Scala Role:  Big Data Engineer Industry Type:  IT Services & Consulting Department:  Engineering - Software & QA Employment Type:  Full Time, Permanent Role Category:  Software Development Education UG:  B.Tech/B.E. in Any Specialization PG:  Any Postgraduate * Location of posting is subject to business requirements",20424007140,15-05-2024,13-08-2024,EducationalOccupationalCredential,36,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"Spark Core, Scala Programming, Big Data",-,9am-6pm,"Full Time, Permanent",Infosys,Organization,Infosys,https://www.naukri.com/hotjobs/images/v3/infosys_nov13.gif,"Bengaluru,Karnataka","Bengaluru,Karnataka",-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
London:  Scala + Big Data Developer - Expert /Consultant - BFSI domain,"%%%%%%%%%%%% Please Note: Please apply only if you have top technical skills in Scala / Bigdata - Experience. Immediately available candidates = Please share  your CV to WhatsApp : +44 77 2725 3636 %%%%%%%%%%%% Job Title: ======== Scala + Big Data Developer - expert /consultant - BFSI domain Job Location: =========== London - United Kingdom Salary Per Annum: =============== GBP 40K - 60K (Depending on Experience) UK - Work permit / visa will be sponsored by the company Qualification: ========== B.Tech / M.Tech / MCA / M.Sc or equivalent Experience Needed: ===================== Over all: 5+ Years IT experience Solid 2+ Year experience in Scala functional programming You must be Scala Developer with sound programming skills along with Big Data project environment skills. Technology Stack:- ===================== Scala (Processing Engine) Spark (Processing Engine) Tools: ===== Apache NIfi Sqoop Kafka Job responsibilities: ================ - Developing Restful micro services in Scala - Finch Spray/Akka, Cats,Shapeless and Play - Services which uses Kafka,Spark streaming and Cassandra - Pair programming / TDD - Develop and deliver Scala components to internal business team users - demonstrate a systematic and disciplined architectural, system design and programming approach - Develop, prepare and maintain system documentation, including program descriptions and operational procedures - Act as support personal during production releases - Good skills with Big Data Nice to have: =========== Machine Learning / Python language skills Business Verticals: =============== - Financial / Banking - Telecom - Travel - Healthcare No.of jobs:  ========= 02 Job Ref: ======== UK_SCALA_0524 Email: ===== spectrumconsulting1983@gmail.com Whats app: =========  +44 77 2725 3636 If you are interested, please email your cv as ATTACHMENT with  job ref. code [ UK_SCALA_0524 ] as subject",20624000772,02-06-2024,31-08-2024,EducationalOccupationalCredential,60,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"SCALA, Hive, Cloudera, Big Data",-,9am-6pm,"Full Time, Permanent",Spectrum Consulting,Organization,Spectrum Consulting,https://www.naukri.com/hotjobs/images/v3/spectrum_mar22.gif,"Hyderabad/ Secunderabad, Pune, Bangalore/ Bengaluru","Hyderabad/ Secunderabad, Pune, Bangalore/ Bengaluru",-,-,-,20-35 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
"Senior Data Engineers (Scala, Spark/PySpark, SQL) Domain: Semicon","primarily experts of Spark 3.X, SCALA, Delta lake implementation, streaming solution implementation for IOT in Spark streaming. Kafka expertise.  Any experience with MFG BI, DWH and Datalake implementation would be a bonus. Required Candidate profile Good knowledge of the business vertical with prior experience in solving different use cases in the manufacturing or similar industry. Experience with big data tools: Spark,Delta,CDC,NiFi, Kafka, etc",30624006965,03-06-2024,01-09-2024,EducationalOccupationalCredential,96,Engineering - Software & QA,Big Data Engineer,Electronics Manufacturing (Electronic Manufacturing Services (EMS)),"Spark 3.X, SCALA, MFG BI, Kafka expertise, IOT, manufacturing or similar industry, DWH, Delta lake implementation, object-oriented languages",-,9am-6pm,"Full Time, Permanent",Mindstech Recruitment,Organization,Mindstech Recruitment,-,Bengaluru,Bengaluru,-,-,-,17-25 Lacs P.A ,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Data Engineer: Big Data,"     Minimum 6 years of experience in Big Data technologies      Minimum 4 Years of experience in Python and Scala Programming      Experience in developing applications on Big Data and Cognitive technologies including API development      Application Development background along with knowledge of Analytics libraries, Open-source Natural Language Processing,Statistical and Big Data Computing libraries      Expertise in Spark, Scala and Kafka technologies      Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers.           Roles Responsibilities              As Data Engineer, you will develop, maintain, evaluate and test big data solutions        You will be involved in the design of data solutions using Hadoop based technologies along with Python Spark programming      Responsible to Ingest data from files, streams and Databases      Process the data with Spark, Scala, Kafka, Hive and Scoop      Develops Hadoop applications using Horton Works or other Hadoop distribution      Experienced with pulling data from various database systems, Network Elements and unstructured text from web, Social Media Sites and other Domain Specific file      Develop efficient software code for multiple use cases leveraging Python and Big Data technologies for various use cases built on the platform          Pan India    ",1.01E+11,10-11-2022,08-02-2023,EducationalOccupationalCredential,72,Engineering - Software & QA,Big Data Engineer,IT Services & Consulting,"IT services, Product engineering, Social media, Application development, Open source, Business solutions, Information technology, Unix shell scripting, Analytics, Python",-,9am-6pm,"Full Time, Permanent",MNJ Software,Organization,MNJ Software,-,Remote,Remote,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer
Kafka Professionals," Global Pharma Tek is looking for Kafka to join our dynamic team and embark on a rewarding career journey        you will be responsible for designing, implementing, and maintaining Kafka-based solutions to support our organization's data processing needs      Responsibilities:Design, develop, and deploy Kafka-based solutions to meet business requirements      Configure and manage Kafka clusters for optimal performance and reliability      Develop Kafka Connectors for integrating Kafka with various data sources and sinks      Implement Kafka Streams for real-time data processing and analytics      Monitor Kafka infrastructure and troubleshoot issues to ensure system stability      Collaborate with cross-functional teams to design and implement end-to-end data pipelines      Implement security measures to protect data in transit and at rest within Kafka clusters      Optimize Kafka configurations for performance, scalability, and resource utilization      Conduct performance tuning and capacity planning for Kafka clusters      Document technical designs, deployment procedures, and best practices    ",1.30E+11,13-02-2024,13-05-2024,EducationalOccupationalCredential,24,Engineering - Software & QA,Big Data Engineer,Pharmaceutical & Life Sciences,"rest, visualforce, project management, performance tuning, web services, configuration, customization, sfdc, triggers, javascript, apex, salesforce, sales force development, salesforce crm, technical design, data loader, java, capacity planning, kafka",-,9am-6pm,"Full Time, Permanent",Global Pharma Tek,Organization,Global Pharma Tek,https://img.naukimg.com/logo_images/groups/v1/1012164.gif,Bengaluru,Bengaluru,-,-,-,Not disclosed,,,,,,,,,,,,,,,,,,,,,,,Big Data Engineer

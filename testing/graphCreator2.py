import os
import pandas as pd
import unicodedata
import numpy as np
import faiss
from sklearn.preprocessing import MultiLabelBinarizer
import re

# Directories for CSV files and for saving FAISS index/mapping files
CSV_DIR = './subsets'
INDEX_DIR = './indexes'
os.makedirs(INDEX_DIR, exist_ok=True)

# FAISS index parameters
M = 32                 # Number of neighbors in HNSW graph
EF_CONSTRUCTION = 200  # Search depth for HNSW graph

def preprocess_text(text):
    """
    Preprocess text by normalizing Unicode, converting to lowercase,
    removing punctuation, and extra whitespace.
    """
    if not isinstance(text, str):
        return ''
    text = unicodedata.normalize('NFKD', text)
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def preprocess_and_split(text):
    """
    Preprocess the text and then split it into a list of skills.
    Assumes that skills in the original text are comma-separated.
    """
    processed_text = preprocess_text(text)
    # Split on commas and remove empty tokens
    skills = [skill.strip() for skill in processed_text.split(',') if skill.strip()]
    return skills

def process_csv(csv_file):
    """
    Process a single CSV file and create a FAISS index using a binary
    representation generated by MultiLabelBinarizer.
    """
    # Derive category name from CSV filename
    category_name = csv_file.replace('_subset.csv', '')
    print(f"\nüìå Processing category: {category_name}")

    csv_path = os.path.join(CSV_DIR, csv_file)
    try:
        df = pd.read_csv(csv_path, header=None)
    except Exception as e:
        print(f"‚ùå Error reading {csv_file}: {e}")
        return

    if 10 not in df.columns:
        print(f"‚ùå Column 10 not found in {csv_file}. Available columns: {df.columns}")
        return

    # Preprocess and split column 10 into a list of skills
    df[10] = df[10].astype(str).apply(preprocess_and_split)

    # Remove rows that end up with no skills
    df = df[df[10].apply(lambda skills: len(skills) > 0)]
    
    # Build the vocabulary from the dataset (a list of unique skills)
    all_skills = [skill for sublist in df[10] for skill in sublist]
    unique_skills = sorted(list(set(all_skills)))
    print(f"‚úÖ Unique skills count (vocabulary): {len(unique_skills)}")

    if not unique_skills:
        print("‚ùå No skills found in dataset, skipping index generation.")
        return

    # Use MultiLabelBinarizer to transform lists of skills into binary feature vectors
    mlb = MultiLabelBinarizer(classes=unique_skills)
    mlb.fit(df[10])
    skill_vectors = mlb.transform(df[10])

    if skill_vectors.shape[0] == 0:
        print("‚ùå No valid binary features generated. Skipping file.")
        return

    print(f"‚úÖ Binary feature matrix shape: {skill_vectors.shape}")

    # Convert to float32 and normalize for FAISS
    skill_vectors = skill_vectors.astype(np.float32)
    faiss.normalize_L2(skill_vectors)

    # FAISS index dimension equals the number of unique skills
    dimension = skill_vectors.shape[1]
    print(f"‚úÖ Feature vector dimension: {dimension}")

    # Create FAISS HNSW index and add the binary vectors
    print("‚ö° Building FAISS HNSW index...")
    index = faiss.IndexHNSWFlat(dimension, M)
    index.hnsw.efConstruction = EF_CONSTRUCTION
    index.add(skill_vectors)

    # Save the FAISS index to disk
    index_file = os.path.join(INDEX_DIR, f"{category_name}_hnsw.index")
    faiss.write_index(index, index_file)
    print(f"‚úÖ Index saved: {index_file}")

    # Save a mapping from FAISS index IDs to the DataFrame's row indices
    mapping_file = os.path.join(INDEX_DIR, f"{category_name}_mapping.npy")
    np.save(mapping_file, df.index.to_numpy())
    print(f"‚úÖ Mapping saved: {mapping_file}")

def main():
    """Main function to process CSV files and build indices."""
    # List your CSV files here; for example:
    csv_files = [f for f in os.listdir(CSV_DIR) if f.endswith('_subset.csv')]
    
    if not csv_files:
        print("‚ùå No CSV files found in the directory.")
        return

    for csv_file in csv_files:
        process_csv(csv_file)

if __name__ == "__main__":
    main()
